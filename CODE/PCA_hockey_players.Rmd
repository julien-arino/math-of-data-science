# Introduction to Principal Components Analysis (PCA)

See this [stackexchange page](https://opendata.stackexchange.com/questions/7793/age-weight-and-height-dataset) for links to height and weight datasets. Let us try something very Canadian..


```{r}
data = read.csv("https://figshare.com/ndownloader/files/5303173")
head(data)
dim(data)
```





In case you are wondering, this is a database of ice hockey players at IIHF world championships, 2001-2016, assembled by the dataset's author. See some comments [here](https://ikashnitsky.github.io/2017/ice-hockey-players-height/). As usual, it is a good idea to plot this to get a sense of the lay of the land.


```{r}
plot(data$height, data$weight,
    pch = 19, col = "dodgerblue4",
    xlab = "Height (cm)", ylab = "Weight (kg)")
```


The author of the study is interested in the evolution of weights, so it is likely that the same person will be in the dataset several times. Let us check this. First check will be `FALSE` if the number of unique names does not match the


```{r}
length(unique(data$name)) == dim(data)[1]
length(unique(data$name))
```


We are not interested in the evolution of weights, so far, so let us simplify things: if we have more than one record for someone, let us take their weight as the average of the weights recorded for them. To be extra careful, we could check as well that there are no major variations on player height, as this could indicate homonymies.


```{r}
# First, we make a data frame with just the names
data_simplified = data.frame(name = unique(data$name))
w = c()
h = c()
for (n in data_simplified$name) {
    tmp = data[which(data$name == n),]
    h = c(h, mean(tmp$height))
    w = c(w, mean(tmp$weight))
}
data_simplified$weight = w
data_simplified$height = h

head(data_simplified)
# Now keep that data (and call it data so it's easier)
data = data_simplified
```



```{r}
plot(data$height, data$weight,
    pch = 19, col = "dodgerblue4",
    xlab = "Height (cm)", ylab = "Weight (kg)")
```


Let us centre the data and plot the result.


```{r}
data$weight.c = data$weight-mean(data$weight)
data$height.c = data$height-mean(data$height)
plot(data$height.c, data$weight.c,
    pch = 19, col = "dodgerblue4",
    xlab = "Height (cm)", ylab = "Weight (kg)")
```




```{r}
mean(data$weight)
mean(data$height)
```



The function `cov` returns the covariance of two samples. Note that the functions deals equally well with data that is not centred as with data that is centred.


```{r}
cov(data$height, data$weight)
cov(data$height.c, data$weight.c)
```


As we could see from plotting the data, there is a positive linear relationship between the two variables. Let us compute the sample covariance matrix.


```{r}
# Select X as the matrix we want to process
X = as.matrix(data[,c("height.c", "weight.c")])
S = 1/(dim(X)[1]-1)*t(X) %*% X
S
```


The off-diagonal entries do match the computed covariance. Let us check that the variances are indeed a match too.


```{r}
var(X[,1])
var(X[,2])
```


Hey, that works. Is math not cool? ;)

Okay, joke aside, let us now compute the principal components. For this, we need eigenvalues and eigenvectors.


```{r}
ev = eigen(S)
ev
```



The result of a call to `eigen` will not necessarily be well ordered. (Here, we are in 2D, so chances are good. When we increase the size, things might become more iffy.) So we order the eigenvalues in decreasing order and apply the same ordering to the eigenvectors. (As the eigenvectors are the columns of `ev$vectors`, we order the columns.


```{r}
idx_order = order(ev$values, decreasing = TRUE)
ev$values = ev$values[idx_order]
ev$vectors = ev$vectors[, idx_order]
ev
```




Let us normalise the first eigenvector. This way, we now that the variance of the first principal component will be the corresponding eigenvalue.


```{r}
ev$vectors[,1] = ev$vectors[,1] / sqrt(sum(ev$vectors[,1]^2))
ev$vectors[,1]
sum(ev$vectors[,1]^2)
```




Let us plot this first eigenvector (well, the line carrying this first eigenvector). To use the function `abline`, we need to give the coefficients of the line in the form of (intercept,slope). Intercept is easy, as the line goes through the origin (by construction and because we have centred the data). The slope is also quite simple..


```{r}
plot(data$height.c, data$weight.c,
    pch = 19, col = "dodgerblue4",
    xlab = "Height (cm)", ylab = "Weight (kg)")
abline(a = 0, b = ev$vectors[2,1]/ev$vectors[1,1], 
       col = "red", lwd = 3)
```



Let us rotate the data so that the red line becomes the "x-axis". To do that, we use a rotation matrix,
$$
R_\theta = \begin{pmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{pmatrix}.
$$
To find the angle $\theta$, recall that $\tan\theta$ is equal to opposite length over adjacent length, i.e., 
$$
\tan\theta = \frac{\tt ev\$vectors[2,1]}{\tt ev\$vectors[1,1]}
$$
So we just use the $\arctan$ of this. (Note that angles are in radians.)


```{r}
theta = atan(ev$vectors[2,1]/ev$vectors[1,1])
theta
R_theta = matrix(c(cos(theta), -sin(theta),
                  sin(theta), cos(theta)),
                nr = 2, byrow = TRUE)
R_theta
```


And now we rotate the points. (In this case, we think of the points as vectors, of course.)


```{r}
tmp_in = matrix(c(data$weight.c, data$height.c),
                nc = 2)
tmp_out = c()
for (i in 1:dim(tmp_in)[1]) {
    tmp_out = rbind(tmp_out,
                    t(R_theta %*% tmp_in[i,]))
}
data$weight.c_r = tmp_out[,1]
data$height.c_r = tmp_out[,2]
```


```{r}
plot(data$height.c_r, data$weight.c_r,
    pch = 19, col = "dodgerblue4",
    xlab = "x-axis", ylab = "y-axis")
```




Note that the axes have changed quite a lot, hence the very different aspect. Let us plot with the same range as for the non-rotated data for the y-axis.


```{r}
plot(data$height.c_r, data$weight.c_r,
    pch = 19, col = "dodgerblue4",
    xlab = "x-axis", ylab = "y-axis",
    ylim = range(data$weight.c))
abline(h = 0, col = "red", lwd = 2)
```



Now do things "properly".


```{r}
if (!require("pracma")) {
    install.packages("pracma")
    library(pracma)
}
```



```{r}
GS = pracma::gramSchmidt(A = ev$vectors)
```


```{r}
GS
```



Now recall we saw a theorem that told us how to construct a new basis..


```{r}
A=matrix(c(GS$Q,1,0,0,1), nr = 2)
A
pracma::rref(A)
P = pracma::rref(A)[,c(3,4)]
P
X.new = X %*% t(P)
#for (i in 1:dim(X)[1]) {
#    X.new[i,] = P %*% X[i,]
#}
plot(X.new[,1], X.new[,2],
    pch = 19, col = "dodgerblue4")
```


