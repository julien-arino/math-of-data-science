\documentclass[aspectratio=169]{beamer}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

% Set lecture number for later use


% Part common to all the lectures
\subtitle{MATH 2740 -- Mathematics of Data Science -- Lecture 13}
\author{\texorpdfstring{Julien Arino\newline\url{julien.arino@umanitoba.ca}}{Julien Arino}}
\institute{Department of Mathematics @ University of Manitoba}
\date{Fall 202X}

% Title of the lecture
\title{Matrix methods -- Regular Markov chains}



\input{slides-setup-whiteBG.tex}



\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

% Set up cross-references and counter persistence
\setcounter{theorem}{89}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE AND OUTLINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titlepagewithfigure{FIGS-slides-admin/Gemini_Generated_Image_Kiki-as-king.png}
\outlinepage{FIGS-slides-admin/Gemini_Generated_Image_Kiki-as-devil.png}


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Markov chains}
\newSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_Kiki-bof.png}

\begin{frame}{Markov chain}
	A Markov chain is a \emph{stochastic process} in which the evolution through time depends only on the current state of the system (we say the process is \emph{memoryless})
	\vfill
	Markov chains are an interesting combination of matrix theory and graph theory
	\vfill
	They form the theoretical foundation for Hidden Markov processes or Markov Chain Monte Carlo (MCMC) methods, are used in ML
\end{frame}



\begin{frame}{Basic principle}
Conduct an experiment with a set of $n$ possible outcomes
\[
S=\{S_1,\dots, S_n\}
\]
\vfill
Experiment repeated $t$ times (with $t$ large, potentially infinite)
\vfill
Think of $t$ as \emph{time}
\vfill 
System has \emph{no memory}: the next state depends only on the present state
\vfill
Probability of $S_i$ occurring at time $t+1$ given that $S_j$ occurred at time $t$ is
\[
p_{ij}=\IP(S_i\,|\,S_j)
\]
\end{frame}


\begin{frame} 
Suppose that $S_i$ is the current state, then one of $S_1, \ldots,S_n$ must be the next state, so
\[
p_{1i}+p_{2i}+\cdots+p_{ni}=1, \quad 1\leq i\leq n
\]
\vfill
Some of the $p_{ij}$ can be zero, all that is needed is that $\sum_{k=1}^n p_{ki}=1$ for all $i=1,\ldots,n$
\end{frame}

\begin{frame}
\begin{definition}[Markov chain]
An experiment with finite number of possible outcomes $S_1,\ldots,S_n$ is repeated. The sequence of outcomes is a \textbf{Markov chain} if there is a set of $n^2$ numbers $\{p_{ij}\}$ such that the conditional probability of outcome $S_i$ on any experiment given outcome $S_j$ on the previous experiment is $p_{ij}$, i.e., for $1\leq i,j\leq n$, $t=1,\ldots$,
\[
	p_{ij}=\IP(S_i\textrm{ on experiment }t+1\;|\;
	S_j\textrm{ on experiment }t)	
\]
Outcomes $S_1,\ldots,S_n$ are \textbf{states} and $p_{ij}$ are \textbf{transition probabilities}. $P=[p_{ij}]$ the \textbf{transition matrix}
\end{definition}
\vfill
In the following, we often write
\[
\IP(S_i\textrm{ on experiment }t+1\;|\;
	S_j\textrm{ on experiment }t)	\text{ as }\IP(S_i(t+1)\;|\;S_j(t))
\]
\end{frame}


\begin{frame} 
The matrix 
\[
P=
\begin{pmatrix}
p_{11} & p_{12} & \cdots & p_{1r} \\
p_{21} & p_{22} & \cdots & p_{2r} \\
&&& \\
p_{r1} & p_{r2} & \cdots & p_{rr}
\end{pmatrix}
\]
has
\begin{itemize}
\item entries that are probabilities, i.e., $0\leq p_{ij}\leq 1$
\item column sum 1, which we write
\[
\sum_{i=1}^n p_{ij}=1,\quad j=1,\ldots,n
\]
or, using the notation $\nbOne^T=(1,\ldots,1)$,
\[
\nbOne^TP=\nbOne^T
\]
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Running example -- Mendelian inheritance}
\newSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_Kiki-bof.png}


\begin{frame}{The ``orange'' gene}
    
    A cat's coat color is determined by many genes. The "orange" trait comes from a specific gene called the \textbf{orange locus}
    
    \vfill
    
    It has two \emph{alleles} (versions):
    \begin{itemize}
        \item \textbf{O} $\implies$ Produces \emph{phaeomelanin} (red/orange pigment)
        \item \textbf{o} $\implies$ Produces \emph{eumelanin} (black/brown pigment)
    \end{itemize}
    
    \vfill
    
    This gene is \emph{sex-linked}. It is located on the \textbf{X chromosome}
    \begin{itemize}
        \item This changes the rules of inheritance!
    \end{itemize}
    
\end{frame}

% --- SLIDE 2: Males vs. Females ---
% This slide replaces the GG/Gg/gg slide
\begin{frame}{How sex-linked genes work}
    
    Because the gene is on the X chromosome, males and females inherit it differently
    
    \vfill
    
    \textbf{Females have two X chromosomes (XX)}
    \begin{itemize}
        \item They get two alleles for this gene (one from each parent)
        \item Possible genotypes: $X^O X^O$, $X^o X^o$, or $X^O X^o$
    \end{itemize}
    
    \vfill
    
    \textbf{Males have one X and one Y chromosome (XY)}
    \begin{itemize}
        \item They get \emph{only one} allele for this gene (always from the mother)
        \item Possible genotypes: $X^O Y$ or $X^o Y$
    \end{itemize}
    
\end{frame}

% --- SLIDE 3: Phenotypes (What we see) ---
% This slide is new and explains the unique "tortoiseshell" case
\begin{frame}{Genotype vs. phenotype}
    
    \textbf{Males (simple):}
    \begin{itemize}
        \item $X^O Y \implies$ \textbf{orange cat}
        \item $X^o Y \implies$ \textbf{non-orange cat} (e.g., black)
    \end{itemize}
    
    \vfill
    
    \textbf{Females (the special case):}
    \begin{itemize}
        \item $X^O X^O \implies$ \textbf{orange cat}
        \item $X^o X^o \implies$ \textbf{non-orange cat} (e.g., black)
        \item $X^O X^o \implies$ \textbf{tortoiseshell cat}
    \end{itemize}
    
    \vfill
    
    A ``tortie'' isn't a simple hybrid. Both alleles ($O$ and $o$) are active in different patches of skin, creating the orange and black mottled pattern

\end{frame}

\begin{frame}{Example 1: Orange dad + black Mom}
    Let's cross an \textbf{orange male ($X^O Y$)} with a \textbf{black female ($X^o X^o$)}
    
    \vfill
    
    \punnett{$X^O$ & $Y$} {
        \textbf{Mother} & $X^o$ & $X^O X^o$ & $X^o Y$ \\ \cmidrule(lr){2-4}
        & $X^o$ & $X^O X^o$ & $X^o Y$
    }
    \vfill
    
    \textbf{Results for their offspring:}
    \begin{itemize}
        \item \textbf{All females} ($X^O X^o$) will be \textbf{tortoiseshell}
        \item \textbf{All males} ($X^o Y$) will be \textbf{black} (non-orange)
    \end{itemize}
    
\end{frame}


\begin{frame}{Example 2: black dad + tortoiseshell mom}
    
    Let's cross a \textbf{black male ($X^o Y$)} with a \textbf{tortoiseshell female ($X^O X^o$)}
    
    \vfill
    
    \punnett{$X^o$ & $Y$} {
        \textbf{Mother} & $X^O$ & $X^O X^o$ & $X^O Y$ \\ \cmidrule(lr){2-4}
        & $X^o$ & $X^o X^o$ & $X^o Y$
    }
    \vfill
    
    \textbf{Results for their offspring (1/4 chance for each):}
    \begin{itemize}
        \item $X^O X^o \implies$ \textbf{Tortoiseshell Female}
        \item $X^o X^o \implies$ \textbf{Black Female}
        \item $X^O Y \implies$ \textbf{Orange Male}
        \item $X^o Y \implies$ \textbf{Black Male}
    \end{itemize}
    
\end{frame}


% --- SLIDE 6: Fun Fact (Optional) ---
\begin{frame}{Fun fact: what about male tortoiseshells?}
    
    \begin{itemize}
        \item As we saw, a male is $XY$. He can only get $X^O$ or $X^o$ from his mother, not both
        \vfill
        
        \item A male tortoiseshell is possible, but \emph{extremely rare}
        
        \vfill
        
        \item It's a genetic anomaly where the cat has an extra X chromosome: \textbf{XXY}
        
        \vfill
        
        \item This genotype (e.g., $X^O X^o Y$) allows the cat to be male ($Y$) but also express both orange and non-orange alleles ($X^O X^o$), just like a female
    \end{itemize}
    
\end{frame}





%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Repetition of the process}
\newSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_Kiki-bof.png}

\begin{frame}{Deriving the update equation}
For $1\leq i\leq n$, let $p_i(t)$ be probability that state $S_i$ occurs at time $t$, which we also write $p_i(t)=\IP(S_i(t))$
\vfill
Since one of the states $S_i$ must occur on the $t^{th}$ repetition, we must have
\[
p_1(t)+p_2(t)+\cdots+p_n(t)=1
\]
\vfill
We want to use this information to derive $p_i(t+1)$, the probability that state $S_i$ occurs at time $t+1$
\
\end{frame}

\begin{frame}{How do we get to $S_i$?}
List all possible ($n$) ways to be in state $S_i$ at time $t+1$:
\vfill
\begin{enumerate}
\item We were in $S_1$, which happened with probability $p_1(t)$, then moved from $S_1$ to $S_i$, which has probability $p_{i1}$. Thus, 
\[
\IP(S_i(t+1)\,|\, S_1(t)) = 
\IP(S_i(t+1)\,|\,S_1(t)) \; \IP(S_1(t)) = 
p_{i1}p_1(t)
\]
\vfill
\item Likewise, if we were in $S_2$ at time $t$, then
\[
\IP(S_i(t+1)\,|\, S_2(t)) = 
\IP(S_i(t+1)\,|\,S_2(t))\,\IP(S_2(t)) =
p_{i2}p_2(t)
\]
\vfill
\item[..]\mbox{}
\vfill
\item[n.] Finally, if we were in $S_n$,
\[
\IP(S_i(t+1)\,|\, S_n(t)) = 
\IP(S_i(t+1)\,|\,S_n(t))\,\IP(S_n(t)) =
p_{in}p_n(t)
\]
\end{enumerate}
\end{frame}


\begin{frame}{Sum things up}
So, for a given state $i=1,\ldots,n$,
\begin{align*}
p_i(t+1) &= P(S_i(t+1)\,|\,S_1(t))+\cdots+P(S_i(t+1)\,|\,S_n(t)) \\
&= p_{i1}p_1(t)+\cdots+p_{in}p_n(t)
\end{align*}
\vfill
Therefore, since this must be true for all states $i=1,\ldots,n$
\begin{align*}
p_1(t+1) &= p_{11}p_1(t)+p_{12}p_2(t)+\dots+p_{1n}p_n(t) \\
& \;\;\vdots\\
p_n(t+1) &= p_{n1}p_1(t)+p_{n2}p_2(t)+\dots+p_{nn}p_n(t)
\end{align*}
\end{frame}

\begin{frame}
In matrix form
\[
p(t+1)=Pp(t), \quad n=1,2,3,\dots
\]
where $p(t)=(p_1(t),p_{2}(t),\dots , p_n(t))^T$ is a probability vector and $P=(p_{ij})$ is an $n\times n$ \emph{transition matrix},
\[
P=
\begin{pmatrix}
p_{11} & p_{12} & \cdots & p_{1r} \\
p_{21} & p_{22} & \cdots & p_{2r} \\
&&& \\
p_{r1} & p_{r2} & \cdots & p_{rr}
\end{pmatrix}
\]
\end{frame}


\begin{frame}
So
\[
\begin{pmatrix}
	p_1(t+1) \\ \cdots \\ p_n(t+1)
\end{pmatrix}
=
\begin{pmatrix}
p_{11} & p_{12} & \cdots & p_{1r} \\
p_{21} & p_{22} & \cdots & p_{2r} \\
&&& \\
p_{r1} & p_{r2} & \cdots & p_{rr}
\end{pmatrix}
\begin{pmatrix}
	p_1(t) \\ \cdots \\ p_n(t)
\end{pmatrix}
\]
\vfill
Easy to check that this gives the same expression as before
\end{frame}



\begin{frame}{Stochastic matrices}
\begin{definition}[Stochastic matrix]
The nonnegative $n\times n$ matrix $M$ is \textbf{row-stochastic} (resp. \textbf{column-stochastic}) if $\sum_{j=1}^na_{ij}=1$ for all $i=1,\dots,n$ (resp. $\sum_{i=1}^na_{ij}=1$ for all $j=1,\dots,n$)
\end{definition}
\vfill
We often say \textbf{stochastic} and let the context determine whether we mean row- or column-stochastic
\vfill
If it is both row- and column-stochastic, the matrix is \textbf{doubly stochastic}
\vfill
\begin{theorem}\label{th:spectrum_stochastic_matrix}
Let $M\in\M_n$ be a stochastic matrix. Then all eigenvalues $\lambda$ of $M$ are such that $|\lambda|\leq 1$.
\end{theorem}
\end{frame}

\begin{frame}
\begin{importanttheorem}
\label{th:one-is-evalue-stochastic-matrix}
Let $M\in\M_n$ be a stochastic matrix. $\lambda =1$ is an eigenvalue of $M$. 
If $M$ is row-stochastic, the eigenvalue 1 is associated to the column vector of ones (a right eigenvector of $M$); if $M$ is column-stochastic, the eigenvalue 1 is associated to the row vector of ones (a left eigenvector of $M$)
\end{importanttheorem}
\end{frame}

\begin{frame}[red]{Proof of Theorem~\ref{th:one-is-evalue-stochastic-matrix}}
Suppose $M\in\M_n$ is row-stochastic. One way to write the requirement that each row sum equals 1 is as 
\begin{equation}\label{eq:M1equal1}
M\mathbf{1}=\mathbf{1}
\end{equation}
where $\mathbf{1}=(1,\ldots,1)\in\IC^n$ is a column vector
\vfill
If $M\in\M_n$, then the eigenpair equation takes the form
\[
M\bv=\lambda\bv,\quad \bv\neq\b0
\]
So, in \eqref{eq:M1equal1}, $\bv=\mathbf{1}$ and $\lambda=1$
\vfill
This works the same way for a column-stochastic matrix, except that here the relation is $\mathbf{1}M=\mathbf{1}$ with $\mathbf{1}$ a row vector and the (left)eigenpair relation is $\bv^TM=\lambda\bv^T$ with $\bv^T$ a row vector
\end{frame}

\begin{frame}{Long time behaviour}
Let $p(0)$ be the initial distribution vector. Then
\begin{align*}
p(1) &= Pp(0) \\
p(2) &= Pp(1) \\
&= P\left(Pp(0)\right) \\
&= P^2p(0)
\end{align*}
\vfill
Continuing, we get, for any $t$,
\[
p(t)=P^tp(0)
\]
\vfill
Therefore, 
\[
\lim_{t\rightarrow +\infty}p(t) =
\lim_{t\rightarrow +\infty}P^tp(0) =
\left(\lim_{t\rightarrow +\infty}P^t\right)
p(0)
\]
if this limit exists
\end{frame}


\begin{frame}{The matrix $P^t$}
\begin{theorem}
If $M,N$ are nonsingular stochastic matrices, then $MN$ is a stochastic matrix
\end{theorem}
\vfill
\begin{corollary}
If $M$ is a nonsingular stochastic matrix, then for any $k\in\IN$, $M^k$ is a stochastic matrix
\end{corollary}
\vfill
So $P^t$ is stochastic
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Regular Markov chains}
\newSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_Kiki-bof.png}


\begin{frame}{Regular Markov chains}
\begin{definition}[Regular Markov chain]
A \textbf{regular} Markov chain has $P^k$ (entry-wise) positive for some integer $k>0$, i.e., $P^k$ has only positive entries
\end{definition}
\vfill
\begin{definition}[Primitive matrix]
A nonnegative matrix $M$ is \textbf{primitive} if, and only if, there is an integer $k>0$ such that $M^k$ is positive.
\end{definition}
\vfill
\begin{theorem}
Markov chain regular $\iff$ transition matrix $P$ primitive
\end{theorem}
\end{frame}

\begin{frame}{Matrices and graphs}
Here and with absorbing chains, there is a lot to gain from using a bit of graph theory
\vfill
Matrices and graphs are intimitely linked 
\vfill
Some matrix problems are easier considered with graphs, some graph problems are easier with matrices
\vfill
Note that I say \emph{graph}, but in other contexts, people speak of \emph{networks}
\end{frame}

\begin{frame}{What is a directed graph?}
\begin{definition}[Digraph]
A \defword{directed graph} (or \textbf{digraph}) $G$ is a pair $(V, A)$ where:
    \begin{itemize}
        \item $V$ is a finite set of elements called \defword{vertices} or \textbf{nodes}
        \item $A \subseteq V \times V$ is a set of ordered pairs of vertices called \defword{arcs} or \textbf{directed edges}
    \end{itemize}
\end{definition}
    \vfill
\begin{definition}[Arc]
    An \defword{arc} $a = (u, v) \in A$ represents a connection \textbf{from} vertex $u$ \textbf{to} vertex $v$
    \begin{itemize}
        \item $u$ is the \textbf{tail} of the arc
        \item $v$ is the \textbf{head} of the arc
    \end{itemize}
\end{definition}
\end{frame}

\begin{frame}{In the context of Markov chains}
\begin{itemize}
    \item Vertices (nodes) represent the \textbf{states} of the system
    \vfill
    \item Arcs represent possible \textbf{transitions} between states
    \vfill
    \item The weights on the arcs represent the probability to make a given transition
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Matrix $\leftrightarrow$ Graph}
    
Given a transition matrix $P = [p_{ij}]$, define an induced digraph $\G = (V, A)$

\begin{itemize}
\item Vertices $V$ correspond to the states
\item An arc $(j, i)$ exists in $A$ if and only if $p_{ij} > 0$
\end{itemize}

\begin{columns}[T]
\begin{column}{.45\textwidth}
  \centering
  \textbf{Transition matrix}
  \[ 
    P = \begin{pmatrix}
              0.5 & 0.8 & 0.1 \\
              0.5 & 0   & 0.2 \\
              0   & 0.2 & 0.7
          \end{pmatrix}
  \]
\end{column}
\begin{column}{.55\textwidth}
\centering
\textbf{Transition graph}
\begin{tikzpicture}[
    ->, % All paths have arrows
    >=Latex, % Use the nicer Latex arrow tip
    node distance=2.5cm, % Distance between nodes
    every path/.style={thick}
]
    \node[state] (S1) at (0, 2) {$S_1$};
    \node[state] (S2) at (-2, 0) {$S_2$};
    \node[state] (S3) at (2, 0) {$S_3$};
    \path (S1) edge [loop above] node[above] {$0.5$} (S1);
    \path (S1) edge [bend left=10] node[right] {$0.5$} (S2);
    \path (S2) edge [bend left=10] node[left] {$0.8$} (S1);
    \path (S2) edge [bend left=10] node[above] {$0.2$} (S3);
    \path (S3) edge node[right] {$0.1$} (S1);
    \path (S3) edge [bend left=10] node[below] {$0.2$} (S2);
    \path (S3) edge [loop right] node[right] {$0.7$} (S3);
\end{tikzpicture}
\end{column}
\end{columns}
\end{frame}


\begin{frame}
\begin{definition}[Reducible/irrecible matrix]
A matrix $M\in\M_n$ is \defword{reducible} if there exists a permutation matrix $P$ such that
\[
P^TMP=
\begin{pmatrix}
P & Q \\ \b0 & R
\end{pmatrix},
\]
i.e., $M$ is similar to a block upper triangular matrix. The matrix $M$ is \defword{irreducible} if no such matrix exists
\end{definition}
\vfill
\begin{definition}[Strongly connected digraph]
A digraph $\G=(V,A)$ is \defword{strongly connected} if for any pair of vertices $u,v\in V$, there is a directed path from $u$ to $v$
\end{definition}
\vfill
\begin{importanttheorem}
$P\in\M_n$ irreducible $\iff$ $\G(P)$ strongly connected
\end{importanttheorem}
\end{frame}

\begin{frame}{A sufficient condition for primitivity}
\begin{theorem}
Let $M\in\M_n$ be a nonnegative matrix. If $\G(M)$ is strongly connected and at least one of the diagonal entries $m_{ii}$ of $M$ is positive, then $M$ is primitive
\end{theorem}
\end{frame}

\begin{frame}{Behaviour of a regular MC}
\begin{theorem}
If $P$ is the transition matrix of a regular Markov chain, then
\begin{enumerate}
\item the powers $P^t$ approach a stochastic matrix $W$
\item each column of $W$ is the same (column) vector $w=(w_1,\ldots,w_n)^T$
\item the components of $w$ are positive
\end{enumerate}
\end{theorem}
\vfill
So if the Markov chain is regular
\[
\lim_{t\rightarrow +\infty}p(t)=\lim_{t\rightarrow +\infty}P^tp(0)
=Wp(0)
\]
\end{frame}


\begin{frame}{Computing $W$}
Recall that since $P$ is a stochastic matrix, 1 is an eigenvalue of $P$. As $P$ is column stochastic, 1 is associated to the left (row) eigenvector $\11$
\vfill
Now, if $\bp(t)$ converges, then $\bp(t+1)=P\bp(t)$ at the limit, so $\bw=\lim_{t\to\infty}\bp(t)$ is a \textbf{fixed point} of the system. Replacing $\bp$ with its limit, we have
\[
\bw=P\bw
\]
\vfill
Solving for $\bw$ thus amounts to finding $\bw$ as a (right) eigenvector corresponding to the eigenvalue 1
\end{frame}

\begin{frame}{Remember to normalise}
$\bw$ might have to be normalized since you want a probability vector
\vfill
Check that the norm $\|\bw\|_1$ defined by
\[
\|\bw\|_1=|w_1|+\cdots+|w_n|=w_1+\cdots+w_n
\]
(since $\bw\geq\b0$) is equal to one
\vfill
If not, use
\[
\tilde\bw = \frac{\bw}{\|\bw\|_1}
\]
\end{frame}

\begin{frame}{Back to orange cats}
     Create a chain by tracking the 3 female genotypes:
    \begin{itemize}
        \item $S_1$: $X^O X^O$ (orange)
        \item $S_2$: $X^o X^o$ (black)
        \item $S_3$: $X^O X^o$ (tortoiseshell)
    \end{itemize}
    \vfill
    To make the chain regular, we mate our female with a male chosen randomly from a \textbf{fixed population} that is:
    \begin{itemize}
        \item 50\% orange males ($X^O Y$)
        \item 50\% black males ($X^o Y$)
    \end{itemize}
\end{frame}

\begin{frame}{State 1: orange female ($X^O X^O$)}
    The mother is $X^O X^O$. We pick a father with 50/50 probability.
    
    \begin{columns}[T]
        \begin{column}{.5\textwidth}
            \centering
            \textbf{Case 1: Father is $X^O Y$}
            \renewcommand{\arraystretch}{1.5}
            \begin{tabular}{c c | c | c |}
            \multicolumn{2}{c}{} & \multicolumn{1}{c}{$X^O$} & \multicolumn{1}{c}{$Y$} \\ \cline{3-4}
            \multirow{2}{*}{\rotatebox{90}{Mother}} & $X^O$ & \cellcolor{punnettorange}$X^O X^O$ ($S_1$) & \cellcolor{punnettorange}Male \\ \cline{2-4}
            & $X^O$ & \cellcolor{punnettorange}$X^O X^O$ ($S_1$) & \cellcolor{punnettorange}Male \\ \cline{2-4}
            \end{tabular}
            \vspace{1em}
            
            Daughters: 100\% $S_1$
        \end{column}
        \begin{column}{.5\textwidth}
            \centering
            \textbf{Case 2: Father is $X^o Y$}
            \renewcommand{\arraystretch}{1.5}
            \begin{tabular}{c c | c | c |}
            \multicolumn{2}{c}{} & \multicolumn{1}{c}{$X^o$} & \multicolumn{1}{c}{$Y$} \\ \cline{3-4}
            \multirow{2}{*}{\rotatebox{90}{Mother}} & $X^O$ & \cellcolor{punnetttortie}$X^O X^o$ ($S_3$) & \cellcolor{punnettorange}Male \\ \cline{2-4}
            & $X^O$ & \cellcolor{punnetttortie}$X^O X^o$ ($S_3$) & \cellcolor{punnettorange}Male \\ \cline{2-4}
            \end{tabular}
            \vspace{1em}
            
            Daughters: 100\% $S_3$
        \end{column}
    \end{columns}
    
    \vfill
    \textbf{Transitions from $S_1$:}
    \begin{itemize}
        \item $\IP(S_1 \to S_1) = 0.5 \times 1.0 = \mathbf{0.5}$
        \item $\IP(S_1 \to S_2) = 0$
        \item $\IP(S_1 \to S_3) = 0.5 \times 1.0 = \mathbf{0.5}$
    \end{itemize}
\end{frame}

\begin{frame}{State 2: black female ($X^o X^o$)}
    The mother is $X^o X^o$. We pick a father with 50/50 probability.
    
    \begin{columns}[T]
        \begin{column}{.5\textwidth}
            \centering
            \textbf{Case 1: Father is $X^O Y$}
            \renewcommand{\arraystretch}{1.5}
            \begin{tabular}{c c | c | c |}
            \multicolumn{2}{c}{} & \multicolumn{1}{c}{$X^O$} & \multicolumn{1}{c}{$Y$} \\ \cline{3-4}
            \multirow{2}{*}{\rotatebox{90}{Mother}} & $X^o$ & \cellcolor{punnetttortie}$X^O X^o$ ($S_3$)& \cellcolor{punnettblack}Male \\ \cline{2-4}
            & $X^o$ & \cellcolor{punnetttortie}$X^O X^o$ ($S_3$)& \cellcolor{punnettblack}Male \\ \cline{2-4}
            \end{tabular}
            \vspace{1em}
            
            Daughters: 100\% $S_3$
        \end{column}
        \begin{column}{.5\textwidth}
            \centering
            \textbf{Case 2: Father is $X^o Y$}
            \renewcommand{\arraystretch}{1.5}
            \begin{tabular}{c c | c | c |}
            \multicolumn{2}{c}{} & \multicolumn{1}{c}{$X^o$} & \multicolumn{1}{c}{$Y$} \\ \cline{3-4}
            \multirow{2}{*}{\rotatebox{90}{Mother}} & $X^o$ & \cellcolor{punnettblack}$X^o X^o$ ($S_2$)& \cellcolor{punnettblack}Male \\ \cline{2-4}
            & $X^o$ & \cellcolor{punnettblack}$X^o X^o$ ($S_2$)& \cellcolor{punnettblack}Male \\ \cline{2-4}
            \end{tabular}
            \vspace{1em}
            
            Daughters: 100\% $S_2$
        \end{column}
    \end{columns}
    
    \vfill
    \textbf{Transitions from $S_2$:}
    \begin{itemize}
        \item $\IP(S_2 \to S_1) = 0$
        \item $\IP(S_2 \to S_2) = 0.5 \times 1.0 = \mathbf{0.5}$
        \item $\IP(S_2 \to S_3) = 0.5 \times 1.0 = \mathbf{0.5}$
    \end{itemize}
\end{frame}

% --- SLIDE 3 ---
\begin{frame}{State 3: tortoiseshell female ($X^O X^o$)}
    The mother is $X^O X^o$. We pick a father with 50/50 probability.
    
    \begin{columns}[T]
        \begin{column}{.5\textwidth}
            \centering
            \textbf{Case 1: Father is $X^O Y$}
            \renewcommand{\arraystretch}{1.5}
            \begin{tabular}{c c | c | c |}
            \multicolumn{2}{c}{} & \multicolumn{1}{c}{$X^O$} & \multicolumn{1}{c}{$Y$} \\ \cline{3-4}
            \multirow{2}{*}{\rotatebox{90}{Mother}} & $X^O$ & \cellcolor{punnettorange}$X^O X^O$ ($S_1$) & \cellcolor{punnettorange}Male \\ \cline{2-4}
            & $X^o$ & \cellcolor{punnetttortie}$X^O X^o$ ($S_3$) & \cellcolor{punnettblack}Male \\ \cline{2-4}
            \end{tabular}
            \vspace{1em}
            
            Daughters: 50\% $S_1$, 50\% $S_3$
        \end{column}
        \begin{column}{.5\textwidth}
            \centering
            \textbf{Case 2: Father is $X^o Y$}
            \renewcommand{\arraystretch}{1.5}
            \begin{tabular}{c c | c | c |}
            \multicolumn{2}{c}{} & \multicolumn{1}{c}{$X^o$} & \multicolumn{1}{c}{$Y$} \\ \cline{3-4}
            \multirow{2}{*}{\rotatebox{90}{Mother}} & $X^O$ & \cellcolor{punnetttortie}$X^O X^o$ ($S_3$) & \cellcolor{punnettorange}Male \\ \cline{2-4}
            & $X^o$ & \cellcolor{punnettblack}$X^o X^o$ ($S_2$) & \cellcolor{punnettblack}Male \\ \cline{2-4}
            \end{tabular}
            \vspace{1em}
            
            Daughters: 50\% $S_2$, 50\% $S_3$
        \end{column}
    \end{columns}
    
    \vfill
    \textbf{Transitions from $S_3$:}
    \begin{itemize}
        \item $\IP(S_3 \to S_1) = 0.5 \times 0.5 = \mathbf{0.25}$
        \item $\IP(S_3 \to S_2) = 0.5 \times 0.5 = \mathbf{0.25}$
        \item $\IP(S_3 \to S_3) = (0.5 \times 0.5) + (0.5 \times 0.5) = \mathbf{0.5}$
    \end{itemize}
\end{frame}

\begin{frame}{Summary of the orange cat problem}
    The transition matrix $P$ for states $\{S_1, S_2, S_3\}$ is:
    
    $$
    P = 
    \begin{pmatrix}
    0.5 & 0 & 0.25 \\
    0 & 0.5 & 0.25 \\
    0.5 & 0.5 & 0.5
    \end{pmatrix}
    $$
    
    \vfill
    
    \textbf{Is this chain regular?}
    \begin{itemize}
        \item \textbf{Irreducible? Yes.} All states communicate.
            \begin{itemize}
                \item $S_1 \to S_3 \to S_2$ (Path from $S_1$ to $S_2$)
                \item $S_2 \to S_3 \to S_1$ (Path from $S_2$ to $S_1$)
                \item All other paths are direct ($S_1 \to S_3$, $S_3 \to S_1$, etc.)
            \end{itemize}
        \item \textbf{Aperiodic? Yes.} All states have self-loops ($p_{11}, p_{22}, p_{33} > 0$).
    \end{itemize}
    \vfill
    Since the chain is irreducible and aperiodic, it is \textbf{regular}.
\end{frame}



\begin{frame}
We must solve the right eigenvector problem $w = Pw$, where $w = (w_1, w_2, w_3)^T$
\[
\begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix}
=
\begin{pmatrix}
0.5 & 0 & 0.25 \\
0 & 0.5 & 0.25 \\
0.5 & 0.5 & 0.5
\end{pmatrix}
\begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix}
\]
This gives the system of (dependent) equations:
\begin{align*}
w_1 &= 0.5 w_1 + 0.25 w_3 \\
w_2 &= 0.5 w_2 + 0.25 w_3 \\
w_3 &= 0.5 w_1 + 0.5 w_2 + 0.5 w_3
\end{align*}
From the first two equations:
\begin{itemize}
    \item $0.5 w_1 = 0.25 w_3 \implies w_3 = 2w_1$
    \item $0.5 w_2 = 0.25 w_3 \implies w_3 = 2w_2$
\end{itemize}
\vfill
This means $w_1 = w_2$. Using the constraint $w_1+w_2+w_3 = 1$:
\[
w_1 + (w_1) + (2w_1) = 1 \implies 4w_1 = 1 \implies w_1 = 0.25
\]
\vfill
The stationary distribution is $w = \begin{pmatrix} 0.25 & 0.25 & 0.5 \end{pmatrix}^T$
\end{frame}




\end{document}
