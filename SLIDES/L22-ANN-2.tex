\documentclass[aspectratio=169]{beamer}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}




\input{slides-setup-whiteBG.tex}

\title{Clustering \& Classification using ANNs}
\author{\texorpdfstring{Julien Arino\newline University of Manitoba\newline\url{julien.arino@umanitoba.ca}}{Julien Arino}}
\date{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE AND OUTLINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titlepagewithfigure{FIGS-slides-admin/Gemini_Generated_Image_pbzl1mpbzl1mpbzl.jpeg}
\outlinepage{FIGS-slides-admin/Gemini_Generated_Image_yegaxcyegaxcyega.jpeg}

%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\section{Neural networks (the perceptron)}
\newSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_5g0m5e5g0m5e5g0m.jpeg}

\begin{frame}[fragile]{Setting Up R}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Install nicely}
\hlkwa{if} \hldef{(}\hlopt{!}\hlkwd{require}\hldef{(}\hlsng{"neuralnet"}\hldef{)) \{}
  \hlkwd{install.packages}\hldef{(}\hlsng{"neuralnet"}\hldef{)}
  \hlkwd{library}\hldef{(neuralnet)}
\hldef{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\vfill
The \texttt{neuralnet} package provides:
\begin{itemize}
    \item Flexible network specification
    \item Training via backpropagation
    \item Prediction on new data
    \item Network visualization
\end{itemize}
\end{frame}

\begin{frame}[fragile]\frametitle{Using \code{neuralnet} to learn OR}
First, create the truth table
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{OR_table} \hlkwb{=} \hlkwd{matrix}\hldef{(}\hlkwd{c}\hldef{(}\hlnum{0}\hldef{,} \hlnum{0}\hldef{,} \hlnum{0}\hldef{,}
                    \hlnum{1}\hldef{,} \hlnum{0}\hldef{,} \hlnum{1}\hldef{,}
                    \hlnum{0}\hldef{,} \hlnum{1}\hldef{,} \hlnum{1}\hldef{,}
                    \hlnum{1}\hldef{,} \hlnum{1}\hldef{,} \hlnum{1}\hldef{),}
                  \hlkwc{nc} \hldef{=} \hlnum{3}\hldef{,}
                  \hlkwc{byrow} \hldef{=} \hlnum{TRUE}\hldef{)}
\hldef{OR_table} \hlkwb{=} \hlkwd{as.data.frame}\hldef{(OR_table)}
\hlkwd{colnames}\hldef{(OR_table)} \hlkwb{=} \hlkwd{c}\hldef{(}\hlsng{"x1"}\hldef{,} \hlsng{"x2"}\hldef{,} \hlsng{"OR"}\hldef{)}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile]\frametitle{Now create and train the NN}
The ``formula'' is to find the OR column using the x1 and x2 columns. We use no hidden layer
\vfill
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{nn_OR} \hlkwb{=} \hlkwd{neuralnet}\hldef{(OR} \hlopt{~} \hldef{x1} \hlopt{+} \hldef{x2,}
                  \hlkwc{data} \hldef{= OR_table,}
                  \hlkwc{act.fct} \hldef{=} \hlsng{"logistic"}\hldef{,}
                  \hlkwc{hidden} \hldef{=} \hlnum{0}\hldef{,}
                  \hlkwc{linear.output} \hldef{=} \hlnum{FALSE}\hldef{)}
\hlcom{# Plot the result}
\hlkwd{plot}\hldef{(nn_OR,} \hlkwc{rep} \hldef{=} \hlsng{"best"}\hldef{)}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{frame}

\maxFrameImage{FIGS/ann-plot-NN-for-OR-1.pdf}

\begin{frame}[fragile]\frametitle{Testing the result}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{pred} \hlkwb{=} \hlkwd{predict}\hldef{(nn_OR, OR_table)}
\hldef{OR_table}\hlopt{$}\hldef{result} \hlkwb{=} \hldef{pred} \hlopt{>} \hlnum{0.5}
\hlkwd{kable}\hldef{(OR_table,} \hlsng{"latex"}\hldef{,} \hlkwc{booktabs} \hldef{=} \hlnum{TRUE}\hldef{)}
\end{alltt}
\end{kframe}
\begin{tabular}{rrrl}
\toprule
x1 & x2 & OR & result\\
\midrule
0 & 0 & 0 & FALSE\\
1 & 0 & 1 & TRUE\\
0 & 1 & 1 & TRUE\\
1 & 1 & 1 & TRUE\\
\bottomrule
\end{tabular}

\end{knitrout}
\end{frame}


\begin{frame}{Learning XOR}
    Let us now look at the XOR truth table
    \begin{center}
        \begin{tabular}{cccc}
            0 & 0 & $\mapsto$ & 0 \\
            1 & 0 & $\mapsto$ & 1 \\
            0 & 1 & $\mapsto$ & 1 \\
            1 & 1 & $\mapsto$ & 0 \\
        \end{tabular}
    \end{center}
    This problem is not solvable with a simple perceptron of the type we just used, as truth table is not \emph{linearly separable}
    \vfill
    Indeed, we would get weights $w_1>0$, $w_2>0$ to activate when presenting $[1,0]$ and $[0,1]$, but would require that the sum of the weights when applied to the input $[1,1]$, give a negative value.
\end{frame}

\begin{frame}{Linear separability and OR and XOR}
    \begin{center}
    \begin{tikzpicture}[scale=3,auto]
        \draw[step=1cm,gray!25!,very thin] (-0.25,-0.25) grid (1.25,1.25);
        \draw[thick,->] (-0.25,0) -- (1.5,0) node[anchor=north west] {$x_1$};
        \draw[thick,->] (0,-0.25) -- (0,1.5) node[anchor=south east] {$x_2$};
        \node[circle,draw,very thick] (c) at (0,0){};
        \fill (1,0)  circle[radius=2pt];
        \fill (0,1)  circle[radius=2pt];
        \fill (1,1)  circle[radius=2pt];
    \end{tikzpicture}\quad
    \begin{tikzpicture}[scale=3,auto]
        \draw[step=1cm,gray!25!,very thin] (-0.25,-0.25) grid (1.25,1.25);
        \draw[thick,->] (-0.25,0) -- (1.5,0) node[anchor=north west] {$x_1$};
        \draw[thick,->] (0,-0.25) -- (0,1.5) node[anchor=south east] {$x_2$};
        \node[circle,draw,very thick] (c) at (0,0){};
        \fill (1,0)  circle[radius=2pt];
        \fill (0,1)  circle[radius=2pt];
        \node[circle,draw,very thick] (c) at (1,1){};
    \end{tikzpicture}
    \end{center}
    A single-layer perceptron can only learn linearly separable problems
\end{frame}


\begin{frame}{Adding a hidden layer}
    It is possible to do XOR, but we need to add a \textbf{hidden layer}
    \begin{center}
        \def\hhskip{4cm}
        \def\vvskip{4cm}
        \begin{tikzpicture}[scale=1, 
		every node/.style={transform shape},
		auto,
		cloud/.style={minimum width={width("N-1")+2pt},
			draw, ellipse},
		connected/.style={dotted,-}]
		%% Input nodes
		\node [cloud] at (0,0*\vvskip) (i1) {$0\lor 1$};
		\node [cloud] at (0,-1*\vvskip) (i2) {$0\lor 1$};
        %% Hidden node
        \node [cloud] at (1*\hhskip,-0.5*\vvskip) (h1) {$\theta=1$};
		%% Output node
		\node [cloud] at (2*\hhskip,-0.5*\vvskip) (o1) {$\theta=0$};
		%% Arcs
		\path [line] (i1) to node [midway, above, sloped] (TextNode) {$w=1$} (o1);
		\path [line] (i2) to node [midway, below, sloped] (TextNode) {$w=1$} (o1);
		\path [line] (i1) to node [midway, below, sloped] (TextNode) {$w=0.6$} (h1);
		\path [line] (i2) to node [midway, above, sloped] (TextNode) {$w=0.6$} (h1);
		\path [line] (h1) to node [pos=0.25, above] (TextNode) {$w=-2$} (o1);
	\end{tikzpicture}
    \end{center}
\end{frame}




\begin{frame}[fragile]\frametitle{Now the XOR truth table}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{XOR_table} \hlkwb{=} \hlkwd{matrix}\hldef{(}\hlkwd{c}\hldef{(}\hlnum{0}\hldef{,} \hlnum{0}\hldef{,} \hlnum{0}\hldef{,}
                     \hlnum{1}\hldef{,} \hlnum{0}\hldef{,} \hlnum{1}\hldef{,}
                     \hlnum{0}\hldef{,} \hlnum{1}\hldef{,} \hlnum{1}\hldef{,}
                     \hlnum{1}\hldef{,} \hlnum{1}\hldef{,} \hlnum{0}\hldef{),}
                  \hlkwc{nc} \hldef{=} \hlnum{3}\hldef{,} \hlkwc{byrow} \hldef{=} \hlnum{TRUE}\hldef{)}
\hldef{XOR_table} \hlkwb{=} \hlkwd{as.data.frame}\hldef{(XOR_table)}
\hlkwd{colnames}\hldef{(XOR_table)} \hlkwb{=} \hlkwd{c}\hldef{(}\hlsng{"x1"}\hldef{,} \hlsng{"x2"}\hldef{,} \hlsng{"XOR"}\hldef{)}
\hlkwd{kable}\hldef{(XOR_table,} \hlsng{"latex"}\hldef{,} \hlkwc{booktabs} \hldef{=} \hlnum{TRUE}\hldef{)}
\end{alltt}
\end{kframe}
\begin{tabular}{rrr}
\toprule
x1 & x2 & XOR\\
\midrule
0 & 0 & 0\\
1 & 0 & 1\\
0 & 1 & 1\\
1 & 1 & 0\\
\bottomrule
\end{tabular}

\end{knitrout}
\end{frame}

\begin{frame}[fragile]\frametitle{Try to learn it without a hidden layer}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{nn_XOR} \hlkwb{=} \hlkwd{neuralnet}\hldef{(XOR} \hlopt{~} \hldef{x1} \hlopt{+} \hldef{x2,}
                   \hlkwc{data} \hldef{= XOR_table,}
                   \hlkwc{act.fct} \hldef{=} \hlsng{"logistic"}\hldef{,}
                   \hlkwc{hidden} \hldef{=} \hlnum{0}\hldef{,}
                   \hlkwc{linear.output} \hldef{=} \hlnum{FALSE}\hldef{)}
\hldef{pred} \hlkwb{=} \hlkwd{predict}\hldef{(nn_XOR, XOR_table)}
\hldef{XOR_table}\hlopt{$}\hldef{result} \hlkwb{=} \hldef{pred} \hlopt{>} \hlnum{0.5}
\hlkwd{kable}\hldef{(XOR_table,} \hlsng{"latex"}\hldef{,} \hlkwc{booktabs} \hldef{=} \hlnum{TRUE}\hldef{)}
\end{alltt}
\end{kframe}
\begin{tabular}{rrrl}
\toprule
x1 & x2 & XOR & result\\
\midrule
0 & 0 & 0 & TRUE\\
1 & 0 & 1 & TRUE\\
0 & 1 & 1 & FALSE\\
1 & 1 & 0 & FALSE\\
\bottomrule
\end{tabular}

\end{knitrout}
\end{frame}

\begin{frame}[fragile]\frametitle{Now with a hidden layer}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{nn_XOR} \hlkwb{=} \hlkwd{neuralnet}\hldef{(XOR} \hlopt{~} \hldef{x1} \hlopt{+} \hldef{x2,}
                   \hlkwc{data} \hldef{= XOR_table,}
                   \hlkwc{act.fct} \hldef{=} \hlsng{"tanh"}\hldef{,}
                   \hlkwc{hidden} \hldef{=} \hlnum{1}\hldef{)}
\hldef{pred} \hlkwb{=} \hlkwd{predict}\hldef{(nn_XOR, XOR_table)}
\hldef{XOR_table}\hlopt{$}\hldef{result} \hlkwb{=} \hldef{pred} \hlopt{>} \hlnum{0.5}
\hlkwd{kable}\hldef{(XOR_table,} \hlsng{"latex"}\hldef{,} \hlkwc{booktabs} \hldef{=} \hlnum{TRUE}\hldef{)}
\end{alltt}
\end{kframe}
\begin{tabular}{rrrl}
\toprule
x1 & x2 & XOR & result\\
\midrule
0 & 0 & 0 & FALSE\\
1 & 0 & 1 & TRUE\\
0 & 1 & 1 & TRUE\\
1 & 1 & 0 & TRUE\\
\bottomrule
\end{tabular}

\end{knitrout}
\vfill
Still hard with one hidden node
\end{frame}

\begin{frame}[fragile]{Using two hidden nodes}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{set.seed}\hldef{(}\hlnum{42}\hldef{)}
\hldef{nn_XOR} \hlkwb{<-} \hlkwd{neuralnet}\hldef{(XOR} \hlopt{~} \hldef{x1} \hlopt{+} \hldef{x2,}
                    \hlkwc{data} \hldef{= XOR_table,}
                    \hlkwc{hidden} \hldef{=} \hlnum{2}\hldef{,}                 \hlcom{# need >=2 neurons}
                    \hlkwc{act.fct} \hldef{=} \hlsng{"logistic"}\hldef{,}       \hlcom{# sigmoid}
                    \hlkwc{linear.output} \hldef{=} \hlnum{FALSE}\hldef{,}      \hlcom{# classification}
                    \hlkwc{err.fct} \hldef{=} \hlsng{"ce"}\hldef{,}             \hlcom{# cross-entropy}
                    \hlkwc{algorithm} \hldef{=} \hlsng{"rprop+"}\hldef{,}       \hlcom{# rprop+ often works well}
                    \hlkwc{stepmax} \hldef{=} \hlnum{1e6}\hldef{,}
                    \hlkwc{threshold} \hldef{=} \hlnum{1e-4}\hldef{,}
                    \hlkwc{rep} \hldef{=} \hlnum{1}\hldef{)}
\hlcom{# Use compute() to get predictions}
\hldef{pr} \hlkwb{<-} \hlkwd{compute}\hldef{(nn_XOR, XOR_table[,} \hlkwd{c}\hldef{(}\hlsng{"x1"}\hldef{,} \hlsng{"x2"}\hldef{)])}\hlopt{$}\hldef{net.result}
\hldef{XOR_table}\hlopt{$}\hldef{pred_prob} \hlkwb{<-} \hldef{pr[,}\hlnum{1}\hldef{]}
\hldef{XOR_table}\hlopt{$}\hldef{result} \hlkwb{<-} \hldef{(pr} \hlopt{>} \hlnum{0.5}\hldef{)}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile]{Did it work?}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{kable}\hldef{(XOR_table,} \hlsng{"latex"}\hldef{,} \hlkwc{booktabs} \hldef{=} \hlnum{TRUE}\hldef{)}
\end{alltt}
\end{kframe}
\begin{tabular}{rrrlr}
\toprule
x1 & x2 & XOR & result & pred\_prob\\
\midrule
0 & 0 & 0 & FALSE & 0.0000767\\
1 & 0 & 1 & TRUE & 0.9999371\\
0 & 1 & 1 & TRUE & 0.9999047\\
1 & 1 & 0 & FALSE & 0.0000766\\
\bottomrule
\end{tabular}

\end{knitrout}
\end{frame}

\begin{frame}[fragile]{Using two hidden nodes (another way)}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{set.seed}\hldef{(}\hlnum{42}\hldef{)}
\hldef{nn_XOR_tanh} \hlkwb{<-} \hlkwd{neuralnet}\hldef{(XOR} \hlopt{~} \hldef{x1} \hlopt{+} \hldef{x2,}
                         \hlkwc{data} \hldef{= XOR_table,}
                         \hlkwc{hidden} \hldef{=} \hlnum{2}\hldef{,}
                         \hlkwc{act.fct} \hldef{=} \hlkwa{function}\hldef{(}\hlkwc{x}\hldef{)} \hlkwd{tanh}\hldef{(x),}
                         \hlkwc{linear.output} \hldef{=} \hlnum{FALSE}\hldef{,}
                         \hlkwc{err.fct} \hldef{=} \hlsng{"sse"}\hldef{,} \hlcom{# SSE is OK with tanh targets}
                         \hlkwc{algorithm} \hldef{=} \hlsng{"rprop+"}\hldef{,}
                         \hlkwc{stepmax} \hldef{=} \hlnum{1e6}\hldef{,}
                         \hlkwc{threshold} \hldef{=} \hlnum{1e-4}\hldef{)}

\hldef{pr2} \hlkwb{<-} \hlkwd{compute}\hldef{(nn_XOR_tanh, XOR_table[,} \hlkwd{c}\hldef{(}\hlsng{"x1"}\hldef{,} \hlsng{"x2"}\hldef{)])}\hlopt{$}\hldef{net.result}
\hldef{XOR_table}\hlopt{$}\hldef{pred_tanh} \hlkwb{<-} \hldef{pr2[,}\hlnum{1}\hldef{]}
\hldef{XOR_table}\hlopt{$}\hldef{result_tanh} \hlkwb{<-} \hlkwd{as.integer}\hldef{(pr2} \hlopt{>} \hlnum{0}\hldef{)}  \hlcom{# positive -> class 1}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile]{Did it work?}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{kable}\hldef{(XOR_table,} \hlsng{"latex"}\hldef{,} \hlkwc{booktabs} \hldef{=} \hlnum{TRUE}\hldef{)}
\end{alltt}
\end{kframe}
\begin{tabular}{rrrlrrr}
\toprule
x1 & x2 & XOR & result & pred\_prob & pred\_tanh & result\_tanh\\
\midrule
0 & 0 & 0 & FALSE & 0.0000767 & 0.0000675 & 1\\
1 & 0 & 1 & TRUE & 0.9999371 & 0.9931660 & 1\\
0 & 1 & 1 & TRUE & 0.9999047 & 0.9918250 & 1\\
1 & 1 & 0 & FALSE & 0.0000766 & 0.0000730 & 1\\
\bottomrule
\end{tabular}

\end{knitrout}
\end{frame}

\begin{frame}[fragile]\frametitle{An example from the \code{neuralnet} manual -- Training vs testing sets}
\code{iris} is a built-in \code{R} dataset detailing physical characteristics of 150 flowers from 3 iris species
\vfill
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{train_idx} \hlkwb{<-} \hlkwd{sample}\hldef{(}\hlkwd{nrow}\hldef{(iris),} \hlnum{2}\hlopt{/}\hlnum{3} \hlopt{*} \hlkwd{nrow}\hldef{(iris))}
\hldef{iris_train} \hlkwb{<-} \hldef{iris[train_idx, ]}
\hldef{iris_test} \hlkwb{<-} \hldef{iris[}\hlopt{-}\hldef{train_idx, ]}
\end{alltt}
\end{kframe}
\end{knitrout}
\vfill
Thus we pick at random 2/3 of the data for training and 1/3 for testing. See some considerations on training, validation and testing on \href{https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets}{this Wikipedia page}
\end{frame}

\begin{frame}[fragile]\frametitle{An example from the \code{neuralnet} manual}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{nn} \hlkwb{<-} \hlkwd{neuralnet}\hldef{(Species} \hlopt{==} \hlsng{"setosa"} \hlopt{~} \hldef{Petal.Length} \hlopt{+} \hldef{Petal.Width,}
                \hldef{iris_train,} \hlkwc{linear.output} \hldef{=} \hlnum{FALSE}\hldef{)}
\hldef{pred} \hlkwb{<-} \hlkwd{predict}\hldef{(nn, iris_test)}
\hlkwd{table}\hldef{(iris_test}\hlopt{$}\hldef{Species} \hlopt{==} \hlsng{"setosa"}\hldef{, pred[,} \hlnum{1}\hldef{]} \hlopt{>} \hlnum{0.5}\hldef{)}
\end{alltt}
\begin{verbatim}
##        
##         FALSE TRUE
##   FALSE    31    0
##   TRUE      0   19
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile]\frametitle{Another example -- multiclass classification}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{nn} \hlkwb{<-} \hlkwd{neuralnet}\hldef{((Species} \hlopt{==} \hlsng{"setosa"}\hldef{)} \hlopt{+}
                  \hldef{(Species} \hlopt{==} \hlsng{"versicolor"}\hldef{)} \hlopt{+}
                  \hldef{(Species} \hlopt{==} \hlsng{"virginica"}\hldef{)}
                 \hlopt{~} \hldef{Petal.Length} \hlopt{+} \hldef{Petal.Width,}
                \hldef{iris_train,} \hlkwc{linear.output} \hldef{=} \hlnum{FALSE}\hldef{)}
\hldef{pred} \hlkwb{<-} \hlkwd{predict}\hldef{(nn, iris_test)}
\hlkwd{table}\hldef{(iris_test}\hlopt{$}\hldef{Species,} \hlkwd{apply}\hldef{(pred,} \hlnum{1}\hldef{, which.max))}
\end{alltt}
\begin{verbatim}
##             
##               1  2  3
##   setosa     19  0  0
##   versicolor  0 14  1
##   virginica   0  2 14
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}

\end{document}
