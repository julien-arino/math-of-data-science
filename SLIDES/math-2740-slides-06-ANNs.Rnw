\documentclass[aspectratio=169]{beamer}

<<set-options,echo=FALSE,warning=FALSE,message=FALSE>>=
# Load required libraries
required_packages = c("dplyr", 
                      "ggplot2",
                      "igraph",
                      "knitr", 
                      "latex2exp",
                      "magick",
                      "pracma", 
                      "readr", 
                      "tidyr")
for (p in required_packages) {
  if (!require(p, character.only = TRUE)) {
    install.packages(p, dependencies = TRUE)
    require(p, character.only = TRUE)
  }
}
# Prepare figure crop hook
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)
# Knitr options
opts_chunk$set(echo = TRUE, 
               warning = FALSE, 
               message = FALSE,
               crop = TRUE,
               dev = c("pdf", "png"),
               fig.width = 6, 
               fig.height = 4, 
               fig.path = "FIGS/graphs-",
               fig.keep = "high",
               fig.show = "hide")
@


<<set-slide-background,echo=FALSE,results='asis'>>=
# Are we plotting for a dark background?
plot_blackBG = FALSE
if (plot_blackBG) {
  bg_color = "black"
  fg_color = "white"
  input_setup = "\\input{slides-setup-blackBG.tex}"
} else {
  bg_color = "white"
  fg_color = "black"
  input_setup = "\\input{slides-setup-whiteBG.tex}"
}
cat(input_setup)
@

\title{Clustering \& Classification using ANNs}
\author{\texorpdfstring{Julien Arino\newline University of Manitoba\newline\url{julien.arino@umanitoba.ca}}{Julien Arino}}
\date{}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE AND OUTLINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titlepagewithfigure{FIGS-transitions/Gemini_Generated_Image_pbzl1mpbzl1mpbzl.jpeg}
\outlinepage{FIGS-transitions/Gemini_Generated_Image_yegaxcyegaxcyega.jpeg}

%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\section{Neural networks (the perceptron)}
\newSectionSlide{FIGS-transitions/Gemini_Generated_Image_5g0m5e5g0m5e5g0m.jpeg}

\begin{frame}{Artificial neural network (ANN) - from \href{https://en.wikipedia.org/wiki/Artificial_neural_network}{Wikipedia}}
    \begin{quote}
    \textbf{Artificial neural networks} (ANNs) are computing systems inspired by the biological neural networks that constitute animal brains
    \vfill
    An ANN is based on a collection of connected units or nodes called \textbf{artificial neurons}, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The ``signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold.
    \end{quote}
\end{frame}


\begin{frame}{The perceptron}
    One of the first neural networks (invented 1943, implemented 1957), made for simple classification tasks, for example recognising letters or numbers
    \vfill
    Two layers: the \emph{input} layer (the \emph{retina}) and the \emph{output} layer
    \vfill
    Inputs are 0 or 1, so are outputs
\end{frame}


\begin{frame}{}
    \begin{center}
        \def\hhskip{8cm}
        \def\vvskip{2.75cm}
        \begin{tikzpicture}[scale=0.75, 
		every node/.style={transform shape},
		auto,
		cloud/.style={minimum width={width("N-1")+2pt},
			draw, ellipse},
		connected/.style={dotted,-}]
		%% Input nodes
		\node [cloud] at (0,0*\vvskip) (i1) {$0\lor 1$};
		\node [cloud] at (0,-1*\vvskip) (i2) {$0\lor 1$};
		\node [cloud] at (0,-2*\vvskip) (i3) {$0\lor 1$};
		\node [cloud] at (0,-3*\vvskip) (i4) {$0\lor 1$};
		%% Output nodes
		\node [cloud] at (1*\hhskip,-0.5*\vvskip) (o1) {$\sum$};
		\node [cloud] at (1*\hhskip,-1.5*\vvskip) (o2) {$\sum$};
		\node [cloud] at (1*\hhskip,-2.5*\vvskip) (o3) {$\sum$};
		%% Arcs
		\path [line] (i1) to node [midway, above] (TextNode) {} (o1);
		\path [line] (i1) to node [midway, above] (TextNode) {} (o2);
		\path [line] (i1) to node [midway, above] (TextNode) {} (o3);
		\path [line] (i2) to node [midway, above] (TextNode) {} (o1);
		\path [line] (i2) to node [midway, above] (TextNode) {} (o2);
		\path [line] (i2) to node [midway, above] (TextNode) {} (o3);
		\path [line] (i3) to node [midway, above] (TextNode) {} (o1);
		\path [line] (i3) to node [midway, above] (TextNode) {} (o2);
		\path [line] (i3) to node [midway, above] (TextNode) {} (o3);
		\path [line] (i4) to node [midway, above] (TextNode) {} (o1);
		\path [line] (i4) to node [midway, above] (TextNode) {} (o2);
		\path [line] (i4) to node [midway, above] (TextNode) {} (o3);
        %% Arrows out of output nodes
		\path [line] (o1) to node [midway, above] (TextNode) {$0\lor 1$} (1.2*\hhskip,-0.5*\vvskip);
		\path [line] (o2) to node [midway, above] (TextNode) {$0\lor 1$} (1.2*\hhskip,-1.5*\vvskip);
		\path [line] (o3) to node [midway, above] (TextNode) {$0\lor 1$} (1.2*\hhskip,-2.5*\vvskip);
        %% Some labels
        \node[align=left] at (0.5*\hhskip,0) {Connections};
        \node[align=left] at (0*\hhskip,0.33*\vvskip) {Retina};
        \node[align=left] at (1*\hhskip,0.33*\vvskip) {Output layer};
	\end{tikzpicture}
    \end{center}
    The connections into the output layer are called \emph{synapses}, they are modifiable
\end{frame}

\begin{frame}{The activation function}
    \begin{center}
        \def\hhskip{8cm}
        \def\vvskip{2cm}
        \begin{tikzpicture}[scale=0.75, 
		every node/.style={transform shape},
		auto,
		cloud/.style={minimum width={width("N-1")+2pt},
			draw, ellipse},
		connected/.style={dotted,-}]
		%% Input nodes
		\node [cloud] at (0,0*\vvskip) (i1) {$x_0$};
		\node [cloud] at (0,-1*\vvskip) (i2) {$x_1$};
		\node [cloud] at (0,-2*\vvskip) (i3) {$x_i$};
		\node [cloud] at (0,-3*\vvskip) (i4) {$x_\ell$};
		%% Output nodes
		\node [cloud] at (1*\hhskip,-1.5*\vvskip) (o1) {$a_j\sum_{i=1}^I w_{ij}x_i$};
		%% Arcs
		\path [line] (i1) to node [midway, above] (TextNode) {$w_{1j}$} (o1);
		\path [line] (i2) to node [midway, above] (TextNode) {$w_{2j}$} (o1);
		\path [line] (i3) to node [midway, above] (TextNode) {$w_{ij}$} (o1);
		\path [line] (i4) to node [midway, above] (TextNode) {$w_{I j}$} (o1);
        %% Arrows out of output nodes
		\path [line] (o1) to node [midway, above] (TextNode) {$o_j=f(a_j)$} (1.5*\hhskip,-1.5*\vvskip);
	\end{tikzpicture}
    \end{center}
    Here,
    \[
        f(a_j)= \begin{cases}
            0 & \text{if }a_j\leq 0 \\
            1 & \text{if }a_j>0
        \end{cases}
    \]
\end{frame}

\begin{frame}{The activation function}
    We have $I$ input neurons taking values $0$ or $1$, $O$ output neurons taking values $0$ or $1$, weights $W=[w_{ij}]\in\M_{IO}$ and a threshold function $f$
    \vfill
    More generally, use a threshold $\theta_j$ for each output neuron
    \[
        o_j= \begin{cases}
            0 & \text{if }a_j\leq \theta_j \\
            1 & \text{if }a_j>\theta_j
        \end{cases}
    \]
    \vfill
    The thresholds (or \emph{response bias}) and the weights are modifiable by learning. To do that easily for the threshold, consider an input neuron that is always on, say neuron 0, and set weights $w_{0j}=-\theta_j$, making the weights matrix an $(I+1)\times O$-matrix
\end{frame}


\begin{frame}
    Another way to write the activation is 
    \[
        o_j= \begin{cases}
            0 & \text{if }a_j+w_{0j}\leq 0 \\
            1 & \text{if }a_j+w_{0j}>0
        \end{cases}
    \]
    where $w_{0j}=-\theta_j$
\end{frame}


\begin{frame}{Learning something simple}
    The aim is to adjust the synaptic weights so that the proper response is provided to a given stimulus
    \vfill
    Let us first do a simple example: the OR truth table
    \begin{center}
        \begin{tabular}{cccc}
            0 & 0 & $\mapsto$ & 0 \\
            1 & 0 & $\mapsto$ & 1 \\
            0 & 1 & $\mapsto$ & 1 \\
            1 & 1 & $\mapsto$ & 1 \\
        \end{tabular}
    \end{center}
    So we have two neurons in the retina and a single output neuron
\end{frame}

\begin{frame}{Supervised learning}
    (From R. Rojas)
    \begin{quote}
    \textbf{Supervised learning}: method in which some input vectors are collected and presented to the network. The output computed by the network is observed and the deviation from the expected answer is measured
    \vskip0.5cm
    The weights are corrected according to the magnitude of the error in the way defined by the learning algorithm
    \vskip0.5cm
    Also called \emph{learning with a teacher}, since a control process knows the correct answer for the set of selected input vectors
    \end{quote}
\end{frame}

\begin{frame}{Further distinctions in supervised learning methods}
    Methods with \emph{reinforcement} or \emph{error correction} 
    \vfill
    \begin{itemize}
        \item \textbf{Reinforcement learning}: used when after each presentation of an input-output example, we only know whether the network produces the desired result or not. 
        Weights are updated based on this information (i.e., the Boolean values true or false), so only the input vector can be used for weight correction
        \vfill
        \item In learning with \textbf{error correction}, the
        magnitude of the error, together with the input vector, determines the magnitude of the corrections to the weights. In many cases, we try to eliminate the error in a single correction step
    \end{itemize}
\end{frame}

\begin{frame}{A first learning algorithm}
    Suppose the training set consists of two sets of points $P$ and $N$
    \vfill
    \begin{itemize}
        \item \textbf{start:} Generate random weight vector $w_0$; set $t := 0$
        \vfill
        \item \textbf{test:} A vector $x\in P\cup N$ is selected randomly
        \begin{itemize}
            \item if $x\in P$ and $\langle w_t,x\rangle > 0$ go to \textbf{test}
            \item if $x\in P$ and $\langle w_t,x\rangle\leq 0$ go to \textbf{add}
            \item if $x\in N$ and $\langle w_t,x\rangle < 0$ go to \textbf{test}
            \item if $x\in N$ and $\langle w_t,x\rangle \geq 0$ go to \textbf{subtract}
        \end{itemize}
        \vfill
        \item \textbf{add:} set $w_{t+1} = w_t + x$ and $t := t + 1$, goto \textbf{test}
        \vfill
        \item \textbf{subtract:} set $w_{t+1} = w_t - x$ and $t := t + 1$, goto \textbf{test}
    \end{itemize}
\end{frame}

\begin{frame}{Widrow-Hoff learning rule}
    Need to provide the correct answer, i.e., this is a supervised learning rule
    \vfill
    An output cell only learns if it is mistaken
    \vfill
    Present random inputs and apply the rule if the output does not match the known output
\end{frame}

\begin{frame}{Widrow-Hoff learning rule}
    \begin{equation}\label{eq:WidrowHoff}
        w_{ij}^{(t+1)} = w_{ij}^{(t)}+\eta(t_j-o_j)x_j = w_{ij}^{(t)}+\Delta w_{ij}        
    \end{equation}
    with
    \begin{itemize}
        \item $\Delta w_{ij}$ correction to add to the weight $w_{ij}$
        \item $x_i$: value ($0$ or $1$) of the $i$th retinal cell
        \item $o_j$: response of the $j$th output cell
        \item $t_j$ target response (correct desired response)
        \item $w_{ij}^{(t)}$: weight of the synapse between the $i$th retinal cell and $j$th output cell at time $t$. Typically initiated at small random values
        \item $\eta$: small positive constant, the \emph{learning constant}
    \end{itemize} 
\end{frame}

\begin{frame}{Learning OR}
    \begin{center}
        \begin{tabular}{cccc}
            0 & 0 & $\mapsto$ & 0 \\
            1 & 0 & $\mapsto$ & 1 \\
            0 & 1 & $\mapsto$ & 1 \\
            1 & 1 & $\mapsto$ & 1 \\
        \end{tabular}
    \end{center}
    Three cells in the retina (two inputs and the ``dummy'' cell used for the threshold) and one output cell. So inputs and outputs must be
    \begin{center}
        \begin{tabular}{ccccc}
            1 & 0 & 0 & $\mapsto$ & 0 \\
            1 & 1 & 0 & $\mapsto$ & 1 \\
            1 & 0 & 1 & $\mapsto$ & 1 \\
            1 & 1 & 1 & $\mapsto$ & 1 \\
        \end{tabular}
    \end{center}
    Initialise the $3\times 1$ weight matrix $W$ to zero:
    \[
        W=\begin{pmatrix}
            w_0 = -\theta \\
            w_1 \\ w_2
        \end{pmatrix}
        = \begin{pmatrix}
            0\\ 0\\ 0
        \end{pmatrix}
    \]
\end{frame}


\begin{frame}{Procedure}
    We choose one random association in 
    \begin{center}
        \begin{tabular}{ccccc}
            1 & 0 & 0 & $\mapsto$ & 0 \\
            1 & 1 & 0 & $\mapsto$ & 1 \\
            1 & 0 & 1 & $\mapsto$ & 1 \\
            1 & 1 & 1 & $\mapsto$ & 1 \\
        \end{tabular}
    \end{center}
    say, the fourth one. So we present $[1,1,1]$ and expect an output of 1. We have
    \[
        a = \sum_i w_ix_i = (1\times 0)+(1\times 0)+(1\times 0) = 0
    \]
    This being $\leq 0$ means that $o=0$, giving an error of 1
\end{frame}

\begin{frame}{Applying the rule}
    Suppose the learning constant $\eta=0.1$. Then applying \eqref{eq:WidrowHoff},
    \begin{align*}
        \Delta w_0 &= \eta(t-o)x_0 = 0.1\times(1-0)\times 1= 0.1 \\
        \Delta w_1 &= \eta(t-o)x_0 = 0.1\times(1-0)\times 1= 0.1 \\
        \Delta w_2 &= \eta(t-o)x_0 = 0.1\times(1-0)\times 1= 0.1         
    \end{align*}
    Applying the correction, $W$ becomes
    \[
        W=\begin{pmatrix}
            0.1\\ 0.1\\ 0.1
        \end{pmatrix}
    \]
\end{frame}

\begin{frame}{Trying another input}
    Suppose we now present the first input $[1,0,0]$. This should produce a result of 0. Then
    \[
        a = \sum_i w_ix_i = (1\times 0.1)+(0\times 0.1)+(0\times 0.1) = 0.1
    \]
    which is $>0$, so $o=1$. We compute the correction
    \begin{align*}
        \Delta w_0 &= \eta(t-o)x_0 = 0.1\times(0-1)\times 1= -0.1 \\
        \Delta w_1 &= \eta(t-o)x_1 = 0.1\times(0-1)\times 0= 0 \\
        \Delta w_1 &= \eta(t-o)x_2 = 0.1\times(0-1)\times 0= 0         
    \end{align*}
    and adjust the weights, giving
    \[
        W=\begin{pmatrix}
            0\\ 0.1\\ 0.1
        \end{pmatrix}
    \]
\end{frame}

\begin{frame}{And we are done!}
    With the weights
    \[
        W=\begin{pmatrix}
            0\\ 0.1\\ 0.1
        \end{pmatrix}
    \]
    we are done. Indeed
    \begin{center}
        \begin{tabular}{cccccc}
            Input 0 & Input 1 & Input 2 & a & o & Should be \\
            1 & 0 & 0 & 0 & 0 & 0 \\
            1 & 1 & 0 & 0+0.1+0 & 1 & 1 \\
            1 & 0 & 1 & 0+0+0.1 & 1 & 1 \\
            1 & 1 & 1 & 0+0.1+0.1 & 1 & 1 \\
        \end{tabular}
    \end{center}    
\end{frame}


\begin{frame}{Learning XOR}
    Let us now look at the XOR truth table
    \begin{center}
        \begin{tabular}{cccc}
            0 & 0 & $\mapsto$ & 0 \\
            1 & 0 & $\mapsto$ & 1 \\
            0 & 1 & $\mapsto$ & 1 \\
            1 & 1 & $\mapsto$ & 0 \\
        \end{tabular}
    \end{center}
    This problem is not solvable with a simple perceptron of the type we just used, as truth table is not \emph{linearly separable}
    \vfill
    Indeed, we would get weights $w_1>0$, $w_2>0$ to activate when presenting $[1,0]$ and $[0,1]$, but would require that the sum of the weights when applied to the input $[1,1]$, give a negative value.
\end{frame}

\begin{frame}{Linear separability and OR and XOR}
    \begin{center}
    \begin{tikzpicture}[scale=3,auto]
        \draw[step=1cm,gray!25!,very thin] (-0.25,-0.25) grid (1.25,1.25);
        \draw[thick,->] (-0.25,0) -- (1.5,0) node[anchor=north west] {$x_1$};
        \draw[thick,->] (0,-0.25) -- (0,1.5) node[anchor=south east] {$x_2$};
        \node[circle,draw,very thick] (c) at (0,0){};
        \fill (1,0)  circle[radius=2pt];
        \fill (0,1)  circle[radius=2pt];
        \fill (1,1)  circle[radius=2pt];
    \end{tikzpicture}\quad
    \begin{tikzpicture}[scale=3,auto]
        \draw[step=1cm,gray!25!,very thin] (-0.25,-0.25) grid (1.25,1.25);
        \draw[thick,->] (-0.25,0) -- (1.5,0) node[anchor=north west] {$x_1$};
        \draw[thick,->] (0,-0.25) -- (0,1.5) node[anchor=south east] {$x_2$};
        \node[circle,draw,very thick] (c) at (0,0){};
        \fill (1,0)  circle[radius=2pt];
        \fill (0,1)  circle[radius=2pt];
        \node[circle,draw,very thick] (c) at (1,1){};
    \end{tikzpicture}
    \end{center}
    A single-layer perceptron can only learn linearly separable problems
\end{frame}


\begin{frame}{Adding a hidden layer}
    It is possible to do XOR, but we need to add a \textbf{hidden layer}
    \begin{center}
        \def\hhskip{4cm}
        \def\vvskip{4cm}
        \begin{tikzpicture}[scale=1, 
		every node/.style={transform shape},
		auto,
		cloud/.style={minimum width={width("N-1")+2pt},
			draw, ellipse},
		connected/.style={dotted,-}]
		%% Input nodes
		\node [cloud] at (0,0*\vvskip) (i1) {$0\lor 1$};
		\node [cloud] at (0,-1*\vvskip) (i2) {$0\lor 1$};
        %% Hidden node
        \node [cloud] at (1*\hhskip,-0.5*\vvskip) (h1) {$\theta=1$};
		%% Output node
		\node [cloud] at (2*\hhskip,-0.5*\vvskip) (o1) {$\theta=0$};
		%% Arcs
		\path [line] (i1) to node [midway, above, sloped] (TextNode) {$w=1$} (o1);
		\path [line] (i2) to node [midway, below, sloped] (TextNode) {$w=1$} (o1);
		\path [line] (i1) to node [midway, below, sloped] (TextNode) {$w=0.6$} (h1);
		\path [line] (i2) to node [midway, above, sloped] (TextNode) {$w=0.6$} (h1);
		\path [line] (h1) to node [pos=0.25, above] (TextNode) {$w=-2$} (o1);
	\end{tikzpicture}
    \end{center}
\end{frame}



\end{document}