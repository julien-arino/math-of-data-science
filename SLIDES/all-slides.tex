\documentclass[aspectratio=169]{beamer}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

% Set lecture number for later use


% Part common to all the lectures
\subtitle{MATH 2740 -- Mathematics of Data Science -- Lecture 00}
\author{\texorpdfstring{Julien Arino\newline\url{julien.arino@umanitoba.ca}}{Julien Arino}}
\institute{Department of Mathematics @ University of Manitoba}
\date{Fall 202X}

% Title of the lecture
\title{All definitions and results}



\input{slides-setup-whiteBG.tex}

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE AND OUTLINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titlepagewithfigure{FIGS-slides-admin/a_robot_looking_at_a_desolate_tree_in_the_style_of_Magritte.png}


\begin{frame}{Definitions are colour coded}
Memorising the definitions is part of the course. To help, definitions are colour coded
\vfill
\begin{definition}[Definitions]
These definitions are important, you need to know them
\end{definition}
\vfill
\begin{minordefinition}[Less important definitions]
These definitions are a little less important, you will not be asked to state them (although it is a good idea to know them anyway)
\end{minordefinition}
\end{frame}

\begin{frame}{Results are colour coded}
Memorising some of the results is part of the course. To help, results are colour coded
\vfill
\begin{theorem}[Theorems]
Theorems in blue boxes are worth knowing but you will not be asked to reproduce them
\end{theorem}
\vfill
\begin{importanttheorem}[Important theorems]  
Theorems in red boxes are important, you should know them and be able to reproduce them
\end{importanttheorem}
\end{frame}


\begin{frame}[red]{You must know how to do some proofs}
There are a few proofs (not many!) that I want you to know how to do
\vfill
Such proofs appear on slides like the present one, with a red background
\end{frame}


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\outlinepage{FIGS-slides-admin/a_robot_looking_at_a_desolate_tree_in_the_style_of_Dali.png}
\Ssection{Preliminary stuff}{FIGS-slides-admin/a_robot_looking_at_a_desolate_tree_in_the_style_of_Magritte.png}

\begin{frame}
\frametitle{Intersection and union of sets}
	Let $X$ and $Y$ be two sets
	\vfill
	\begin{definition}[Intersection]
		The intersection of $X$ and $Y$, $X\cap Y$, is the set of elements that belong
		to $X$ \textbf{and} to $Y$,
		\[
		X\cap Y=\{x:x\in X\textbf{ and } x\in Y\}
		\]
	\end{definition}
	\vfill
	\begin{definition}[Union]
		The union of $X$ and $Y$, $X\cup Y$, is the set of elements that belong
		to $X$ \textbf{or} to $Y$,
		\[
		X\cup Y=\{x:x\in X\textbf{ or } x\in Y\}
		\]
	\end{definition}
In mathematics, or=and/or in common parlance. We also have an \textbf{exclusive or} (xor)
\end{frame}

\begin{frame}{Complex numbers}
	\begin{definition}[Complex numbers]
		A \textbf{complex number} is an ordered pair $(a,b)$, where $a,b\in\IR$. Usually written $a+ib$ or $a+bi$, where $i^2=-1$ (i.e., $i=\sqrt{-1}$)
		\vfill
		The set of all complex numbers is denoted $\IC$, 
		\[
		\IC=\{a+ib: a,b\in\IR\}
		\]
	\end{definition}
\end{frame}

\begin{frame}
	\begin{definition}[Addition and multiplication on $\IC$]
		Letting $a+ib$ and $c+id\in\IC$, addition on $\IC$ is defined by
		\[
		(a+ib)+(c+id) = (a+c)+i(b+d)
		\]
		and multiplication on $\IC$ is defined by
		\[
		(a+ib)(c+id) = (ac-bd)+i(ad+bc)
		\]
	\end{definition}
	\vfill
	Latter is easy to obtain using regular multiplication and $i^2=-1$
\end{frame}

\begin{frame}
	\begin{definition}[Real and imaginary parts]
		Let $z=a+ib$. Then $\Re z=a$ is \textbf{real part} and $\Im z=b$ is \textbf{imaginary part} of $z$
	\end{definition}
	\vfill
	If ambiguous, write $\Re(z)$ and $\Im(z)$
	\vfill
	\begin{definition}[Conjugate and Modulus]
		Let $z=a+ib\in\IC$. Then
		\begin{itemize}
			\item \textbf{Complex conjugate} of $z$ is
			\[
			\bar z = a-ib
			\]
			\item \textbf{Modulus} (or \textbf{absolute value}) of $z$ is
			\[
			|z|=\sqrt{a^2+b^2} \geq 0
			\]
		\end{itemize}
	\end{definition}
	\vfill
	$z\bar z=|z|^2$ and $\overline{\bar z}=z$
\end{frame}

\begin{frame}{Vectors}
	A \textbf{vector} $\bv$ is an ordered $n$-tuple of real or complex numbers
	\vfill 
	Denote $\IF=\IR$ or $\IC$ (real or complex numbers). 
	For
	$v_1,\ldots,v_n\in\IF$, 
	\[
	\bv=(v_1,\ldots,v_n)\in\IF^n
	\]
	is a vector. $v_1,\ldots,v_n$ are the \textbf{components} of $\bv$
	\vfill
	If unambiguous, we write $v$. Otherwise, $\bv$ or $\vec{v}$
\end{frame}


\begin{frame}
\frametitle{Vector space}
	\begin{minordefinition}[Vector space]
		A \textbf{vector space} over $\IF$ is a set $V$
		together with two binary operations, \textbf{vector addition}, denoted $+$,
		and \textbf{scalar multiplication}, that satisfy the relations:
		\begin{enumerate}
			\item $\forall\bu, \bv, \bw\in V$, $\bu+(\bv+\bw)=(\bu+\bv)+\bw$
			\item $\forall\bv, \bw\in V$, $\bv + \bw = \bw + \bv$
			\item $\exists\b0\in V$, the zero vector, such that $\bv +\b0
			= \bv$ for all $\bv\in V$
			\item $\forall\bv\in V$, there exists an element $\bw\in V$, the additive
			inverse of $\bv$, such that $\bv + \bw = \b0$
			\item $\forall\alpha\in\IR$ and $\forall \bv,\bw\in V$, $\alpha(\bv + \bw) = \alpha \bv +
			\alpha \bw$
			\item $\forall\alpha,\beta\in\IR$ and $\forall \bv\in V$, $(\alpha+\beta)\bv=
			\alpha \bv + \beta \bv$
			\item $\forall\alpha,\beta\in\IR$ and $\forall\bv\in V$, $\alpha (\beta\bv) =
			(\alpha\beta)\bv$
			\item $\forall\bv\in V$, $1\bv =\bv$
		\end{enumerate}
	\end{minordefinition}
\end{frame}

\begin{frame}
\frametitle{Norms}
	\begin{definition}[Norm]
		Let $V$ be a vector space over $\IF$, and $\bv\in V$ be a vector. The
		\textbf{norm} of $\bv$, denoted $\|\bv\|$, is a function from $V$ to $\IR_+$ that has the
		following properties:
		\begin{enumerate}
			\item For all $\bv\in V$, $\|\bv\|\geq 0$ with $\|\bv\|=0$ iff $\bv=\b0$
			\item For all $\alpha\in\IF$ and all $\bv\in V$, $\|\alpha \bv\|=|\alpha|\ \|\bv\|$
			\item For all $\bu,\bv\in V$, $\|\bu+\bv\|\leq\|\bu\|+\|\bv\|$
		\end{enumerate}
	\end{definition}
\end{frame}

\begin{frame}
	Let $V$ be a vector space (for example, $\IR^2$ or $\IR^3$)
	\vfill
	The \textbf{zero element} (or \textbf{zero vector}) is the vector $\b0=(0,\ldots,0)$
	\vfill
	The \textbf{additive inverse} of $\bv=(v_1,\ldots,v_n)$ is $-\bv=(-v_1,\ldots,-v_n)$ 
	\vfill
	For $\bv=(v_1,\ldots,v_n)\in V$, the length (or Euclidean norm) of $\bv$ is the
	\textbf{scalar}
	\[
	\|\bv\|=\sqrt{v_1^2+\cdots+v_n^2}
	\]
	\vfill
	To \textbf{normalize} the vector $\bv$ consists in considering $\tilde\bv=\bv/\|\bv\|$, i.e., the vector in the same direction as $\bv$ that has unit length
\end{frame}

\begin{frame}
\frametitle{Dot product}
	\begin{definition}[Dot product]
		Let $\ba=(a_1,\ldots,a_n)\in \IR^n$, $\bb=(b_1,\ldots,b_n)\in\IR^n$. 
		The \defword{dot product} of $\ba$ and $\bb$ is the \textbf{scalar}
		\[
		\ba\bullet \bb=\sum_{i=1}^n a_ib_i=a_1b_1+\cdots+a_nb_n
		\]
	\end{definition}
\end{frame}

\begin{frame}
\frametitle{Properties of the dot product}
	\begin{theorem}
		For $\ba,\bb,\bc\in \IR^n$ and $\alpha\in\IR$,
		\begin{itemize}
			\item $\ba\bullet\ba=\|\ba\|^2$ \hfill (so $\ba\bullet \ba\geq 0$, with $\ba\bullet \ba=0$ iff $\ba=\b0$)
			\item $\ba\bullet \bb=\bb\bullet \ba$ \hfill ($\bullet$ is commutative)
			\item $\ba\bullet(\bb+\bc)=\ba\bullet \bb+\ba\bullet \bc$ \hfill ($\bullet$ distributive over $+$)
			\item $(\alpha \ba)\bullet \bb=\alpha(\ba\bullet \bb)=\ba\bullet(\alpha \bb)$
			\item $\b0\bullet \ba=0$
		\end{itemize}
	\end{theorem}
\end{frame}

\begin{frame}
\frametitle{Some results stemming from the dot product}
	\begin{theorem}
		If $\theta$ is the angle between the vectors $\ba$ and $\bb$, then
		\[
		\ba\bullet \bb=\|\ba\|\;\|\bb\|\;\cos\theta
		\]
	\end{theorem}
	\vfill
	\begin{theorem}
		$\ba$ and $\bb$ are orthogonal if and only if $\ba\bullet \bb=0$.
	\end{theorem}
\end{frame}

\begin{frame}
\frametitle{Scalar and vector projections}
	Scalar projection of $\bv$ onto $\ba$ (or component of $\bv$ along $\ba$):
	\[
  	\mathsf{comp}_{\ba}\bv=
  	\frac{\ba\bullet\bv}{\|\ba\|}
	\]
	\begin{minipage}{0.59\textwidth}
		Vector (or orthogonal) projection of $\bv$ onto $\ba$:
		\[
		\mathsf{proj}_{\ba}\bv=
		  \left(
		    \frac{\ba\bullet\bv}{\|\ba\|}
		  \right)
		  \frac{\ba}{\|\ba\|}
		  =\frac{\ba\bullet\bv}{\|\ba\|^2}\ba
		\]
	\end{minipage}
	\begin{minipage}{0.39\textwidth}
		\begin{center}
			\includegraphics[width=1.1\textwidth]{FIGS/proj_v_onto_a}
		\end{center}
	\end{minipage}
\end{frame}


\begin{frame}{Linear systems}
\begin{definition}[Linear system]
	A \defword{linear system} of $m$ equations in $n$ unknowns takes the form
	\begin{equation}\label{sys:linear_system}
	\begin{matrix}
	a_{11}x_1 &+& a_{12}x_2 &+& \cdots &+& a_{1n}x_n &=& b_1 \\
	a_{21}x_1 &+& a_{22}x_2 &+& \cdots &+& a_{2n}x_n &=& b_2 \\
	\vdots && \vdots && \vdots && \vdots && \vdots \\
	a_{m1}x_1 &+& a_{m2}x_2 &+& \cdots &+& a_{mn}x_n &=& b_n
	\end{matrix}
	\end{equation}
\end{definition}
The $a_{ij}$, $x_j$ and $b_j$ could be in $\IR$ or $\IC$, although here we typically assume they are in $\IR$
\vfill
The aim is to find $x_1,x_2,\ldots,x_n$ that satisfy all equations simultaneously
\end{frame}

\begin{frame}
\begin{importanttheorem}[Nature of solutions to a linear system]
\label{th:nature_solutions_linear_system}
A linear system can have
\begin{itemize}
	\item no solution
	\item a unique solution
	\item infinitely many solutions
\end{itemize}
\end{importanttheorem}
\end{frame}


\begin{frame}{Matrices and linear systems}
Writing
\[
A=
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots &\vdots & & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix},\quad
\bx=
\begin{pmatrix}
x_1\\ x_2 \\ \vdots \\ x_n
\end{pmatrix}
\quad\textrm{and}\quad
\bb=
\begin{pmatrix}
b_1\\ b_2 \\ \vdots \\ b_n
\end{pmatrix}
\]
where $A$ is an $m\times n$ \textbf{matrix}, $\bx$ and $\bb$ are $n$ (column) \textbf{vectors} (or $n\times 1$ matrices), then the linear system in the previous slide takes the form
\[
A\bx=\bb
\]
\vfill
If $\bb=\b0$, the system is \defword{homogeneous} and always has the solution $\bx=0$ and so the ``no solution'' option in Theorem~\ref{th:nature_solutions_linear_system} goes away
\end{frame}



\begin{frame}
	\begin{definition}[Matrix]
		An $m$-by-$n$ or $m\times n$ matrix is a rectangular array of elements of $\IR$ or $\IC$ with $m$ rows and $n$ columns,
		\[
		A=[a_{ij}]=
		\begin{pmatrix}
		a_{11} & \cdots & a_{1n} \\
		\vdots & & \vdots \\
		a_{m1} & \cdots & a_{mn}
		\end{pmatrix}
		\]
	\end{definition}
	\vfill
	We always list indices as ``row,column''
	\vfill
	We denote $\M_{mn}(\IF)$ or $\IF^{mn}$ the set of $m\times n$ matrices with entries in $\IF=\{\IR,\IC\}$. Often, we omit $\IF$ in $\M_{mn}$ if the nature of $\IF$ is not important
	\vfill
	When $m=n$, we usually write $\M_n$
\end{frame}

\begin{frame}{Basic matrix arithmetic}
Let $A\in\M_{mn},B\in\M_{mn}$ be matrices (of the same size) and $c\in\IF=\{\IR,\IC\}$ be a scalar
\begin{itemize}
	\item \textbf{Scalar multiplication}
	\[
	cA = [ca_{ij}]
	\]
	\item \textbf{Addition}
	\[
	A+B = [a_{ij}+b_{ij}]
	\]
	\item \textbf{Subtraction} (addition of $-B=(-1)B$ to $A$)
	\[
	A-B=A+(-1)B=[a_{ij}+(-1)b_{ij}]=[a_{ij}-b_{ij}]
	\]
	\item \textbf{Transposition} of $A$ gives a matrix $A^T=\M_{nm}$ with
	\[
	A^T=[a_{ji}],\quad j=1,\ldots,n,\quad i=1,\ldots,m
	\]
\end{itemize}
\end{frame}

\begin{frame}{Matrix multiplication}
The (matrix) \textbf{product} of $A$ and $B$, $AB$, requires the ``inner dimensions'' to match, i.e., the number of columns in $A$ must equal the number of rows in $B$
\vfill
Suppose that is the case, i.e., let $A\in\M_{mn}$, $B\in\M_{np}$. Then the $i,j$ entry in $C:=AB$ takes the form
\[
c_{ij} = \sum_{k=1}^n a_{ik}b_{kj}
\]
\vfill
Recall that the matrix product is not commutative, i.e., in general, $AB\neq BA$ (when both those products are defined, i.e., when $A,B\in\M_n$)
\end{frame}

\begin{frame}{Special matrices}
\begin{definition}[Zero and identity matrices]
The \textbf{zero} matrix is the matrix $0_{mn}$ whose entries are all zero.
The \textbf{identity} matrix is a square $n\times n$ matrix $\II_n$ with all entries on the main diagonal equal to one and all off diagonal entries equal to zero
\end{definition}
\vfill
\begin{definition}[Symmetric matrix]
A square matrix $A\in\M_n$ is \textbf{symmetric} if $\forall i,j=1,\ldots,n$, $a_{ij}=a_{ji}$. In other words, $A\in\M_n$ is symmetric if $A=A^T$
\end{definition}
\vfill
\begin{importanttheorem}\label{th:symmetric_matrices}
\begin{enumerate}
	\item If $A\in\M_n$, then $A+A^T$ is symmetric
	\item If $A\in\M_{mn}$, then $AA^T\in\M_m$ and $A^TA\in\M_n$ are symmetric
\end{enumerate}
\end{importanttheorem}
\end{frame}

\begin{frame}[red]
\frametitle{Proof of Theorem~\ref{th:symmetric_matrices}}
$X$ symmetric $\iff$ $X=X^T$, so use $X=$ the matrix whose symmetric property you want to check

1. True if $A+A^T=(A+A^T)^T$. We have 
\[
(A+A^T)^T=A^T+(A^T)^T=A^T+A=A+A^T
\]

2. $AA^T$ symmetric if $AA^T=(AA^T)^T$. We have 
\[
(AA^T)^T=(A^T)^TA^T=AA^T
\]
$A^TA$ works similarly
\end{frame}

\begin{frame}{Two special matrices and their determinants}
\begin{definition}
$A\in\M_n$ is \textbf{upper triangular} if $a_{ij}=0$ when $i>j$, \textbf{lower triangular} if $a_{ij}=0$ when $j>i$, \textbf{triangular} if it is \emph{either} upper or lower triangular and \textbf{diagonal} if it is \emph{both} upper and lower triangular
\end{definition}
When $A$ diagonal, we often write $A=\diag(a_{11},a_{22},\ldots,a_{nn})$
\begin{importanttheorem}
Let $A\in\M_n$ be triangular or diagonal. Then
\[
\det(A)=\prod_{i=1}^n a_{ii}=a_{11}a_{22}\cdots a_{nn}
\]
\end{importanttheorem}
\end{frame}

\begin{frame}{Inversion/Singularity}
\begin{definition}[Matrix inverse]
$A\in\M_n$ is \textbf{invertible} (or \textbf{nonsingular}) if $\exists A^{-1}\in\M_n$ s.t.
\[
AA^{-1}=A^{-1}A=\II
\]
$A^{-1}$ is the \textbf{inverse} of $A$. If $A^{-1}$ does not exist, $A$ is \textbf{singular}
\end{definition}
\end{frame}

\begin{frame}{Eigenvalues / Eigenvectors / Eigenpairs}
\begin{definition}
Let $A\in\M_n$. A vector $\bx\in\IF^n$ such that $\bx\neq\b0$ is an \textbf{eigenvector} of $A$ if $\exists\lambda\in\IF$ called an \textbf{eigenvalue}, s.t.
\[
A\bx=\lambda \bx
\]
A couple $(\lambda,\bx)$ with $\bx\neq\b0$ s.t. $A\bx=\lambda\bx$ is an \textbf{eigenpair}
\end{definition}
\vfill
If $(\lambda,\bx)$ eigenpair, then for $c\neq 0$, $(\lambda,c\bx)$ also eigenpair since $A(c\bx)=cA\bx=c\lambda\bx$ and dividing both sides by $c$..
\end{frame}



\begin{frame}{Similarity}
\begin{minordefinition}[Similarity]
$A,B\in\M_n$ are \defword{similar} ($A\sim B$) if $\exists P\in\M_n$ invertible s.t.
\[
P^{-1}AP=B
\]
\end{minordefinition}
\vfill
\begin{theorem}
$A,B\in\M_n$ with $A\sim B$. Then
\begin{itemize}
	\item $\det\ A=\det\ B$
	\item $A$ invertible $\iff$ $B$ invertible
	\item $A$ and $B$ have the same eigenvalues
\end{itemize}
\end{theorem}
\end{frame}


\begin{frame}{Diagonalisation}
\begin{definition}[Diagonalisability]
$A\in\M_n$ is \defword{diagonalisable} if $\exists D\in\M_n$ diagonal s.t. $A\sim D$
\end{definition}
\vfill
In other words, $A\in\M_n$ is diagonalisable if there exists a diagonal matrix $D\in\M_n$ and a nonsingular matrix $P\in\M_n$ s.t. $P^{-1}AP=D$
\vfill
Could of course write $PAP^{-1}=D$ since $P$ invertible, but $P^{-1}AP$ makes more sense for computations
\end{frame}


\begin{frame}
\begin{importanttheorem}
$A\in\M_n$ diagonalisable $\iff$ $A$ has $n$ linearly independent eigenvectors
\end{importanttheorem}
\vfill
\begin{corollary}[Sufficient condition for diagonalisability]
$A\in\M_n$ has all its eigenvalues distinct $\implies$ $A$ diagonalisable
\end{corollary}
\vfill
For $P^{-1}AP=D$: in $P$, put the linearly independent eigenvectors as columns and in $D$, the corresponding eigenvalues
\end{frame}


\begin{frame}{Linear combination and span}
	\begin{definition}[Linear combination]
		Let $V$ be a vector space.
		A \defword{linear combination} of a set $\{\bv_1,\ldots,\bv_k\}$ of vectors in $V$ is a \emph{vector}
		\[
		c_1\bv_1+\cdots+c_k\bv_k
		\]
		where $c_1,\ldots,c_k\in\IF$
	\end{definition}
	\vfill
	\begin{definition}[Span]
		The set of all linear combinations of a set of vectors $\bv_1,\ldots,\bv_k$ is the \defword{span} of $\{\bv_1,\ldots,\bv_k\}$,
		\[
		\Span(\bv_1,\ldots,\bv_k)=
		\left\{
		c_1\bv_1+\cdots+c_k\bv_k:c_1,\ldots,c_k\in\IF
		\right\}
		\]
	\end{definition}
\end{frame}


\begin{frame}{Finite/infinite-dimensional vector spaces}
	\begin{theorem}
		The span of a set of vectors in $V$ is the smallest subspace of $V$ containing all the vectors in the set
	\end{theorem}
	\vfill
	\begin{definition}[Set of vectors spanning a space]
		If $\Span(\bv_1,\ldots,\bv_k)=V$, we say $\bv_1,\ldots,\bv_k$ \defword{spans} $V$
	\end{definition}
	\vfill
	\begin{definition}[Dimension of a vector space]
		A vector space $V$ is \defword{finite-dimensional} if some set of vectors in it spans $V$.
		A vector space $V$ is \defword{infinite-dimensional} if it is not finite-dimensional
	\end{definition}
\end{frame}


\begin{frame}{Linear (in)dependence}
	\begin{definition}[Linear independence/Linear dependence]
		A set $\{\bv_1,\ldots,\bv_k\}$ of vectors in a vector space $V$ is \defword{linearly independent} if
		\[
		\left(c_1\bv_1+\cdots+c_k\bv_k=0\right)
		\Leftrightarrow
		\left(c_1=\cdots=c_k=0\right),
		\]
		where $c_1,\ldots,c_k\in\IF$. 
		A set of vectors is \defword{linearly dependent} if it is not linearly independent.
	\end{definition}
	\vfill
	If linearly dependent, assume w.l.o.g. that $c_1\neq 0$, then
	\[
	\bv_1 = -\frac{c_2}{c_1}\bv_2-\cdots-\frac{c_k}{c_1}\bv_k
	\]
	i.e., $\bv_1$ is a linear combination of the other vectors in the set
\end{frame}


\begin{frame}{Basis}
	\begin{definition}[Basis]
		Let $V$ be a vector space. A \defword{basis} of $V$ is a set of vectors in $V$ that is both linearly independent and spanning
	\end{definition}
	\vfill
	\begin{theorem}[Criterion for a basis]
		A set $\{\bv_1,\ldots,\bv_k\}$ of vectors in a vector space $V$ is a basis of $V$ $\iff$ $\forall \bv\in V$, $\bv$ can be written uniquely in the form
		\[
		\bv=c_1\bv_1+\cdots+c_k\bv_k,
		\]
		where $c_1,\ldots,c_k\in\IF$
	\end{theorem}
\end{frame}

\begin{frame}{More on bases}
	\begin{importanttheorem}
		Any two bases of a finite-dimensional vector space have the same number of vectors
	\end{importanttheorem}
	\vfill
	\begin{definition}[Dimension]
		The \defword{dimension} $\dim V$ of a finite-dimensional vector space $V$ is the number of vectors in any basis of the vector space
	\end{definition}
\end{frame}


\begin{frame}{Linear algebra in a nutshell}
\begin{importanttheorem}
\label{th:L04-linalg-in-a-nutshell}
Let $A\in\M_n$. The following statements are equivalent (TFAE)
\begin{enumerate}
\item The matrix $A$ is invertible
\item $\forall\bb\in\IF^n$, $A\bx = \bb$ has a unique solution ($\bx=A^{-1}\bb$)
\item The only solution to $A\bx = \b0$ is the trivial solution $\bx = \b0$
\item $RREF(A)=\II_n$
\item The matrix A is equal to a product of elementary matrices
\item $\forall\bb\in\IF^n$, $A\bx = \bb$ has a solution
\item There is a matrix $B\in\M_n$ such that $AB = \II_n$
\item There is an invertible matrix $B\in\M_n$ such that $AB = \II_n$
\item $\det(A)\neq 0$
\item $0$ is not an eigenvalue of $A$
\end{enumerate}
\end{importanttheorem}
\end{frame}

\begin{frame}{The gradient}
    $f:\IR^n\to\IR$ function of several variables, $\nabla=\left(\frac{\partial}{\partial x_1},\ldots,\frac{\partial}{\partial x_n}\right)$ the gradient operator
    \vfill
    Then
    \[
    \nabla f = \left(
    \frac{\partial}{\partial x_1}f,\ldots,
    \frac{\partial}{\partial x_n}f
    \right)
    \]
    \vfill
    So $\nabla f$ is a \emph{vector-valued} function, $\nabla f:\IR^n\to\IR^n$; also written as
    \[
    \nabla f = f_{x_1}(x_1,\ldots,x_n)\be_1+\cdots f_{x_n}(x_1,\ldots,x_n)\be_n
    \]
    where $f_{x_i}$ is the partial derivative of $f$ with respect to $x_i$ and $\{\be_1,\ldots,\be_n\}$ is the standard basis of $\IR^n$
\end{frame}

%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\begin{frame}{Linearly separable points}
    Let $X_1$ and $X_2$ be two sets of points in $\IR^p$ 
    \vfill
    Then $X_1$ and $X_2$ are \textbf{linearly separable} if there exist $w_{1}, w_{2},..,w_{p}, k\in\IR$ such that 
    \begin{itemize}
        \item every point $x \in X_1$ satisfies $\sum^{p}_{i=1} w_{i}x_{i} > k$ 
        \item every point $x \in X_2$ satisfies $\sum^{p}_{i=1} w_{i}x_{i} < k$
    \end{itemize}
    where $x_{i}$ is the $i$th component of $x$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{Least squares}{FIGS-slides-admin/a_robot_looking_at_a_desolate_tree_in_the_style_of_Magritte.png}


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssubsection{Least squares problem}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}


\begin{frame}{The least squares problem}
\begin{definition}[Least squares solutions]
Consider a collection of points $(x_1,y_1),\ldots,(x_n,y_n)$, a matrix $A\in\M_{mn}$, $\bb\in\IR^m$. A \textbf{least squares solution} of $A\bx=\bb$ is a vector $\tilde \bx\in\IR^n$ s.t.
\[
\forall \bx\in\IR^n,\quad \|\bb-A\tilde\bx\|\leq \|\bb-A\bx\|
\]
\end{definition}
\end{frame}

\begin{frame}{Least squares theorem}
\begin{importanttheorem}[Least squares theorem]\label{th:least_squares}
$A\in\M_{mn}$, $\bb\in\IR^m$. Then
\begin{enumerate}
\item $A\bx=\bb$ always has at least one least squares solution $\tilde\bx$
\item $\tilde\bx$ least squares solution to $A\bx=\bb$ $\iff$ $\tilde\bx$ is a solution to the normal equations $A^TA\tilde\bx = A^T\bb$
\item $A$ has linearly independent columns $\iff$ $A^TA$ invertible.  
\newline In this case, the least squares solution is unique and 
\[
\tilde\bx = \left(A^TA\right)^{-1}A^T\bb
\]
\end{enumerate}
\end{importanttheorem}
\end{frame}


\begin{frame}{Fitting an affine function}
	For a data point $i=1,\ldots,n$
	\[
	\varepsilon_i = y_i-\tilde y_i = y_i - (a+bx_i)
	\]
	So if we write this for all data points,
	\begin{align*}
	\varepsilon_1 &= y_1 - (a+bx_1) \\
	&\;\;\vdots \\
	\varepsilon_n &= y_n - (a+bx_n) \\
	\end{align*}
	In matrix form
	\[
	\be = \bb-A\bx
	\]
	with
	\[
	\be = \begin{pmatrix}
	\varepsilon_1\\ \vdots\\ \varepsilon_n
	\end{pmatrix},
	A=\begin{pmatrix}
	1 & x_1 \\ \vdots & \vdots \\ 1 & x_n
	\end{pmatrix},
	\bx = \begin{pmatrix}
	a\\b
	\end{pmatrix}\textrm{ and }
	\bb = \begin{pmatrix}
	y_1\\ \vdots\\ y_n
	\end{pmatrix}
	\]
\end{frame}

\begin{frame}{Fitting the quadratic}
We have the data points $(x_1,y_1),(x_2,y_2),\ldots,(x_n,y_n)$ and want to fit
\[
y = a_0+a_1x+a_2x^2
\]
At $(x_1,y_1)$,
\[
\tilde y_1 = a_0+a_1x_1+a_2x_1^2
\]
$\vdots$\\
At $(x_n,y_n)$,
\[
\tilde y_n = a_0+a_1x_n+a_2x_n^2
\]
\end{frame}

\begin{frame}
In terms of the error
\begin{align*}
\varepsilon_1 &= y_1-\tilde y_1 = y_1-(a_0+a_1x_1+a_2x_1^2) \\
&\;\;\vdots\\
\varepsilon_n &= y_n-\tilde y_n = y_n-(a_0+a_1x_n+a_2x_n^2)
\end{align*}
i.e.,
\[
\be = \bb-A\bx 
\]
where
\[
\be = \begin{pmatrix}
\varepsilon_1\\ \vdots\\ \varepsilon_n
\end{pmatrix},
A=\begin{pmatrix}
1 & x_1 & x_1^2\\ \vdots & \vdots & \vdots \\ 1 & x_n & x_n^2
\end{pmatrix},
\bx = \begin{pmatrix}
a_0\\a_1\\a_2
\end{pmatrix}\textrm{ and }
\bb = \begin{pmatrix}
y_1\\ \vdots\\ y_n
\end{pmatrix}
\]
\vfill
Theorem~\ref{th:least_squares} applies, with here $A\in\M_{n3}$ and $\bb\in\IR^n$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssubsection{Orthogonality and Gram-Schmidt}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}


\begin{frame}
\begin{definition}[Orthogonal set of vectors]
The set of vectors $\{\bv_1,\ldots,\bv_k\}\in\IR^n$ is an \textbf{orthogonal set} if
\[
\forall i,j=1,\ldots,k,\quad i\neq j \implies \bv_i\bullet\bv_j=0
\]
\end{definition}
\vfill
\begin{definition}[Orthogonal basis]
Let $S$ be a basis of the subspace $W\subset\IR^n$ composed of an orthogonal set of vectors. We say $S$ is an \textbf{orthogonal basis} of $W$
\end{definition}
\end{frame}

\begin{frame}{Orthonormal version of things}
\begin{definition}[Orthonormal set]
The set of vectors $\{\bv_1,\ldots,\bv_k\}\in\IR^n$ is an \textbf{orthonormal set} if it is an orthogonal set and furthermore
\[
\forall i=1,\ldots,k,\quad \|\bv_i\|=1
\]
\end{definition}
\begin{definition}[Orthonormal basis]
A basis of the subspace $W\subset\IR^n$ is an \textbf{orthonormal basis} if the vectors composing it are an orthonormal set
\end{definition}
\vfill
$\{\bv_1,\ldots,\bv_k\}\in\IR^n$ is orthonormal if
\[
\bv_i\bullet\bv_j =
\begin{cases}
1 &\textrm{if }i=j \\
0 &\textrm{otherwise}
\end{cases}
\]
\end{frame}


\begin{frame}{Projections}
	\begin{minordefinition}[Orthogonal projection onto a subspace]
	$W\subset\IR^n$ a subspace and $\{\bu_1,\ldots,\bu_k\}$ an orthogonal basis of $W$. $\forall\bv\in\IR^n$, the \textbf{orthogonal projection} of $\bv$ \textbf{onto} $W$ is
	\[
	\mathsf{proj}_W(\bv) =
	\frac{\bu_1\bullet\bv}{\|\bu_1\|^2}\bu_1
	+\cdots+
	\frac{\bu_k\bullet\bv}{\|\bu_k\|^2}\bu_k
	\]  
	\end{minordefinition}
	\vfill
	\begin{minordefinition}[Component orthogonal to a subspace]
	$W\subset\IR^n$ a subspace and $\{\bu_1,\ldots,\bu_k\}$ an orthogonal basis of $W$. $\forall\bv\in\IR^n$, the \textbf{component} of  $\bv$ \textbf{orthogonal to} W is
	\[
	\mathsf{perp}_W(\bv)=\bv-\mathsf{proj}_W(\bv)
	\]
	\end{minordefinition}	
	\end{frame}
	

\begin{frame}{Gram-Schmidt process}
\begin{importanttheorem}
$W\subset\IR^n$ a subset and $\{\bx_1,\ldots,\bx_k\}$ a basis of $W$. Let
\begin{align*}
\bv_1 &= \bx_1 \\
\bv_2 &= \bx_2 -\frac{\bv_1\bullet\bx_2}{\|\bv_1\|^2}\bv_1 \\
\bv_3 &= \bx_3 -\frac{\bv_1\bullet\bx_3}{\|\bv_1\|^2}\bv_1 -\frac{\bv_2\bullet\bx_3}{\|\bv_2\|^2}\bv_2 \\
&\;\;\vdots & \\
\bv_k &= \bx_k -\frac{\bv_1\bullet\bx_k}{\|\bv_1\|^2}\bv_1 -\cdots-\frac{\bv_{k-1}\bullet\bx_k}{\|\bv_{k-1}\|^2}\bv_{k-1}
\end{align*}
and
\[
W_1=\mathsf{span}(\bx_1),W_2 = \mathsf{span}(\bx_1,\bx_2),\ldots,
W_k = \mathsf{span}(\bx_1,\ldots,\bx_k)
\]
Then $\forall i=1,\ldots,k$, $\{\bv_1,\ldots,\bv_i\}$ orthogonal basis for $W_i$
\end{importanttheorem}
\end{frame}
	
	
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssubsection{The QR decomposition}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}


\begin{frame}
\begin{definition}[Orthogonal matrix]
$Q\in\M_n$ is an \textbf{orthogonal matrix} if its columns form an orthonormal set 
\end{definition}
\vfill
So $Q\in\M_n$ orthogonal if $Q^TQ=\II$, i.e., $Q^T=Q^{-1}$
\vfill
\begin{importanttheorem}[NSC for orthogonality]
$Q\in\M_n$ orthogonal $\iff$ $Q^{-1} = Q^T$
\end{importanttheorem}
\end{frame}


\begin{frame}
\begin{theorem}[Orthogonal matrices ``encode" isometries]
\label{th:TFAE_orthogonal_matrices}
Let $Q\in\M_n$. TFAE
\begin{enumerate}
\item $Q$ orthogonal
\item $\forall\bx\in\IR^n$, $\|Q\bx\|=\|\bx\|$
\item $\forall\bx,\by\in\IR^n$, $Q\bx\bullet Q\by=\bx\bullet\by$
\end{enumerate}
\end{theorem}
\vfill
\begin{importanttheorem}\label{th:properties_orthogonal_matrices}
Let $Q\in\M_n$ be orthogonal. Then
\begin{enumerate}
\item The rows of $Q$ form an orthonormal set
\item $Q^{-1}$ orthogonal
\item $\det Q=\pm 1$
\item $\forall\lambda\in\sigma(Q)$, $|\lambda|=1$
\item If $Q_2\in\M_n$ also orthogonal, then $QQ_2$ orthogonal
\end{enumerate}
\end{importanttheorem}
\end{frame}


\begin{frame}{The QR factorisation}
\begin{importanttheorem}\label{th:QR_factorisation}
Let $A\in\M_{mn}$ with LI columns. Then $A$ can be factored as
\[
A=QR
\]
where $Q\in\M_{mn}$ has orthonormal columns and $R\in\M_n$ is nonsingular upper triangular
\end{importanttheorem}
\vfill
\begin{importanttheorem}[Least squares with QR factorisation]
\label{th:LSQ_with_QR}
$A\in\M_{mn}$ with LI columns, $\bb\in\IR^m$. If $A=QR$ is a QR factorisation of $A$, then the unique least squares solution $\tilde\bx$ of $A\bx=\bb$ is
\[
\tilde\bx = R^{-1}Q^T\bb
\]
\end{importanttheorem}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssubsection{The SVD}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}


\begin{frame}{Singular values}
\begin{definition}[Singular value]
Let $A\in\M_{mn}(\IR)$. The \textbf{singular values} of $A$ are the real numbers 
\[
\sigma_1\geq \sigma_2\geq\cdots\sigma_n\geq 0
\]
that are the square roots of the eigenvalues of $A^TA$
\end{definition}
\end{frame}


\begin{frame}{Singular values are real and nonnegative?}
Recall that $\forall A\in\M_{mn}$, $A^TA$ is symmetric
\vfill
\textbf{Claim 1.} Real symmetric matrices have real eigenvalues
\vfill
\textbf{Claim 2.} For $A\in\M_{mn}(\IR)$, the eigenvalues of $A^TA$ are real and nonnegative
\vfill
\textbf{Claim 3.} For $A\in\M_{mn}(\IR)$, the nonzero eigenvalues of $A^TA$ and $AA^T$ are the same
\end{frame}

\begin{frame}[red]
\textbf{Claim 2.} For $A\in\M_{mn}(\IR)$, the eigenvalues of $A^TA$ are real and nonnegative

\vfill
\textbf{Proof.}
We know that for $A\in\M_{mn}$, $A^TA$ symmetric and from previous claim, if $A\in\M_{mn}(\IR)$, then $A^TA$ is symmetric and real and with real eigenvalues
\vfill
Let $(\lambda,\bv)$ be an eigenpair of $A^TA$, with $\bv$ chosen so that $\|\bv\|=1$
\vfill 
Norms are functions $V\to\IR_+$, so $\|A\bv\|$ and $\|A\bv\|^2$ are $\geq 0$ and thus
\begin{align*}
0\leq \|A\bv\|^2 &= (A\bv)\bullet(A\bv) = (A\bv)^T(A\bv) \\
&= \bv^TA^TA\bv = \bv^T(A^TA\bv) = \bv^T(\lambda\bv) \\
&= \lambda(\bv^T\bv) = \lambda(\bv\bullet\bv) = \lambda\|\bv\|^2 \\
&= \lambda\hfill\qedhere
\end{align*}
\end{frame}

\begin{frame}{The singular value decomposition (SVD)}
\begin{importanttheorem}[SVD]\label{th:SVD}
$A\in\M_{mn}$ with singular values $\sigma_1\geq\cdots\geq\sigma_r>0$ and $\sigma_{r+1}=\cdots=\sigma_n=0$
\vskip0.5cm
Then there exists $U\in\M_m$ orthogonal, $V\in\M_n$ orthogonal and a block matrix $\Sigma\in\M_{mn}$ taking the form
\[
\Sigma=
\begin{pmatrix}
D & 0_{r,n-r} \\
0_{m-r,r} & 0_{m-r,n-r}
\end{pmatrix}
\]
where 
\[
D = \mathsf{diag}(\sigma_1,\ldots,\sigma_r)\in\M_r
\] 
such that
\[
A=U\Sigma V^T
\]
\end{importanttheorem}
\end{frame}


\begin{frame}
\begin{definition}
We call a factorisation as in Theorem~\ref{th:SVD} the \textbf{singular value decomposition} of $A$. The columns of $U$ and $V$ are, respectively, the \textbf{left} and \textbf{right singular vectors} of $A$
\end{definition}
\vfill
$U$ and $V^T$ are \emph{rotation} or \emph{reflection} matrices, $\Sigma$ is a \emph{scaling} matrix
\vfill
$U\in\M_m$ orthogonal matrix with columns the eigenvectors of $AA^T$
\vfill
$V\in\M_n$ orthogonal matrix with columns the eigenvectors of $A^TA$
\end{frame}


\begin{frame}{Outer product form of the SVD}
\begin{theorem}[Outer product form of the SVD]\label{th:SVD_outer_product_form}
$A\in\M_{mn}$ with singular values $\sigma_1\geq\cdots\geq\sigma_r>0$ and $\sigma_{r+1}=\cdots=\sigma_n=0$, $\bu_1,\ldots,\bu_r$ and $\bv_1,\ldots,\bv_r$, respectively, left and right singular vectors of $A$ corresponding to these singular values
\vskip0.5cm
Then 
\begin{equation}\label{eq:outer-product-form-SVD}
A=\sigma_1\bu_1\bv_1^T+\cdots+\sigma_r\bu_r\bv_r^T
\end{equation}
\end{theorem}
\end{frame}

\begin{frame}{Computing the SVD (case of $\neq$ eigenvalues)}
To compute the SVD, we use the following result
\vfill
\begin{theorem}\label{th:eigenvectors_of_symmetric_are_orthogonal}
Let $A\in\M_n$ symmetric, $(\lambda_1,\bu_1)$ and $(\lambda_2,\bu_2)$ be eigenpairs, $\lambda_1\neq\lambda_2$. Then $\bu_1\bullet\bu_2=0$
\end{theorem}
\end{frame}

\begin{frame}[red]{Proof of Theorem~\ref{th:eigenvectors_of_symmetric_are_orthogonal}}
$A\in\M_n$ symmetric, $(\lambda_1,\bu_1)$ and $(\lambda_2,\bu_2)$ eigenpairs with $\lambda_1\neq\lambda_2$
\begin{align*}
\lambda_1(\bv_1\bullet\bv_2) 
&= (\lambda_1\bv_1)\bullet\bv_2 \\
&= A\bv_1\bullet\bv_2 \\
&= (A\bv_1)^T\bv_2 \\
&= \bv_1^TA^T\bv_2 \\
&= \bv_1^T(A\bv_2)  \qquad\textrm{[$A$ symmetric so $A^T=A$]} \\
&= \bv_1^T(\lambda_2\bv_2) \\
&= \lambda_2(\bv_1^T\bv_2) \\
&= \lambda_2(\bv_1\bullet\bv_2)
\end{align*}
\vfill
So $(\lambda_1-\lambda_2)(\bv_1\bullet\bv_2)=0$. But $\lambda_1\neq\lambda_2$, so $\bv_1\bullet\bv_2=0$\hfill\qed
\end{frame}



\begin{frame}{Pseudoinverse of a matrix}
\begin{definition}[Pseudoinverse]
$A=U\Sigma V^T$ an SVD for $A\in\M_{mn}$, where 
\[
\Sigma = \begin{pmatrix}
D & 0 \\ 0 & 0
\end{pmatrix},
\textrm{ with }
D=\mathsf{diag}(\sigma_1,\ldots,\sigma_r)
\]
($D$ contains the nonzero singular values of $A$ ordered as usual)
\vskip0.5cm
The \textbf{pseudoinverse} (or \textbf{Moore-Penrose inverse}) of $A$ is $A^+\in\M_{nm}$ given by
\[
A^+ = V\Sigma^+ U^T
\]
with
\[
\Sigma^+ =
\begin{pmatrix}
D^{-1} & 0 \\ 0 & 0
\end{pmatrix}\in\M_{nm}
\]
\end{definition}
\end{frame}



\begin{frame}{Least squares revisited}
\begin{importanttheorem}
Let $A\in\M_{mn}$, $\bx\in\IR^n$ and $\bb\in\IR^m$. The least squares problem $A\bx=\bb$ has a unique least squares solution $\tilde\bx$ of \emph{minimal length} (closest to the origin) given by
\[
\tilde\bx = A^+\bb
\]
where $A^+$ is the \emph{pseudoinverse} of $A$
\end{importanttheorem}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{PCA}{FIGS-slides-admin/a_robot_looking_at_a_desolate_tree_in_the_style_of_Magritte.png}


\begin{frame}{Change of basis}
\begin{definition}[Change of basis matrix]
$\B=\{\bu_1,\ldots,\bu_n\}$ and $\C=\{\bv_1,\ldots,\bv_n\}$ bases of vector space $V$
\vfill
The \textbf{change of basis matrix} $P_{\C\leftarrow\B}\in\M_n$,
\[
P_{\C\leftarrow\B}
=\left[
[\bu_1]_\C \cdots [\bu_n]_\C
\right]
\]
has columns the coordinate vectors $[\bu_1]_\C,\ldots,[\bu_n]_\C$ of vectors in $\B$ with respect to $\C$
\end{definition}
\vfill
\begin{theorem}
$\B=\{\bu_1,\ldots,\bu_n\}$ and $\C=\{\bv_1,\ldots,\bv_n\}$ bases of vector space $V$ and $P_{\C\leftarrow\B}$ a change of basis matrix from $\B$ to $\C$
\begin{enumerate}
\item $\forall\bx\in V$, $P_{\C\leftarrow\B}[\bx]_\B = [\bx]_\C$
\item $P_{\C\leftarrow\B}$ s.t. $\forall\bx\in V$, $P_{\C\leftarrow\B}[\bx]_\B = [\bx]_\C$ is \textbf{unique}
\item $P_{\C\leftarrow\B}$ invertible and $P_{\C\leftarrow\B}^{-1}=P_{\B\leftarrow\C}$
\end{enumerate}
\end{theorem}
\end{frame}


\begin{frame}{Row-reduction method for changing bases}
\begin{importanttheorem}
\label{th:change-basis-construction}
$\B=\{\bu_1,\ldots,\bu_n\}$ and $\C=\{\bv_1,\ldots,\bv_n\}$ bases of vector space $V$. Let $\E$ be any basis for $V$,
\[
B = [[\bu_1]_\E,\ldots,[\bu_n]_\E] 
\textrm{ and }
C = [[\bv_1]_\E,\ldots,[\bv_n]_\E] 
\]
and let $[C|B]$ be the augmented matrix constructed using $C$ and $B$. Then
\[
RREF\left([C|B]\right)
=[\II|P_{\C\leftarrow\B}]
\]
\end{importanttheorem}
\vfill
If working in $\IR^n$, this is quite useful with $\E$ the standard basis of $\IR^n$ (it does not matter if $\B=\E$)
\end{frame}


\begin{frame}
\begin{definition}[Variance]
Let $X$ be a random variable. The \textbf{variance} of $X$ is given by
\[
\Var X = E\left[\left(X-E(X)\right)^2\right]
\]
where $E$ is the expected value
\end{definition}
\vfill
\begin{definition}[Covariance]
Let $X,Y$ be jointly distributed random variables. The \textbf{covariance} of $X$ and $Y$ is given by
\[
\cov (X,Y) = E\left[\left(X-E(X)\right)\left(Y-E(Y)\right)\right]
\]
\end{definition}
\vfill
Note that $\cov(X,X)=E\left[\left(X-E(X)\right)^2\right] = \Var X$
\end{frame}

\begin{frame}
\begin{definition}[Unbiased estimators of the mean and variance]
Let $x_1,\ldots,x_n$ be data points (the \emph{sample}) and 
\[
\bar x = \frac 1n \sum_{i=1}^n x_i
\]
be the \textbf{mean} of the data. An unbiased estimator of the variance of the sample is
\[
\sigma^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i-\bar x)^2
\]
\end{definition}
\end{frame}

\begin{frame}
\begin{definition}[Unbiased estimator of the covariance]
Let $(x_1,y_1),\ldots,(x_n,y_n)$ be data points,
\[
\bar x = \frac 1n \sum_{i=1}^n x_i
\textrm{ and }
\bar y = \frac 1n \sum_{i=1}^n y_i
\]
be the means of the data. An estimator of the covariance of the sample is
\[
\cov(x,y) = \frac{1}{n}\sum_{i=1}^n (x_i-\bar x)(y_i-\bar y)
\]
\end{definition}
\end{frame}

\begin{frame}{The covariance matrix (we usually have more than 2 variables)}
\begin{definition}
Suppose $p$ random variables $X_1,\ldots,X_p$. Then the covariance matrix is the symmetric matrix
\[
\begin{pmatrix}
\Var X_1 & \cov(X_1,X_2) & \cdots & \cov(X_1,X_p) \\
\cov(X_1,X_2) & \Var X_2 & \cdots & \cov(X_2,X_p) \\
\vdots & \vdots & & \vdots \\
\cov(X_1,X_p) & \cov(X_2,X_p) & \cdots & \Var X_p 
\end{pmatrix}
\]
\end{definition}
\end{frame}

\begin{frame}{Picking the right eigenvalue}
$(\lambda,\bm{\alpha}_1)$ eigenpair of $\Sigma$, with $\bm{\alpha}_1$ having unit length
\vfill
But which $\lambda$ to choose?
\vfill
Recall that we want $\Var \bm{\alpha}_1^T\bx=\bm{\alpha}_1^T\Sigma\bm{\alpha}_1$ maximal
\vfill
We have
\[
\Var \bm{\alpha}_1^T\bx 
= \bm{\alpha}_1^T\Sigma\bm{\alpha}_1 
= \bm{\alpha}_1^T(\Sigma\bm{\alpha}_1) 
= \bm{\alpha}_1^T(\lambda\bm{\alpha}_1) 
= \lambda(\bm{\alpha}_1^T\bm{\alpha}_1) = \lambda
\]
\vfill
$\implies$ we pick $\lambda=\lambda_1$, the largest eigenvalue (covariance matrix symmetric so eigenvalues real)
\end{frame}


\begin{frame}{What we have this far..}
The first principal component is $\bm{\alpha}_1^T\bx$ and has variance $\lambda_1$, where $\lambda_1$ the largest eigenvalue of $\Sigma$ and $\bm{\alpha}_1$ an associated eigenvector with $\|\bm{\alpha}_1\|=1$
\vfill
We want the second principal component to be \emph{uncorrelated} with $\bm{\alpha}_1^T\bx$ and to have maximum variance $\Var \bm{\alpha}_2^T\bx=\bm{\alpha}_2^T\Sigma\bm{\alpha}_2$, under the constraint that $\|\bm{\alpha}_2\|=1$
\vfill
$\bm{\alpha}_2^T\bx$ uncorrelated to $\bm{\alpha}_1^T\bx$ if $\cov(\bm{\alpha}_1^T\bx,\bm{\alpha}_2^T\bx)=0$
\end{frame}

\begin{frame}
We have
\begin{align*}
\cov(\bm{\alpha}_1^T\bx,\bm{\alpha}_2^T\bx) &= 
\bm{\alpha}_1^T\Sigma\bm{\alpha}_2 \\
&= \bm{\alpha}_2^T\Sigma^T\bm{\alpha}_1 \\
&= \bm{\alpha}_2^T\Sigma\bm{\alpha}_1 \quad\textrm{[$\Sigma$ symmetric]} \\
&= \bm{\alpha}_2^T(\lambda_1\bm{\alpha}_1) \\
&= \lambda \bm{\alpha}_2^T\bm{\alpha}_1
\end{align*}
\vfill
So $\bm{\alpha}_2^T\bx$ uncorrelated to $\bm{\alpha}_1^T\bx$ if $\bm{\alpha}_1\perp\bm{\alpha}_2$
\vfill
This is beginning to sound a lot like Gram-Schmidt, no?
\end{frame}

\begin{frame}{In short}
Take whatever covariance matrix is available to you (known $\Sigma$ or sample $S_X$) -- assume sample from now on for simplicity
\vfill
For $i=1,\ldots,p$, the $i$th principal component is
\[
z_i = \bv_i^T\bx
\]
where $\bv_i$ eigenvector of $S_X$ associated to the $i$th largest eigenvalue $\lambda_i$
\vfill
If $\bv_i$ is normalised, then $\lambda_i=\Var z_k$
\end{frame}


\begin{frame}{Covariance matrix}
$\Sigma$ the covariance matrix of the random variable, $S_X$ the sample covariance matrix
\vfill
$X\in\M_{mp}$ the data, then the (sample) covariance matrix $S_X$ takes the form
\[
S_X = \frac{1}{n-1}X^TX
\]
where the data is centred!
\vfill
Sometimes you will see $S_X=1/(n-1)XX^T$. This is for matrices with observations in columns and variables in rows. Just remember that you want the covariance matrix to have size the number of variables, not observations, this will give you the order in which to take the product
\end{frame}


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\Ssection{Markov chains}{FIGS-slides-admin/Gemini_Generated_Image_Kiki-bof.png}

\begin{frame}
\begin{definition}[Markov chain]
An experiment with finite number of possible outcomes $S_1,\ldots,S_n$ is repeated. The sequence of outcomes is a \textbf{Markov chain} if there is a set of $n^2$ numbers $\{p_{ij}\}$ such that the conditional probability of outcome $S_i$ on any experiment given outcome $S_j$ on the previous experiment is $p_{ij}$, i.e., for $1\leq i,j\leq n$, $t=1,\ldots$,
\[
	p_{ij}=\IP(S_i\textrm{ on experiment }t+1\;|\;
	S_j\textrm{ on experiment }t)	
\]
Outcomes $S_1,\ldots,S_n$ are \textbf{states} and $p_{ij}$ are \textbf{transition probabilities}. $P=[p_{ij}]$ the \textbf{transition matrix}
\end{definition}
\vfill
In the following, we often write
\[
\IP(S_i\textrm{ on experiment }t+1\;|\;
	S_j\textrm{ on experiment }t)	\text{ as }\IP(S_i(t+1)\;|\;S_j(t))
\]
\end{frame}


\begin{frame} 
The matrix 
\[
P=
\begin{pmatrix}
p_{11} & p_{12} & \cdots & p_{1r} \\
p_{21} & p_{22} & \cdots & p_{2r} \\
&&& \\
p_{r1} & p_{r2} & \cdots & p_{rr}
\end{pmatrix}
\]
has
\begin{itemize}
\item entries that are probabilities, i.e., $0\leq p_{ij}\leq 1$
\item column sum 1, which we write
\[
\sum_{i=1}^n p_{ij}=1,\quad j=1,\ldots,n
\]
or, using the notation $\nbOne^T=(1,\ldots,1)$,
\[
\nbOne^TP=\nbOne^T
\]
\end{itemize}
\end{frame}

\begin{frame}
In matrix form
\[
p(t+1)=Pp(t), \quad n=1,2,3,\dots
\]
where $p(t)=(p_1(t),p_{2}(t),\dots , p_n(t))^T$ is a probability vector and $P=(p_{ij})$ is an $n\times n$ \emph{transition matrix},
\[
P=
\begin{pmatrix}
p_{11} & p_{12} & \cdots & p_{1r} \\
p_{21} & p_{22} & \cdots & p_{2r} \\
&&& \\
p_{r1} & p_{r2} & \cdots & p_{rr}
\end{pmatrix}
\]
\end{frame}


\begin{frame}{Stochastic matrices}
\begin{definition}[Stochastic matrix]
The nonnegative $n\times n$ matrix $M$ is \textbf{row-stochastic} (resp. \textbf{column-stochastic}) if $\sum_{j=1}^na_{ij}=1$ for all $i=1,\dots,n$ (resp. $\sum_{i=1}^na_{ij}=1$ for all $j=1,\dots,n$)
\end{definition}
\vfill
We often say \textbf{stochastic} and let the context determine whether we mean row- or column-stochastic
\vfill
If it is both row- and column-stochastic, the matrix is \textbf{doubly stochastic}
\vfill
\begin{theorem}\label{th:spectrum_stochastic_matrix}
Let $M\in\M_n$ be a stochastic matrix. Then all eigenvalues $\lambda$ of $M$ are such that $|\lambda|\leq 1$.
\end{theorem}
\end{frame}

\begin{frame}
\begin{importanttheorem}
\label{th:one-is-evalue-stochastic-matrix}
Let $M\in\M_n$ be a stochastic matrix. $\lambda =1$ is an eigenvalue of $M$. 
If $M$ is row-stochastic, the eigenvalue 1 is associated to the column vector of ones (a right eigenvector of $M$); if $M$ is column-stochastic, the eigenvalue 1 is associated to the row vector of ones (a left eigenvector of $M$)
\end{importanttheorem}
\end{frame}

\begin{frame}[red]{Proof of Theorem~\ref{th:one-is-evalue-stochastic-matrix}}
Suppose $M\in\M_n$ is row-stochastic. One way to write the requirement that each row sum equals 1 is as 
\begin{equation}\label{eq:M1equal1}
M\mathbf{1}=\mathbf{1}
\end{equation}
where $\mathbf{1}=(1,\ldots,1)\in\IC^n$ is a column vector
\vfill
If $M\in\M_n$, then the eigenpair equation takes the form
\[
M\bv=\lambda\bv,\quad \bv\neq\b0
\]
So, in \eqref{eq:M1equal1}, $\bv=\mathbf{1}$ and $\lambda=1$
\vfill
This works the same way for a column-stochastic matrix, except that here the relation is $\mathbf{1}M=\mathbf{1}$ with $\mathbf{1}$ a row vector and the (left)eigenpair relation is $\bv^TM=\lambda\bv^T$ with $\bv^T$ a row vector
\end{frame}

\begin{frame}{Long time behaviour}
Let $p(0)$ be the initial distribution vector. Then
\begin{align*}
p(1) &= Pp(0) \\
p(2) &= Pp(1) \\
&= P\left(Pp(0)\right) \\
&= P^2p(0)
\end{align*}
\vfill
Continuing, we get, for any $t$,
\[
p(t)=P^tp(0)
\]
\vfill
Therefore, 
\[
\lim_{t\rightarrow +\infty}p(t) =
\lim_{t\rightarrow +\infty}P^tp(0) =
\left(\lim_{t\rightarrow +\infty}P^t\right)
p(0)
\]
if this limit exists
\end{frame}


\begin{frame}{The matrix $P^t$}
\begin{theorem}
If $M,N$ are nonsingular stochastic matrices, then $MN$ is a stochastic matrix
\end{theorem}
\vfill
\begin{corollary}
If $M$ is a nonsingular stochastic matrix, then for any $k\in\IN$, $M^k$ is a stochastic matrix
\end{corollary}
\vfill
So $P^t$ is stochastic
\end{frame}


\begin{frame}{Regular Markov chains}
\begin{definition}[Regular Markov chain]
A \textbf{regular} Markov chain has $P^k$ (entry-wise) positive for some integer $k>0$, i.e., $P^k$ has only positive entries
\end{definition}
\vfill
\begin{definition}[Primitive matrix]
A nonnegative matrix $M$ is \textbf{primitive} if, and only if, there is an integer $k>0$ such that $M^k$ is positive.
\end{definition}
\vfill
\begin{theorem}
Markov chain regular $\iff$ transition matrix $P$ primitive
\end{theorem}
\end{frame}

\begin{frame}{What is a directed graph?}
\begin{definition}[Digraph]
A \defword{directed graph} (or \textbf{digraph}) $G$ is a pair $(V, A)$ where:
    \begin{itemize}
        \item $V$ is a finite set of elements called \defword{vertices} or \textbf{nodes}
        \item $A \subseteq V \times V$ is a set of ordered pairs of vertices called \defword{arcs} or \textbf{directed edges}
    \end{itemize}
\end{definition}
    \vfill
\begin{definition}[Arc]
    An \defword{arc} $a = (u, v) \in A$ represents a connection \textbf{from} vertex $u$ \textbf{to} vertex $v$
    \begin{itemize}
        \item $u$ is the \textbf{tail} of the arc
        \item $v$ is the \textbf{head} of the arc
    \end{itemize}
\end{definition}
\end{frame}


\begin{frame}
\begin{definition}[Reducible/irrecible matrix]
A matrix $M\in\M_n$ is \defword{reducible} if there exists a permutation matrix $P$ such that
\[
P^TMP=
\begin{pmatrix}
P & Q \\ \b0 & R
\end{pmatrix},
\]
i.e., $M$ is similar to a block upper triangular matrix. The matrix $M$ is \defword{irreducible} if no such matrix exists
\end{definition}
\vfill
\begin{definition}[Strongly connected digraph]
A digraph $\G=(V,A)$ is \defword{strongly connected} if for any pair of vertices $u,v\in V$, there is a directed path from $u$ to $v$
\end{definition}
\vfill
\begin{importanttheorem}
$P\in\M_n$ irreducible $\iff$ $\G(P)$ strongly connected
\end{importanttheorem}
\end{frame}

\begin{frame}{A sufficient condition for primitivity}
\begin{theorem}
Let $M\in\M_n$ be a nonnegative matrix. If $\G(M)$ is strongly connected and at least one of the diagonal entries $m_{ii}$ of $M$ is positive, then $M$ is primitive
\end{theorem}
\end{frame}

\begin{frame}{Behaviour of a regular MC}
\begin{theorem}
If $P$ is the transition matrix of a regular Markov chain, then
\begin{enumerate}
\item the powers $P^t$ approach a stochastic matrix $W$
\item each column of $W$ is the same (column) vector $w=(w_1,\ldots,w_n)^T$
\item the components of $w$ are positive
\end{enumerate}
\end{theorem}
\vfill
So if the Markov chain is regular
\[
\lim_{t\rightarrow +\infty}p(t)=\lim_{t\rightarrow +\infty}P^tp(0)
=Wp(0)
\]
\end{frame}


\begin{frame}{Computing $W$}
Recall that since $P$ is a stochastic matrix, 1 is an eigenvalue of $P$. As $P$ is column stochastic, 1 is associated to the left (row) eigenvector $\11$
\vfill
Now, if $\bp(t)$ converges, then $\bp(t+1)=P\bp(t)$ at the limit, so $\bw=\lim_{t\to\infty}\bp(t)$ is a \textbf{fixed point} of the system. Replacing $\bp$ with its limit, we have
\[
\bw=P\bw
\]
\vfill
Solving for $\bw$ thus amounts to finding $\bw$ as a (right) eigenvector corresponding to the eigenvalue 1
\end{frame}

\begin{frame}{Remember to normalise}
$\bw$ might have to be normalized since you want a probability vector
\vfill
Check that the norm $\|\bw\|_1$ defined by
\[
\|\bw\|_1=|w_1|+\cdots+|w_n|=w_1+\cdots+w_n
\]
(since $\bw\geq\b0$) is equal to one
\vfill
If not, use
\[
\tilde\bw = \frac{\bw}{\|\bw\|_1}
\]
\end{frame}

\begin{frame}{Absorbing Markov chains}
\begin{definition}[Absorbing state]
A state $S_i$ in a Markov chain is \textbf{absorbing} if whenever it occurs on the $t^{th}$ generation of the experiment, it then occurs on every subsequent step. In other words, $S_i$ is absorbing if $p_{ii}=1$ and $p_{ij}=0$ for $i\neq j$
\end{definition}
\vfill
\begin{definition}[Absorbing chain]
A Markov chain is \textbf{absorbing} if it has at least one absorbing state, and if from every state it is possible to go to an absorbing state.
In an absorbing Markov chain, a state that is not absorbing is called \textbf{transient}
\end{definition}
\end{frame}


\begin{frame}{Questions about absorbing chains}
\begin{enumerate}
\item Does the process eventually reach an absorbing state?
\item What is the average number of steps spent in a transient state, if starting in a transient state?
\item What is the average number of steps before entering an absorbing state?
\item What is the probability of being absorbed by a given absorbing state, when there are more than one, when starting in a given transient state?
\end{enumerate}
\end{frame}

\begin{frame}
The answer to the first question (``Does the process eventually reach an absorbing state?'') is given by the following result
\vfill
\begin{theorem}
In an absorbing Markov chain, the probability of reaching an absorbing state is 1
\end{theorem}
\end{frame}

\begin{frame}
To answer the other questions, write the transition matrix in \textbf{standard} form
\vfill
For an absorbing chain with $k$ absorbing states and $r-k$ transient states, write transition matrix as
\[
P=\begin{pmatrix}
\mathbb{I}_k & R \\
\b0 & Q
\end{pmatrix}
\]
with following meaning
\begin{center}\footnotesize
\begin{tabular}{ccc}
& Absorbing states & Transient states \\
Absorbing states & $\mathbb{I}_k$ & $R$ \\
Transient states & $\b0$ & $Q$
\end{tabular}
\end{center}
with $\mathbb{I}_k$ the $k\times k$ identity matrix, $\mathbf{0}$ an $(r-k)\times k$ matrix of zeros, $R$ an $k\times (r-k)$ matrix and $Q$ an $(r-k)\times(r-k)$ matrix.
The matrix $\mathbb{I}_{r-k}-Q$ is invertible. Let
\begin{itemize}
\item $N=(\mathbb{I}_{r-k}-Q)^{-1}$ the \textbf{fundamental matrix} of the MC
\item $T_i$ sum of the entries on column $i$ of $N$
\item $B=RN$
\end{itemize}
\end{frame}

\begin{frame}
Answers to our remaining questions:
\vfill
\begin{enumerate}
\setcounter{enumi}{1}
\item $N_{ij}$ average number of times the process is in the $i$th transient state if it starts in the $j$th transient state
\vfill
\item $T_i$ average number of steps before the process enters an absorbing state if it starts in the $i$th transient state
\vfill
\item $B_{ij}$ probability of eventually entering the $i$th absorbing state if the process starts in the $j$th transient state
\end{enumerate}
\vfill
\end{frame}


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\Ssection{Graph theory}{FIGS-slides-admin/Gemini_Generated_Image_Kiki-bof.png}


\begin{frame}\frametitle{Binary relation}
	\begin{definition}[Binary relation]
	\begin{itemize}
	\item A \defword{binary relation} is an arbitrary association of elements of one set with elements of another (maybe the same) set
	\item  A binary relation over the sets $X$ and $Y$ is defined as a subset of the Cartesian product $X\times Y =\{(x,y)| x\in X , y\in Y\}$
	\item $(x,y)\in R$ is read ``$x$ is $R$-related to $y$'' and is denoted $xRy$
	\item If $(x,y)\not\in R$, we write ``not $x R y$'' or $x\cancel{R} y$
	\end{itemize}
	\end{definition}
\end{frame}
	
\begin{frame}
	\begin{definition}[Properties of binary relations]
		A binary relation $R$ over a set $X$ is
	\begin{itemize}
	\item \defword{Reflexive} if $\forall x\in X$, $xRx$
	\item \defword{Irreflexive} if there does not exist $x\in X$ such that $xRx$
	\item \defword{Symmetric} if $xRy \Rightarrow yRx$
	\item \defword{Asymmetric} if $xRy \Rightarrow $ $y\cancel R x$
	\item \defword{Antisymmetric} if $xRy$ and $yRx$ $\Rightarrow$ $x=y$
	\item \defword{Transitive} if $xRy$ and $yRz$ $\Rightarrow$  $xRz$
	\item \defword{Total} (or \defword{complete}) if $\forall x, y\in X$, $x R y$ or $y R x$
	\end{itemize}
	\end{definition}
\end{frame}
	
	
\begin{frame} 
	\begin{definition}[Equivalence relation]
	 A relation that is reflexive ($\forall x\in X$, $xRx$), symmetric ($xRy \Rightarrow yRx$) and transitive ($xRy$ and $yRz$ $\Rightarrow$  $xRz$) is an \defword{equivalence relation}
	\end{definition}
	\vfill
	\begin{minordefinition}[Partial order]
	 A relation that is reflexive ($\forall x\in X$, $xRx$), antisymmetric ($xRy$ and $yRx$ $\Rightarrow$ $x=y$) and transitive ($xRy$ and $yRz$ $\Rightarrow$  $xRz$) is a \defword{partial order}
	\end{minordefinition}
	\vfill
	\begin{minordefinition}[Total order]
	A partial order that is total ($\forall x, y\in X$, $x R y$ or $y R x$) is a \defword{total order}
	\end{minordefinition}
\end{frame}
	



\begin{frame}\frametitle{Graph, vertex and edge} 
	\begin{definition}[Graph]
	An \defword{undirected graph} is a pair $G=(V,E)$ of sets such that
	\begin{itemize}
	\item $V$ is a set of points:  $V=\{v_1,\ldots,v_p\}$
	\item $E$ is a set of 2-element subsets of $V$: $E=\{\{v_i,v_j\},\{v_i,v_k\},\ldots,\{v_n,v_p\}\}$ or $E=\{v_iv_j,v_iv_k,\ldots,v_nv_p\}$
	\end{itemize}
	\end{definition}
	\begin{definition}[Vertex]
	The elements of $V$ are the \defword{vertices} (or nodes, or points) of the graph $G$.
	$V$ (or $V(G)$) is the vertex set of the graph $G$
	\end{definition}
	\begin{definition}[Edge]
	The elements of $E$ are the \defword{edges} (or lines) of the graph $G$.
	$E$ (or $E(G)$) is the edge set of the graph $G$
	\end{definition}
\end{frame}


\begin{frame}\frametitle{Order and Size}
	\begin{definition}[Order of a graph]
	The number of vertices in $G$ is the \defword{order} of $G$. Using the notation $|V(G)|$ for the \emph{cardinality} of $V(G)$,
	$$|V(G)|=\textrm{order of G}$$
	\end{definition}
	\vfill
	\begin{definition}[Size of a graph]
	The number of edges in $G$ is the \defword{size} of $G$,
	$$|E(G)|=\textrm{size of G}$$
	\end{definition}
	\vfill
	\begin{itemize}
	\item A graph having order $p$ and size $q$ is called a $(p,q)-$graph
	\item A graph is finite if $|V(G)|<\infty$
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Incident -- Adjacent}
	\begin{definition}[Incident]
	\begin{itemize}
	\item A vertex $v$ is \defword{incident} with an edge $e$ if $v\in e$; then $e$ is an edge at $v$
	\item If $e=uv\in E(G)$, then $u$ and $v$ are each incident with $e$
	\item The two vertices incident with an edge are its ends
	\item An edge $e=uv$ is incident with both vertices $u$ and $v$
	\end{itemize}
	\end{definition}
	\vfill
	\begin{definition}[Adjacent]
	\begin{itemize}
	\item Two vertices $u$ and $v$ are \defword{adjacent} in a graph $G$ if $uv\in E(G)$
	\item If $uv$ and $uw$ are distinct edges (i.e. $v\not=w$) of a graph $G$, then $uv$ and $uw$ are adjacent edges
	\end{itemize}
	\end{definition}
\end{frame}


\begin{frame}
	\begin{definition}[Multiple edge]
	\defword{Multiple edges} are two or more edges connecting the same two vertices within a multigraph
	\end{definition}
	\vfill
	\begin{definition}[Loop]
	A \defword{loop} is an edge with both the same ends; \emph{e.g.} $\{u,u\}$ is a loop
	\end{definition}
	\vfill
	\begin{definition}[Simple graph]
		A \defword{simple graph} is a graph which contains no loops or multiple edges
	\end{definition}
	\vfill
	\begin{definition}[Multigraph]
		A \defword{multigraph} is a graph which can contain multiple edges or loops
	\end{definition}
\end{frame}


\begin{frame}
	\begin{definition}[Degree of a vertex]
	Let $v$ be a vertex of $G=(V,E)$.
	\begin{itemize}
	\item The number of edges of $G$ incident with $v$ is the \defword{degree} of $v$ in $G$
	\item The degree of $v$ in $G$ is noted $d_G(v)$ or $deg_G(v)$
	\end{itemize}
	\end{definition}
	\vfill
	\begin{importanttheorem}\label{th:sum-degrees}
	Let $G$ be a $(p,q)-$graph with vertices $v_1$, $\dots$, $v_p$, then
	\[
		\sum_{i=1}^{p}d_G(v_i)=2q
	\]
	\end{importanttheorem}
\end{frame}


\begin{frame}
\begin{definition}[{Odd vertex}]
A vertex is an \defword{odd vertex} if its degree is odd
\end{definition}
\vfill
\begin{importanttheorem}\label{th:even-nb-odd-vertices}
Every graph contains an even number of odd vertices
\end{importanttheorem}
\end{frame}

\begin{frame} \frametitle{Isomorphic graphs} 
\begin{minordefinition}[Isomorphic graphs]
Let $G_1=(V(G_1),E(G_1))$ and $G_2=(V(G_2),E(G_2))$ be two graphs.
$G_1$ and $G_2$ are \defword{isomorphic} if there exists an isomorphism $\phi$ from $G_1$ to $G_2$, that is defined as an injective mapping $\phi:\; V(G_1) \rightarrow V(G_2)$ such that two vertices $u_1$ and $v_1$ are adjacent in $G_1$ $\iff$ the vertices $\phi(u_1)$ and $\phi(v_1)$ are adjacent in $G_2$
\end{minordefinition}
\end{frame}
 
 
 
\begin{frame}
If $\phi$ is an isomorphism from $G_1$ to $G_2$, then the inverse mapping $\phi ^{-1}$ from $V(G_2)$ to $V(G_1)$ also satisfies the definition of an isomorphism.
As a consequence, if $G_1$ and $G_2$ are isomorphic graphs, then
\begin{itemize}
\item $G_1$ is isomorphic to $G_2$
\item $G_2$ is isomorphic to $G_1$
\end{itemize}
\vfill
\begin{theorem}
The relation ``is isomorphic to'' is an equivalence relation on the set of all graphs
\end{theorem}
\vfill
\begin{theorem}
If $G_1$ and $G_2$ are isomorphic graphs, then the degrees of vertices of $G_1$ are exactly the degrees of vertices of $G_2$
\end{theorem}
\end{frame}

\begin{frame}\frametitle{Subgraph}
\begin{definition}[Subgraph]
Let $G=(V,E)$ be a graph.
A graph $H=(V(H),E(H))$ is a \defword{subgraph} of $G$ if $V(H)\subseteq V$ and $E(H)\subseteq E$
\end{definition}
\end{frame}

\begin{frame}
Let $G_1=(V_1,E_1)$ and $G_2=(V_2,E_2)$ be two graphs
\begin{definition}[{Union of $G_1$ and $G_2$}]
$G_1\cup G_2=(V_1\cup V_2,E_1\cup E_2)$
\end{definition}
\begin{definition}[{Intersection of $G_1$ and $G_2$}]
$G_1\cap G_2=(V_1\cap V_2,E_1\cap E_2)$
\end{definition}
\begin{definition}[{Disjoint graphs}]
If $G_1\cap G_2=(\emptyset,\emptyset)= \emptyset$ (empty graph) then $G_1$ and $G_2$ are \defword{disjoint}
\end{definition}
\begin{definition}[{Complement of $G_1$}]
The \defword{complement} $\bar G_1$ of $G_1$ is the graph on $V_1$, with the edge set $E(\bar G_1)=[V_1]^2\backslash E_1$ ($e\in E(\bar G_1)$ $\iff$ $e\not \in E_1$)
\end{definition}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\Ssubsection{Undirected graphs}{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}

\begin{frame}\frametitle{Connected vertices and graph, components}
\begin{definition}[{Connected vertices}]
Two vertices $u$ and $v$ in a graph $G$ are \defword{connected}\index{connected vertices} if $u=v$, or if $u\not =v$ and there exists a path in $G$ that links $u$ and $v$
\end{definition}
(For \emph{path}, see Definition~\ref{def:path} later)
\vfill
\begin{definition}[{Connected graph}]
A graph is \defword{connected}\index{connected graph} if every two vertices of $G$ are connected; otherwise, $G$ is \defword{disconnected}\index{disconnected graph}
\end{definition}
\end{frame}

\begin{frame}\frametitle{A necessary condition for connectedness}
	\begin{theorem}
		A connected graph on $p$ vertices has at least $p-1$ edges
	\end{theorem}
	\vfill
	In other words, a connected graph $G$ of order $p$ has $\text{size}(G)\geq p-1$
\end{frame}

\begin{frame}{Connectedness is an equivalence relation}
	Denote $x\equiv y$ the relation  ``$x=y$, or $x\neq y$ and there exists a path in $G$ connecting $x$ and $y$''. $\equiv$ is an equivalence relation since
	\begin{enumerate}
		\item $x\equiv y$ \hfill[reflexivity]
		\item $x\equiv y\implies y\equiv x$ \hfill[symmetry]
		\item $x\equiv y, y\equiv z\implies x\equiv z$ \hfill[transitivity]
	\end{enumerate}
	\vfill
	\begin{definition}[Connected component of a graph]
		The classes of the equivalence relation $\equiv$ partition $V$ into connected sub-graphs of $G$ called \defword{connected components} (or \defword{components} for short) of $G$
	\end{definition}
	\vfill
	A connected subgraph $H$ of a graph $G$ is a component of $G$ if $H$ is not contained in any connected subgraph of $G$ having more vertices or edges than $H$
\end{frame}



\begin{frame}{Vertex deletion \& cut vertices}
\begin{minordefinition}[{Vertex deletion}]
If $v\in V(G)$ is a vertex of $G$, the graph $G-v$ is the graph formed from $G$ by removing $v$ and all edges incident with $v$
\end{minordefinition}
\vfill
\begin{definition}[{Cut-vertices}]
	Let $G$ be a connected graph. Then $v$ is a \defword{cut-vertex} $G$ if $G-v$ is disconnected
\end{definition}
\end{frame}


\begin{frame}{Edge deletion \& bridges}
\begin{minordefinition}[{Edge deletion}]
	If $e$ is an edge of $G$, the graph $G-e$ is the graph formed from $G$ by removing $e$ from $G$
\end{minordefinition}
\vfill
\begin{definition}[{Bridge}]
An edge $e$ in a connected graph $G$ is a \defword{bridge} if $G-e$ is disconnected
\end{definition}
\vfill
\begin{theorem}
Let $G$ be a connected graph. An edge $e$ of $G$ is a bridge of $G$ $\iff$ $e$ does not lie on any cycle of $G$
\end{theorem}
(For \emph{cycle}, see Definition~\ref{def:cycle} later)
\end{frame}

 


%\begin{frame}\frametitle{Undirected network}
%\begin{definition}{Definition}
%A undirected network is a graph together with a function which maps the edge set into the set of real number.
%\end{definition}\end{frame}



%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\subsection{Walks, trails, paths}
\newSubSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}


\begin{frame}\frametitle{Walk}
\begin{definition}[{Walk}]
A \defword{walk}\index{walk, graph} in a graph $G=(V,E)$ is a non-empty alternating sequence $v_0 e_0 v_1 e_1 v_2 \dots e_{k-1} v_k$ of vertices and edges in $G$ such that $e_i=\{v_i, v_{i+1}\}$ for all $i<k$. 
This walk begins with $v_0$ and ends with $v_k$
\end{definition}
\vfill
\begin{definition}[Length of a walk]
The \defword{length} of a walk is equal to the number of edges in the walk
\end{definition}
\vfill
\begin{definition}[{Closed walk}]
If $v_0=v_k$, the walk is \defword{closed}
\end{definition}
\end{frame}


\begin{frame}\frametitle{Trail and path}
\begin{definition}[{Trail}]
If the edges in the walk are all distinct, it defines a \defword{trail} in $G=(V,E)$
\end{definition}
\vfill
\begin{definition}[{Path}]\label{def:path}
If the vertices in the walk are all distinct, it defines a \defword{path} in $G$
\end{definition}
\vfill
The sets of vertices and edges determined by a trail is a subgraph
\end{frame}


\begin{frame}\frametitle{Distance between two vertices}
\begin{definition}[{Distance between two vertices}]
The (\defword{geodesic}) \defword{distance} $d(u,v)$ in $G=(V,E)$ between two vertices $u$ and $v$ is the length of the shortest path linking $u$ and $v$ in $G$
\vskip0.2cm
If no such path exists, we assume $d(u,v)=\infty$
\end{definition}
\end{frame}


\begin{frame}\frametitle{Circuit and cycle}
\begin{definition}[Circuit]
A trail linking $u$ to $v$, containing at least 3 edges and in which $u=v$, is a \defword{circuit}
\end{definition}
\vfill
\begin{definition}[Cycle]\label{def:cycle}
A circuit which does not repeat any vertices (except the first and the last) is a \defword{cycle} (or \defword{simple circuit})
\end{definition}
\vfill
\begin{definition}[Length of a cycle]
The \defword{length of a cycle} is its number of edges
\end{definition}
\end{frame}



\begin{frame}{Eulerian and Hamiltonian trails and circuits}
\begin{center}
\begin{tabular}{p{6.5cm}p{6.5cm}}
Eulerian & Hamiltonian \\ \midrule
A walk in an undirected multigraph $M$ that uses each edge \textbf{exactly once} is a \defword{Eulerian trail} of $M$
& 
A path containing all vertices of a graph $G$ is a \defword{Hamiltonian path} of $G$ 
\\ \midrule
If a graph $G$ has a Eulerian trail, then $G$ is a \defword{traversable graph}
& 
If a graph $G$ has an Hamiltonian path, then $G$ is a \defword{traceable graph}
\\ \midrule
A circuit containing all the vertices and edges of a multigraph $M$ is a \defword{Eulerian circuit} of $M$
& 
A cycle containing all vertices of a graph $G$ is a \defword{Hamiltonian cycle} of $G$
\\ \midrule
A graph (resp. multigraph) containing an Eulerian circuit is a \defword{Eulerian graph} (resp. \defword{Eulerian multigraph})
& 
A graph containing a Hamiltonian cycle is a \defword{Hamiltonian graph}
\\ \bottomrule
\end{tabular}
\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Complete, bipartite and other notable graphs}
\newSubSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}


\begin{frame}
\begin{definition}[{Complete graph}]
A graph is complete if every two of its vertices are adjacent
\end{definition}
\vfill
\begin{definition}[$n$-clique]
	A simple, complete graph on $n$ vertices is called an $n$-\defword{clique} and is often denoted $K_n$
\end{definition}
\vfill
Note that a complete graph of order $p$ is $(p-1)$-regular
\end{frame}

\begin{frame}\frametitle{Bipartite graph}
	\begin{definition}[Bipartite graph]
		A graph is \defword{bipartite} if its vertices can be partitioned into two sets $V_1$ and $V_2$, such that no two vertices in the same set are adjacent.
		This graph may be written $G=(V_1,V_2,E)$
	\end{definition}
	\vfill
	\begin{definition}[Complete bipartite graph]
		A bipartite graph in which every two vertices from the 2 different partitions are adjacent is called a \defword{complete bipartite graph}
		\vskip0.2cm
		We often denote $K_{p,q}$ a simple, complete bipartite graph with $|V_1|=p$ and $|V_2|=q$
	\end{definition}
\end{frame}

\begin{frame}\frametitle{Some specific graphs}
	\begin{definition}[{Tree}]
		Any connected graph that has no cycles is a \defword{tree}
	\end{definition}
	\begin{definition}[{Cycle $C_n$}]
		For $n\geq 3$, the \defword{cycle} $C_n$ is a connected graph of order $n$ that is a cycle on $n$ vertices
	\end{definition}
	\begin{definition}[{Path $P_n$}]
		The \defword{path} $P_n$ is a connected graph that consists of $n\geq 2$ vertices and $n-1$ edges. Two vertices of $P_n$ have degree 1 and the rest are of degree 2
	\end{definition}
	\begin{definition}[{Star $S_n$}]
		The \defword{star} of order $n$ is the complete bipartite graph $K_{1,n-1}$ (1 vertex of degree $n-1$ and $n-1$ vertices of degree 1)
	\end{definition}
\end{frame}






%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Planar graphs}
\newSubSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}



\begin{frame}\frametitle{Planar graph}
\begin{definition}[Planar graph]
A graph is \defword{planar} if it \emph{can be} drawn in the plane with no crossing edges (except at the vertices). Otherwise, it is \defword{nonplanar}
\end{definition}
\vfill
\begin{definition}[{Plane graph}]
A \defword{plane graph} is a graph \emph{that is drawn} in the plane with no crossing edges. (This is only possible if the graph is planar)
\end{definition}
\vfill
(To see the difference, have you ever played \href{https://www.chiark.greenend.org.uk/~sgtatham/puzzles/js/untangle.html}{this game}?)
\end{frame}



\begin{frame}
Let $G$ be a plane graph
\begin{itemize}
\item the connected parts of the plane are called \defword{regions}
\item vertices and edges that are incident with a region $R$ make up a \defword{boundary} of $R$
\end{itemize}
\vfill
\begin{theorem}[{Euler's formula}]
Let $G$ be a connected plane graph with $p$ vertices, $q$ edges, and $r$ regions, then $$p-q+r=2$$
\end{theorem}
\vfill
\begin{corollary}
Let $G$ be a plane graph with $p$ vertices, $q$ edges, $r$ regions, and $k$ connected components, then $$p-q+r=k+1$$
\end{corollary}
\end{frame}


% \begin{frame}
% \begin{theorem}
% Let $G$ be a connected planar graph with $p$ vertices and $q$ edges, where $p\geq 3$, then $$q\leq 3p-6.$$
% (a maximal connected planar graph with $p$ vertices has $q=3p-6$ edges)
% \end{theorem}
% \vfill
% \begin{corollary}
% If $G$ is a planar graph, then $\delta(G)\leq 5,$
% where $\delta(G)$ is the minimal degree of $G$.
% (every planar graph contains a vertex of degree less than 6)
% \end{corollary}
% \end{frame}


\begin{frame}{Two well-known non-planar graphs}
$K_{3,3}$ and $K_5$ are nonplanar
\vfill
\begin{theorem}[{Kuratowski Theorem}]
A graph $G$ is planar $\iff$ it contains no subgraph isomorphic to $K_5$ or $K_{3,3}$ or any subdivision of $K_5$ or $K_{3,3}$
\end{theorem}
\vfill
\textbf{Note:} If a graph $G$ is nonplanar and $G$ is a subgraph of $G'$, then $G'$ is also nonplanar
\end{frame}


\subsection{Graph colouring}
\newSubSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}

\begin{frame}
\begin{definition}[{Colouring of a graph $G$}]
A \defword{colouring} of a graph $G$ is an assignment of colours to the vertices of $G$ such that adjacent vertices have different colours
\end{definition}
\vfill
\begin{definition}[{$n$-colouring of $G$}]
A \defword{$n$-colouring} is a colouring of $G$ using $n$ colours
\end{definition}
\vfill
\begin{definition}[{$n$-colourable}]
$G$ is \defword{$n$-colourable} if there exists a colouring of $G$ that uses $n$ colours
\end{definition}
\end{frame}


\begin{frame}
\begin{definition}[{Chromatic number}]
The \defword{chromatic number} $\chi (G)$ of a graph $G$ is the minimal value $n$ for which an $n$-colouring of $G$ exists
\end{definition}
\vfill
\begin{property}
\begin{itemize}
\item $\chi (G)=1$ $\iff$ $G$ have no edges
\item If $G=K_{n,m}$, then $\chi(G)=2$
\item If $G=K_n$, then $\chi(G)=n$
\item For any graph $G$, $$\chi (G)\leq 1+ \Delta (G)$$
where $\Delta (G)$ is the maximum degree of $G$
\item If $G$ is a planar graph, then $\chi(G) \leq 4$
\end{itemize}
\end{property}
\end{frame}


\begin{frame}\frametitle{``Real life'' problem}
What is the minimal number of colours to colour all states in the map so that two adjacent states have different colours?
\begin{center}
\includegraphics[width=.85\textwidth]{FIGS/4-colours-Europe}
\end{center}\end{frame}



%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\Ssubsection{Directed graphs}{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}


\begin{frame} \frametitle{Definitions} 
\begin{definition}[{Digraph}]
A directed graph (or \defword{digraph}) is a pair $G=(V,A)$ of sets such that
\begin{itemize}
\item $V$ is a set of points:  $V=\{v_1,v_2,v_3,..,v_p\}$
\item $A$ is a set of ordered pairs of $V$: $A=\{(v_i,v_j),(v_i,v_k),\dots,(v_n,v_p)\}$ or $A=\{v_iv_j,v_iv_k,\dots,v_nv_p\}$
\end{itemize}
\end{definition}
\begin{definition}[{Vertex}]
The elements of $V$ are the vertices of the digraph $G$.
$V$ or $V(G)$ is the vertex set of the digraph $G$
\end{definition}
\begin{definition}[{Arc}]
The elements of $A$ are the \defword{arcs} (directed edges) of the digraph $G$.
$A$ or $A(G)$ is the arc set of the digraph $G$
\end{definition}
\end{frame}



\begin{frame}\frametitle{Directed network/weighted (di)graph}
\begin{definition}[Directed network]
A directed network is a digraph together with a function $f$,
\[
f: A \rightarrow \mathbb{R},
\]
which maps the arc set $A$ into the set of real number. 
The value of the arc $uv \in A$ is $f(uv)$
\end{definition}
\vfill
Another name is \defword{weighted} (di)graph
\end{frame}



\begin{frame}{Loops \& Multiple arcs}
	\begin{definition}[{Loop}]
	A \defword{loop} is an arc with both the same ends; \emph{e.g.} $(u,u)$ is a loop
\end{definition}
\vfill
	\begin{definition}[{Multiple arcs}]
	\defword{Multiple arcs} (or multi-arcs) are two or more arcs connecting the same two vertices
	\end{definition}
\end{frame}
\begin{frame}{Multidigraph/Digraph}
	\begin{definition}[{Multidigraph}]
	A \defword{multidigraph} is a digraph which allows repetition of arcs or loops
	\end{definition}
	\vfill
	\begin{definition}[{Digraph}]
	In a digraph, no more than one arc can join any pair of vertices
\end{definition}
\end{frame}

\begin{frame}
	Let $G=(V,A)$ be a digraph
	\begin{definition}[Arc endpoints]
	For an arc $u=(x,y)$, vertex $x$ is the \defword{initial endpoint}, and vertex $y$ is the \defword{terminal endpoint}
	\end{definition}
	\begin{definition}[{Predecessor - Successor}]
		If $(u,v)\in A(G)$ is an arc of $G$, then
		\begin{itemize}
			\item  $u$ is a \defword{predecessor} of $v$
			\item  $v$ is a \defword{successor} of $u$
		\end{itemize}
	\end{definition}
	\begin{definition}[Neighbours of a vertex]
	Let $x\in V$ be a vertex. The \defword{neighbours} of $x$ is the set $\Gamma(x) = \Gamma^+_G(x)\cup\Gamma^-_G(x)$, where $\Gamma^+_G(x)$ and $\Gamma^-_G(x)$ are, respectively, the set of successors and predecessors of $v$
	\end{definition}
\end{frame}
	
	
\begin{frame}{Sources and sinks}
	\begin{minordefinition}[{Directed away - Directed towards}]
		If $a=(u,v)\in A(G)$ is an arc of $G$, then
		\begin{itemize}
			\item the arc $a$ is said to be \defword{directed away} from $u$
			\item the arc $a$ is said to be \defword{directed towards} $v$
		\end{itemize}
	\end{minordefinition}
	\vfill
	\begin{minordefinition}[{Source - Sink}]
		\begin{itemize}
			\item Any vertex which has no arcs directed towards it is a \defword{source}
			\item Any vertex which has no arcs directed away from it is a \defword{sink}
		\end{itemize}
	\end{minordefinition}
\end{frame}

\begin{frame}{Adjacent arcs}
	\begin{definition}[Adjacent arcs]
		Two arcs are \defword{adjacent} if they have at least one endpoint in common
	\end{definition}
\end{frame}

\begin{frame}{Arcs incident to a subset of arcs}
	\begin{definition}[Arc incident out of $X\subset A(G)$]
		If the initial endpoint of an arc $u$ belongs to $X\subset A(G)$ and if the terminal endpoint of arc $u$ does not belong to $X$, then $u$ is said to be \defword{incident out of} $X$; we write $u\in\omega^+(X)$
		\vskip0.2cm
		Similarly, we define an \defword{arc incident into} $X$ and the set $\omega^-(X)$
		\vskip0.2cm
		Finally, the set of arcs \defword{incident to} $X$ is denoted
		\[
		\omega(X) = \omega^+(X)\cup\omega^-(X)
		\]
	\end{definition}
\end{frame}

\begin{frame}
	\begin{definition}[Subgraph of $G$ generated by $A\subset V$]
		The \defword{subgraph} of $G$ generated by $A$ is the graph with $A$ as its vertex set and with all the arcs in $G$ that have both their endpoints in $A$. If $G=(V,\Gamma)$ is a 1-graph, then the subgraph generated by $A$ is the 1-graph $G_A=(A,\Gamma_A)$ where
		\[
		\Gamma_A(x)=\Gamma(x)\cap A\qquad (x\in A)
		\]
	\end{definition}
	\vfill
	\begin{definition}[Partial graph of $G$ generated by $V\subset U$]
		\label{def:partial_graph}
		The graph $(X,V)$ whose vertex set is $X$ and whose arc set is $V$. 
		In other words, it is graph $G$ without the arcs $U-V$
	\end{definition}
	\vfill
	\begin{definition}[Partial subgraph of $G$]
		A partial subgraph of $G$ is the subgraph of a partial graph of $G$
	\end{definition}
\end{frame}




%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\Ssubsection{Directed graphs}{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}

\begin{frame}{Degree}
	Let $v$ be a vertex of a digraph $G=(V,A)$
	\begin{definition}[{Outdegree of a vertex}]
	The number of arcs directed away from a vertex $v$, in a digraph is called the \defword{outdegree} of $v$ and is written $d^+_G(v)$
	\end{definition}
	\begin{definition}[{Indegree of a vertex}]
	The number of arcs directed towards a vertex $v$, in a digraph is called the \defword{indegree} of $v$ and is written $d^-_G(v)$
	\end{definition}
	\begin{definition}[{Degree}]
	For any vertex $v$ in a digraph, the \defword{degree}\index{degree, digraph} of $v$ is defined as 
	\[
		d_G(v)=d^+_G(v)+d^-_G(v)
	\]
	\end{definition}
\end{frame}


\begin{frame}
	\begin{theorem}
		For any (di)graph, the sum of the degrees of the vertices equals twice the number of edges (arcs)
	\end{theorem}
	\begin{corollary}
		In any (di)graph, the sum of the degrees of the vertices is a nonnegative even integer
	\end{corollary}
	\begin{theorem}
		If $G$ is a digraph with vertex set $V(G)=\{v_1, \dots , v_p\}$ and $q$ arcs, then $$\sum_{i=1}^p d^+_G(v_i)=\sum_{i=1}^p d^-_G(v_i)=q$$
	\end{theorem}
\end{frame}

\begin{frame}
	\begin{definition}[{Regular digraph}]
		A digraph $G$ is $r$-regular if $d_G^+(v)=d_G^-(v)=r$ for all $v\in V(G)$
\end{definition}\end{frame}

\begin{frame}{Symmetric/antisymmetric digraphs}
	\begin{definition}[Symmetric digraph]
		Let $G=(V,A)$ be a digraph with associated binary relation $R$.
		If $R$ is \emph{symmetric}, the digraph is symmetric
	\end{definition}
	\vfill
	\begin{definition}[Anti-symmetric digraph]
		Let $G=(V,A)$ be a digraph with associated binary relation $R$.
		The digraph $G$ is \defword{anti-symmetric} if
		\[
		xRy \implies y\cancel R x
		\]
	\end{definition}
	\vfill
	\begin{definition}[Symmetric multidigraph]
		Let $G=(V,A)$ be a multidigraph. $G$ is symmetric if $\forall x,y\in V(G)$, the number of arcs from $x$ to $y$ equals the number of arcs from $y$ to $x$
	\end{definition}
\end{frame}



%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\subsection{Walks, paths, etc.}
\newSubSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}

\begin{frame}\frametitle{Walks}
	Let $G=(V,A)$ be a digraph.
	\begin{definition}[Directed walk]
	A \defword{directed walk} in a digraph $G$ is a non-empty alternating sequence $v_0 a_0 v_1 a_1 v_2 \dots a_{k-1} v_k$ of vertices and arcs in $G$ such that $a_i=(v_i, v_{i+1})$ for all $i<k$. 
	This walk begins with $v_0$ and ends with $v_k$
	\end{definition}
	\begin{definition}[{Length of a directed walk}]
	The length of a directed walk is equal to the number of arcs in the directed walk
	\end{definition}
	\begin{definition}[{Closed walk}]
	If $v_0=v_k$, the walk is closed
	\end{definition}
\end{frame}



\begin{frame}\frametitle{Trails}
	Let $G=(V,A)$ be a digraph.
	\begin{definition}[{Directed trail}]
	A directed walk in $G$ in which all arcs are distinct is a \defword{directed trail} in $G$
	\end{definition}
	\begin{definition}[Directed path]
	A directed walk in $G$ in which all vertices are distinct is a \defword{directed path} in $G$
	\end{definition}
	\begin{definition}[{Directed cycle}]
	A closed walk is a \defword{directed cycle} if it contains at least three vertices and all its vertices are distinct except for $v_0=v_k$
	\end{definition}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Connectivity in digraphs}
\newSubSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}

\begin{frame}\frametitle{Definitions}
	%Let $D=(V,A)$ be a digraph.
	\begin{minordefinition}[{Underlying graph}]
	Given a digraph, the undirected graph with each arc replaced by an edge is called the \defword{underlying graph}
	\end{minordefinition}
	\vfill
	\begin{definition}[{Weakly connected digraph}]
	If the underlying graph is a connected graph, then the digraph is \defword{weakly connected}
	\end{definition}
	\vfill
	\begin{definition}[{Strongly connected digraph}]
	A digraph $G$ is \defword{strongly connected} if for every two distinct vertices $u$ and $v$ of $G$, there exists a directed path from $u$ to $v$
	\end{definition}
	\vfill
	\begin{definition}[{Disconnected digraph}]
	A digraph is said to be \defword{disconnected}\index{disconnected digraph} if it is not weakly connected
	\end{definition}
\end{frame}

\begin{frame}{Strong connectedness is an equivalence relation}
Denote $x\equiv y$ the relation  ``$x=y$, or $x\neq y$ and there exists a directed path in $G$ from $x$ to $y$''. $\equiv$ is an equivalence relation since
\begin{enumerate}
	\item $x\equiv y$ \hfill[reflexivity]
	\item $x\equiv y\implies y\equiv x$ \hfill[symmetry]
	\item $x\equiv y, y\equiv z\implies x\equiv z$ \hfill[transitivity]
\end{enumerate}
\begin{definition}[Connected component of a graph]
	\label{def:strong_components}
	Sets of the form 
	\[
	A(x_0)=\{x:x\in V, x\equiv x_0\}	
	\]
	are equivalence classes. 
	They partition $V$ into strongly connected sub-digraphs of $G$ called \defword{strongly connected components} (or \defword{strong components}) of $G$
\end{definition}
A strong component in $G$ is a maximal strongly connected subdigraph of $G$
\end{frame}


\begin{frame}
	\begin{theorem}[Properties]
		Let $G=(V,A)$ be a digraph
	\begin{itemize}
	\item If $G$ is strongly connected, it has only one strongly connected component
	\item The strongly connected components partition the vertices $V(G)$, with every vertex in exactly one strongly connected component
	\end{itemize}
	\end{theorem}
\end{frame}


\begin{frame}\frametitle{Condensation of a digraph}
	\begin{definition}[{Condensation of a digraph}]
		The condensation $G^*$ of a digraph $G$ is a digraph having as vertices the strongly connected components (SCC) of $G$ and such that there exists an arc in $G^*$ from a SCC $C_i$ to another SCC $C_j$ if there is an arc in $G$ from some vertex of $S_i$ to a vertex of $S_j$
	\end{definition}
\end{frame}

\begin{frame} 
	\begin{minordefinition}[Articulation set]
		For a connected graph, a set $X$ of vertices is called an \defword{articulation set} (or a \defword{cutset}) if the subgraph of $G$ generated by $V-X$ is not connected
	\end{minordefinition}
	\vfill
	\begin{minordefinition}[Stable set]
		A set $S$ of vertices is called a \defword{stable set} if no arc joins two distinct vertices in $S$
	\end{minordefinition}
	\vfill
\end{frame}




%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{Orientable graphs}
\newSubSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}

\begin{frame}\frametitle{Orientation}
\begin{definition}[Orienting a graph]
Given a connected \emph{graph}, we describe the act of assigning a direction to each edge (edge $\rightarrow$ arc) as \defword{orienting the graph}
\end{definition}
\vfill
\begin{definition}[{Strong orientation}]
If the digraph resulting from orienting a graph is strongly connected, the orientation is a \defword{strong orientation}
\end{definition}
\end{frame}
 
 
\begin{frame}\frametitle{Orientable graph}
\begin{definition}[Orientable graph]
A connected graph $G$ is \defword{orientable} if it admits a strong orientation
\end{definition}
\vfill
\begin{theorem}
A connected graph $G=(V,E)$ is orientable $\iff$ $G$ contains no bridges
\end{theorem}
(in other words, iff every edge is contained in a cycle)
\end{frame}
 

\Ssubsection{Matrices associated to a graph/digraph}{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}

\begin{frame}{Matrices associated to a graph/digraph}
	There are multiple matrices associated to a graph/digraph
	\vfill
	The branch of graph theory that studies the properties of matrices derived from graphs and uses of these matrices in determining graph properties is \emph{spectral graph theory}
	\vfill
	Graphs greatly simplify some problems in linear algebra and vice versa
\end{frame}

\subsection{Adjacency matrices}
\newSubSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}


\begin{frame}\frametitle{Adjacency matrix (undirected case)}
	Let $G=(V,E)$ be a graph of order $p$ and size $q$, with vertices $v_1, \ldots, v_p$ and edges $e_1, \dots , e_q$
	\begin{definition}[{Adjacency matrix}]
		The \defword{adjacency matrix} is $$M_A=M_A(G)=[m_{ij}]$$ is a $p\times p$ matrix in which
		$$m_{ij}=\left \{ 
		\begin{array}{cc}
			1 & \textrm{if } v_i \textrm{ and } v_j \textrm{ are adjacent}\\
			0 & \textrm{otherwise}
		\end{array}
		\right .
		$$
	\end{definition}
\end{frame}

\begin{frame}
	\begin{theorem}[{Adjacency matrix and degree}]
		The sum of the entries in row $i$ of the adjacency matrix is the degree of $v_i$ in the graph
	\end{theorem}
	\vfill
	We often write $A(G)$ and, reciprocally, if $A$ is an adjacency matrix, $G(A)$ the corresponding graph
	\vfill
	$G$ undirected $\implies$ $A(G)$ symmetric
	\vfill
	$A(G)$ has nonzero diagonal entries if $G$ is not simple
\end{frame}

\begin{frame}\frametitle{Adjacency matrix (directed case)}
	Let $G=(V,A)$ be a digraph of order $p$ with vertices $v_1,\ldots,v_p$
	\begin{definition}[Adjacency matrix]
	The \defword{adjacency matrix} $M=M(G)=[m_{ij}]$ is a $p\times p$ matrix in which
	$$m_{ij}=\left \{ 
	\begin{array}{cl}
	1 & \textrm{if arc } v_iv_j \in A\\
	0 & \textrm{otherwise}
	\end{array}
	\right .
	$$
	\end{definition}
\end{frame}

\begin{frame}
	\begin{importanttheorem}[{Properties of the adjacency matrix}]
	Let $M$ be the adjacency matrix of a digraph $G$
	\begin{itemize}
	\item $M$ is not necessarily symmetric
	\item The sum of any column of $M$ is equal to the number of arcs directed towards $v_j$
	\item The sum of the entries in row $i$ is equal to the number of arcs directed away from vertex $v_i$
	\item The $(i,j)-$entry of $M^n$ is equal to the number of walks of length $n$ from vertex $v_i$ to $v_j$
	\end{itemize}
	\end{importanttheorem}
\end{frame}
	
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Adjacency matrix (directed)}
\hldef{M} \hlkwb{<-} \hlkwd{as.matrix}\hldef{(}\hlkwd{as_adjacency_matrix}\hldef{(G,} \hlkwc{sparse} \hldef{=} \hlnum{FALSE}\hldef{))}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'as.matrix': object 'G' not found}}\begin{alltt}
\hlcom{# Column sums = indegrees, row sums = outdegrees (directed case)}
\hlkwd{all.equal}\hldef{(}\hlkwd{rowSums}\hldef{(M),} \hlkwd{degree}\hldef{(G,} \hlkwc{mode} \hldef{=} \hlsng{"out"}\hldef{))}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in h(simpleError(msg, call)): error in evaluating the argument 'target' in selecting a method for function 'all.equal': error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'M' not found}}\begin{alltt}
\hlkwd{all.equal}\hldef{(}\hlkwd{colSums}\hldef{(M),} \hlkwd{degree}\hldef{(G,} \hlkwc{mode} \hldef{=} \hlsng{"in"}\hldef{))}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in h(simpleError(msg, call)): error in evaluating the argument 'target' in selecting a method for function 'all.equal': error in evaluating the argument 'x' in selecting a method for function 'colSums': object 'M' not found}}\begin{alltt}
\hlcom{# Walks of length 3 from 1 to 15 via M^3}
\hldef{M3} \hlkwb{<-} \hldef{M} \hlopt{%*%} \hldef{M} \hlopt{%*%} \hldef{M}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error: object 'M' not found}}\begin{alltt}
\hldef{M3[}\hlnum{1}\hldef{,} \hlnum{15}\hldef{]}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error: object 'M3' not found}}\end{kframe}
\end{knitrout}

	

\begin{frame}
	\begin{minordefinition}[Multiplicity of a pair]
		The \defword{multiplicity} of a pair $x,y$ is the number $m_G^+(x,y)$ of arcs with initial endpoint $x$ and terminal endpoint $y$. Let
		\begin{align*}
			m_G^-(x,y) &= m_G^+(y,x) \\
			m_G(x,y) &= m_G^+(x,y)+m_G^-(x,y)
		\end{align*}
		If $x\neq y$, then $m_G(x,y)$ is number of arcs with both $x$ and $y$ as endpoints. If $x=y$, then $m_G(x,y)$ equals twice the number of loops attached to vertex $x$. If $A,B\subset V$, $A\neq B$, let
		\begin{align*}
			m_G^+(A,B) &= \{u:u\in U, u=(x,y),x\in A,y\in B\} \\
			m_G(A,B) &= m_G^+(A,B)+m_G^+(A,B)
		\end{align*}
	\end{minordefinition}
\end{frame}

% \begin{frame}{Adjacency matrix of a multigraph}
% \begin{definition}[Matrix associated with $G$]
% 	If $G$ has vertices $x_1,x_2,\ldots,x_n$, then the \defword{matrix associated} with $G$ is 
% 	\[
% 	a_{ij}=m_G^+(x_i,x_j)
% 	\]
% \end{definition}
% \vfill
% \begin{definition}[Adjacency matrix]
% 	The matrix $a_{ij}+a_{ji}$ is the \defword{adjacency matrix} associated with $G$
% \end{definition}
% \end{frame}

\begin{frame}{Adjacency matrix (multigraph case)}
	\begin{definition}[Adjacency matrix of a multigraph]
		$G$ an $\ell$-graph, then the adjacency matrix $M_A=[m_{ij}]$ is defined as follows
		\[
		m_{ij} = \begin{cases}
			k & \text{if arc there are $k$ arcs } (i,j)\in U\\
			0 & \text{otherwise}
		\end{cases}
		\]
		with $k\leq \ell$
	\end{definition}
	\vfill
	$G$ undirected $\implies$ $M_A(G)$ symmetric
	\vfill
	$M_A(G)$ has nonzero diagonal entries if $G$ is not simple.
\end{frame}

\begin{frame}{Weighted adjacency matrices}
	Sometimes, adjacency matrices (typically for 1-graphs) have real entries, usually positive
	\vfill
	This means that the arcs/edges have been given a weight
\end{frame}


\begin{frame}
	\begin{theorem}[{Number of walks of length $n$}]
		Let $A$ be the adjacency matrix of a graph $G=(V(G),E(G))$, where $V(G)=\{v_1,v_2, \dots , v_p\}$. Then the $(i,j)-$entry of $A^n$, $n\geq 1$, is the number of different walks linking $v_i$ to $v_j$ of length $n$ in $G$.
	\end{theorem}
	(two walks of the same length are equal if their edges occur in exactly the same order)
	
	Example: let $A$ be the adjacency matrix of a graph $G=(V(G),E(G))$. 
	\begin{itemize}
		\item the $(i,i)-$entry of $A^2$ is equal to the degree of $v_i$.
		\item the $(i,i)-$entry of $A^3$ is equal to twice the number of $C_3$ containing $v_i$.
	\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Other matrices associated to a graph/digraph}
\newSubSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}

\begin{frame}\frametitle{Incidence matrix (undirected case)}
	Let $G=(V,E)$ be a graph of order $p$, and size $q$, with vertices $v_1, \ldots , v_p$, and edges $e_1, \ldots , e_q$
	\begin{definition}[{Incidence matrix}]
		The incidence matrix is $$B=B(G)=[b_{ij}]$$ is that $p\times q$ matrix in which
		$$b_{ij}=\left \{ 
		\begin{array}{cc}
			1 & \textrm{if } v_i \textrm{ is incident with } e_j\\
			0 & \textrm{otherwise}
		\end{array}
		\right .
		$$
	\end{definition}
	\begin{theorem}[{Incidence matrix and degrees}]
		The sum of the entries in row $i$ of the incidence matrix is the degree of $v_i$ in the graph
	\end{theorem}
\end{frame}


\begin{frame}\frametitle{Incidence matrix (directed case)}
	Let $G=(V,A)$ be a digraph of order $p$ and size $q$, with vertices $v_1, \ldots , v_p$ and arcs $a_1, \ldots , a_q$
	\begin{definition}[Incidence matrix]
	The \defword{incidence matrix} $B=B(G)=[b_{ij}]$ is a $p\times q$ matrix in which
	$$b_{ij}=\left \{ 
	\begin{array}{cl}
	1 & \textrm{if arc } a_j  \textrm{ is directed away from a vertex } v_i\\
	-1 & \textrm{if arc } a_j  \textrm{ is directed towards a vertex } v_i\\
	0 & \textrm{otherwise}
	\end{array}
	\right .
	$$
	\end{definition}
\end{frame}
	



\begin{frame}{Spectrum of a graph}
	We will come back to this later, but for now..
	\vfill
	\begin{definition}[Spectrum of a graph]
		The \defword{spectrum} of a graph $G$ is the spectrum (set of eigenvalues) of its associated adjacency matrix $M(G)$
	\end{definition}
	\vfill
	This is regardless of the type of adjacency matrix or graph
\end{frame}



\begin{frame}\frametitle{Distance matrix}
	Let $G$ be a graph of order $p$ with vertices $v_1, \ldots , v_p$
	\begin{definition}[Distance matrix]
	The distance matrix $\Delta(G)=[d_{ij}]$ is a $p\times p$ matrix in which
	$$\delta_{ij}=
	d_G(v_i,v_j)
	$$
	Note $\delta_{ii}=0$ for $i=1,\ldots, p$
	\end{definition}
\end{frame}

\begin{frame}
	\begin{property}
		\begin{itemize}
		\item $M$ is not necessarily symmetric
		\item The sum of any column of $M$ is equal to the number of arcs directed towards $v_j$
		\item The sum of the entries in row $i$ is equal to the number of arcs directed away from vertex $v_i$
		\item The $(i,j)-$entry of $M^n$ is equal to the number of walks of length $n$ from vertex $v_i$ to $v_j$
		\end{itemize}
	\end{property}
\end{frame}



\subsection{Linking graphs and linear algebra}
\newSubSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_fto8nofto8nofto8.jpeg}

\begin{frame}{Counting paths}
	\begin{theorem}
		$G$ a digraph and $M_A(G)$ its adjacency matrix. Denote $P=[p_{ij}]$ the matrix $P=M_A^k$. Then $p_{ij}$ is the number of distinct paths of length $k$ from $i$ to $j$ in $G$
	\end{theorem}
	\vfill
	\begin{definition}[Irreducible matrix]
		A matrix $A\in\M_n$ is \defword{reducible} if $\exists P\in\M_n$, permutation matrix, s.t. $P^TAP$ can be written in block triangular form. If no such $P$ exists, $A$ is \defword{irreducible}
	\end{definition}
	\vfill
	\begin{theorem}
		$A$ irreducible $\iff$ $G(A)$ strongly connected
	\end{theorem}
\end{frame}

\begin{frame}
	\begin{theorem}
		Let $A$ be the adjacency matrix of a graph $G$ on $p$ vertices. 
		A graph $G$ on $p$ vertices is connected $\iff$ $$I+A+A^2+\dots+A^{p-1}=C$$
		has no zero entries
	\end{theorem}
	\vfill
	\begin{theorem}
		Let $M$ be the adjacency matrix of a digraph $D$ on $p$ vertices. 
		A digraph $D$ on $p$ vertices is strongly connected $\iff$ $$I+M+M^2+\dots+M^{p-1}=C$$
		has no zero entries
	\end{theorem}
\end{frame}


\begin{frame}{Nonnegative matrix}
\vfill
$A=[a_{ij}]\in\M_n(\IR)$ \defword{nonnegative} if $a_{ij}\geq 0$ $\forall i,j=1,\ldots,n$; $\bv\in\IR^n$ nonnegative if $v_i\geq 0$ $\forall i=1,\ldots,n$.  \defword{Spectral radius} of $A$
\[
\rho(A) = \max_{\lambda\in\mathsf{Sp}(A)}\{|\lambda|\}
\]
$\mathsf{Sp}(A)$ the \defword{spectrum} of $A$
\end{frame}

\begin{frame}{Perron-Frobenius (PF) theorem}
\begin{theorem}[PF -- Nonnegative case]
$0\leq A\in\M_n(\IR)$. Then $\exists\bv\geq\b0$ s.t. 
\[
	A\bv=\rho(A)\bv
\]
\end{theorem}
\vfill
\begin{theorem}[PF -- Irreducible case]
	\label{th:PF}
	Let $0\leq A\in \M_n(\IR)$ irreducible. Then $\exists\bv>\b0$ s.t.
	\[
		A\bv=\rho(A)\bv
	\]
	$\rho(A)>0$ and with algebraic multiplicity 1.
	No nonnegative eigenvector is associated to any other eigenvalue of $A$
\end{theorem}
\end{frame}



\begin{frame}{Primitive matrices}
\begin{definition}
$0\leq A\in\M_n(\IR)$ \defword{primitive} (with \defword{primitivity index} $k\in \IN_{+}^{*}$) if $\exists k\in \IN_{+}^{*}$ s.t.
\[
A^k>0,
\]
with $k$ the smallest integer for which this is true.
$A$ \defword{imprimitive} if it is not primitive
\end{definition}
\vfill
$A$ primitive $\implies$ $A$ irreducible; the converse is false
\end{frame}

\begin{frame}
\begin{theorem}
	$A\in\M_n(\IR)$ irreducible and $\exists i=1,\ldots,n$ s.t. $a_{ii}>0$ $\implies$ $A$ primitive
\end{theorem}
\vfill
Here $d$ is the index of imprimitivity (i.e., the number of eigenvalues that have the same modulus as $\lambda_p=\rho(A)$). If $d=1$, then $A$ is primitive. We have that $d=\mathsf{gcd}$ of all the lengths of closed walks in $G(A)$
\end{frame}

\begin{frame}
\begin{theorem}\label{th:PF_allCases}
	$\b0\leq A\in\M_n$, $\lambda_P=\rho(A)$ the Perron root of $A$, $\bv_P$ and $\bw_P$ the corresponding right and left Perron vectors of $A$, respectively, $d$ the index of imprimitivity of $A$ (with $d=1$ when $A$ is primitive) and $\lambda_j\in\sigma(A)$ the spectrum of $A$, with $j=2,\ldots,n$ unless otherwise specified (assuming $\lambda_1=\lambda_P$) 
	\vskip0.5cm
	{\centering
		\begin{tikzpicture}[scale=0.62, every node/.style={transform shape},
			auto,
		cloud/.style={minimum width={width("N-1")+2pt},
			draw, rectangle}]
		\node [cloud] at (10,0) (nonneg) {\sc Nonnegative};
		\node [cloud] (reduc) at (2,-3) [text width=3cm] {{\sc Reducible}\\ \noindent\begin{itemize}
			\item $\lambda_P\geq 0$
			\item $\bw_P\geq 0$
			\item $\bv_P\geq 0$
			\item $\lambda_P \geq | \lambda_j |$
			\end{itemize}
		};
		\node [cloud] (irred) at (12,-2) {\sc Irreducible};	
		\node [cloud] (notprim) at (8,-4) [text width=3cm] {{\sc Imprimitive}\\ \begin{itemize}
			\item $\lambda_P> 0$
			\item $\bw_P> 0$
			\item $\bv_P> 0$
			\item $\lambda_P=|\lambda_j|$, $j=2,\ldots,d$
			\item $\lambda_P > |\lambda_j|$, $j>d$
			\end{itemize}
		};	
		\node [cloud] (prim) [text width=3cm] at (16,-4) {{\sc Primitive}\\
			\begin{itemize}
			\item $\lambda_P> 0$
			\item $\bw_P> 0$
			\item $\bv_P> 0$
			\item $\lambda_P>|\lambda_j|$, $j\neq P$ 
			\end{itemize}
		};	
		%% Links
		\path [line, thick] (nonneg) to (reduc);
		\path [line, thick] (nonneg) to (irred);
		\path [line, thick] (irred) to (notprim);
		\path [line, thick] (irred) to (prim);
		\end{tikzpicture}
	}
	\end{theorem}	
\end{frame}



\begin{frame}
\begin{definition}[Minimally connected graph]
$G$ is \defword{minimally connected} if it is strongly connected and removal of any arc destroys strong-connectedness
\end{definition}
A minimally connected graph is 1-graph without loops
\vfill
\begin{definition}[Contraction]
$G=(V,U)$. The \defword{contraction} of the set $A\subset V$ of vertices consists in replacing $A$ by a single vertex $a$ and replacing each arc into (resp. out of) $A$ by an arc with same index into (resp. out of) $a$
\end{definition}
\end{frame}


\begin{frame}
\begin{theorem}
$G$ minimally connected, $A\subset V$ generating a strongly connected subgraph of $G$. Then the contraction of $A$ gives a minimally connected graph
\end{theorem}
\end{frame}

\begin{frame}{Arborescences}
\begin{definition}[Root]
Vertex $a\in V$ in $G=(V,U)$ is a \defword{root} if all vertices of $G$ can be reached by paths \emph{starting} from $a$
\end{definition}
Not all graphs have roots
\vfill
\begin{definition}[Quasi-strong connectedness]
$G$ is \defword{quasi-strongly connected} if $\forall x,y\in V$, exists $z\in V$ (denoted $z(x,y)$ to emphasize dependence on $x,y$) from which there is a path to $x$ and a path to $y$
\end{definition}
Strongly connected $\implies$ quasi-strongly connected (take $z(x,y)=x$); converse not true

Quasi-strongly connected $\implies$ connected
\end{frame}


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssubsection{Characterisation of graphs}{FIGS-slides-admin/a_robot_looking_at_a_desolate_tree_in_the_style_of_Magritte.png}



\begin{frame}{Geodesic distance}
\begin{definition}[Geodesic distance]
For $x,y\in V$, the \defword{geodesic distance} $d(x,y)$ is the length of the shortest path from $x$ to $y$, with $d(x,y)=\infty$ if no such path exists
\end{definition}
\end{frame}

\begin{frame}{Eccentricity}
\begin{definition}[Vertex eccentricity]
	The \defword{eccentricity} $e(x)$ of vertex $x\in V$ is 
	\[
		e(x)=\max_{\stackrel{y\in V}{y\neq x}}d(x,y)
	\]
\end{definition}
\end{frame}


\begin{frame}{Central points, radius and centre}
\begin{definition}[Central point]
	A \defword{central point} of $G$ is a vertex $x_0$ with smallest eccentricity
\end{definition}
\vfill
\begin{definition}[Radius]
	The \defword{radius} of $G$ is $\rho(G)=e(x_0)$, where $x_0$ is a centre of $G$
	In other words,
	\[
	\rho(G)=\min_{x\in V}e(x)
	\]
\end{definition}
\vfill
\begin{definition}[Centre]
The \defword{centre} of $G$ is the set of vertices that are central points of $G$, i.e.,
\[
    \{x\in V: e(x)=\rho(G)\}
\]
\end{definition}
\end{frame}


\begin{frame}{Betweenness}
\begin{definition}[Betweenness]
	$G=(V,A)$ a (di)graph. The \defword{betweenness} of $v\in V$ is
	\[
	b_\D(v)= \sum_{s\neq t\neq v  \in V} \frac{\sigma_{st}(v)}{\sigma_{st}}
	\]
	where
	\begin{itemize}
	\item $\sigma_{st}$ is number of shortest geodesic paths from $s$ to $t$
	\item $\sigma_{st}(v)$ is number of shortest geodesic paths from $s$ to $t$ through $v$
	\end{itemize}		
\end{definition}
\end{frame}

\begin{frame}
In other words
\begin{itemize}
	\item For each pair of vertices $(s,t)$, compute the shortest paths between them
	\item For each pair of vertices $(s,t)$, determine the fraction of shortest paths that pass through vertex $v$
	\item Sum this fraction over all pairs of vertices $(s,t)$
\end{itemize}
\end{frame}


\begin{frame}{Closeness}
	\begin{definition}
		$G=(V,A)$. The \defword{closeness} of $v\in V$ is
		\[
		c_\D(v)=\frac{1}{n-1}\displaystyle \sum_{t\in V\setminus\{v\}}d_\D(v,t)
		\]
		i.e., mean geodesic distance between a vertex $v$ and all other vertices it has access to
		\vskip0.2cm 
		Another definition is
		\[
		c_\D(v)=\frac{1}{\displaystyle\sum_{t \in V\setminus\{v\}}d_\D(v,t)}
		\]				
	\end{definition}
\end{frame}


\begin{frame}{Diametre and periphery of a graph}
\begin{definition}[Diametre of a graph]
The \defword{diametre} of $G$ is 
\[
	\delta(G) = \max_{\stackrel{x,y\in V}{x\neq y}}d(x,y)
	= \max_{x\in V}e(x)
\]
\end{definition}
\vfill
$\delta(G)<\infty$ $\iff$ $G$ strongly connected
\vfill
\begin{definition}[Periphery]
    The \defword{periphery} of a graph is the set of vertices whose eccentricity achieves the diametre, i.e.,
    \[
        \{x\in V:e(x)=\delta(G)\}
    \]
\end{definition}
\begin{definition}[{Antipodal vertices}]
		Vertices $x,y\in V$ are \defword{antipodal} if $d(x,y)=\delta(G)$
\end{definition}
\end{frame}


\begin{frame}{Degree distribution}
	\begin{definition}[Arc incident to a vertex]
		If a vertex $x$ is the initial endpoint of an arc $u$, which is not a loop, the arc $u$ is \defword{incident out of vertex} $x$ 
		\vskip0.2cm
		The number of arcs incident out of $x$ plus the number of loops attached to $x$ is denoted $d_G^+(x)$ and is the \defword{outer demi-degree} of $x$
		\vskip0.2cm
		An arc \defword{incident into vertex} $x$ and the \defword{inner demi-degree} $d_G^-(x)$ are defined similarly
		\end{definition}
		\vfill
		\begin{definition}[Degree]
		The \defword{degree} of vertex $x$ is the number of arcs with $x$ as an endpoint, each loop being counted twice. The degree of $x$ is denoted $d_G(x)=d_G^+(x)+d_G^-(x)$
		\vskip0.2cm
		If each vertex has the same degree, the graph is \defword{regular}
	\end{definition}
\end{frame}

\begin{frame}
	\begin{definition}[{Isolated vertex}]
	A vertex of degree $0$ is \defword{isolated}.
	\end{definition}
	\begin{definition}[{Average degree of $G$}]
	$d(G)=\frac{1}{|V|}\sum_{v\in V}deg_G(v)$.
	\end{definition}
	\begin{definition}[{Minimum degree of $G$}]
	$\delta(G)=\min \{deg_G(v)|v\in V\}$.
	\end{definition}
	\begin{definition}[{Maximum degree of $G$}]
	$\Delta(G)=\max \{deg_G(v)|v\in V\}$.
	\end{definition}
\end{frame}





\begin{frame}
	\begin{itemize}
		\item Average (nearest) neighbour degree, to encode for \emph{preferential attachment} (one prefers to hang out with popular people)
		\[
			k_i^{nn} = \frac{1}{k(i)}\sum_{j\in\mathcal{N}(i)}k(j)
		\]
		or, in terms of the adjacency matrix $A=[a_{ij}]$,
		\[
			k_i^{nn} = \frac{1}{k(i)}\sum_{j}a_{ij}k(j)
		\]
		\item \emph{Excess degree}: take nearest neighbour degree but do not consider the edge/arc followed to get to the neighbour
		\item Degree, nearest neighbour and excess degree distributions
	\end{itemize}
\end{frame}



\begin{frame}{Degree from adjacency matrix}
	Suppose adjacency matrix take the form $A=[a_{ij}]$ with $a_{ij}=1$ if there is an arc from the vertex indexed $i$ to the vertex indexed $j$ and 0 otherwise. (Could be the other way round, using $A^T$, just make sure)
	\vfill
	Let $\be=(1,\ldots,1)^T$ be the vector of all ones 
	\vfill
	$A\be=(d_G^+(1),\ldots,d_G^+(1))^T$ (out-degree)
	\vfill
	$\be^TA=(d_G^-(1),\ldots,d_G^-(1))$ (in-degree)
\end{frame}


\begin{frame}{Circumference}
\begin{definition}[Circumference]
    In an undirected (resp. directed) graph, the total number of edges (resp. arcs) in the longest cycle of graph $G$ is the \defword{circumference} of $G$
\end{definition}
\end{frame}


\begin{frame}{Girth}
\begin{definition}[Girth]
    The total number of edges in the shortest cycle of graph $G$ is the \defword{girth} $g(G)$
\end{definition}
\end{frame}


\begin{frame}{Completeness}
	\begin{definition}[{Complete undirected graph}]
		An undirected graph is complete if every two of its vertices are adjacent.
	\end{definition}
	\begin{definition}[{Complete digraph}]
		A digraph $D(V,A)$ is complete if $\forall u,v\in V$, $uv\in A$.
	\end{definition}
	\vfill
In case of simple graphs, completeness effectively means that ``information'' can be transmitted from every vertex to every other vertex quickly (1 step)
\vfill
It can be useful to know how far away we are from being complete
\end{frame}

\begin{frame}{Number of edges/arcs in a complete graph}
$G=(V,E)$ undirected and simple of order $n$ has at most
\[
	\frac {n(n-1)}{2}
\]
edges, while $G=(V,A)$ directed and simple of order $n$ has at most
\[
	n(n-1)
\]
arcs 
\end{frame}

\begin{frame}{Density of a graph}
	\begin{definition}[Density]
		The fraction of maximum number of edges or arcs present in the graph is the \defword{density} of the graph.
	\end{definition}
If the graph has $p$ edges or arcs, then its density is, respectively,
\[
	\frac {2p}{n(n-1)}
\]
or 
\[
	\frac{p}{n(n-1)}
\]
\end{frame}

\begin{frame}{Connectedness}
We have already seen connectedness (quasi- or strong in the oriented case)
\vfill
Connectedness is important in terms of characteristing graph properties, as it shows the capacity of the graph to convey information to all the members of the graph (the vertices)
\end{frame}


\begin{frame}
	\begin{definition}[Connected graph]
	A \defword{connected graph} is a graph that contains a chain $\mu[x,y]$ for each pair $x,y$ of distinct vertices
	\end{definition}
	\vfill
	Denote $x\equiv y$ the relation  ``$x=y$, or $x\neq y$ and there exists a chain in $G$ connecting $x$ and $y$''. $\equiv$ is an equivalence relation since
	\begin{enumerate}
		\item $x\equiv y$ \hfill[reflexivity]
		\item $x\equiv y\implies y\equiv x$ \hfill[symmetry]
		\item $x\equiv y, y\equiv z\implies x\equiv z$ \hfill[transitivity]
	\end{enumerate}
	\begin{definition}[Connected component of a graph]
	The classes of the equivalence relation $\equiv$ partition $V$ into connected sub-graphs of $G$ called \defword{connected components}
	\end{definition}
	\end{frame}
	
\begin{frame}{Articulation set} 
	\begin{definition}[Articulation set]
	For a connected graph, a set $A$ of vertices is called an \defword{articulation set} (or a \defword{cutset}) if the subgraph of $G$ generated by $V-A$ is not connected
	\end{definition}
	\vfill
	{\tt articulation\_points(G)} in {\tt igraph} (assumes the graph is undirected, makes it so if not)
\end{frame}

	
	\begin{frame}{Strongly connected graphs}
		$G=(V,U)$ connected. 
		A \defword{path of length 0} is any sequence $\{x\}$ consisting of a single vertex $x\in V$
		\vfill
		For $x,y\in V$, let $x\equiv y$ be the relation ``there is a path $\mu_1[x,y]$ from $x$ to $y$ as well as a path $\mu_2[y,x]$ from $y$ to $x$''. This is an equivalence relation (it is reflexive, symmetric and transitive)
		\vfill
		\begin{definition}[Strong components]\label{def:strong_components}
		Sets of the form 
		\[
		A(x_0)=\{x:x\in V, x\equiv x_0\}	
		\]
		are equivalence classes; they partition $V$ and are the \defword{strongly connected components} of $G$	
		\end{definition}
		\vfill
		\begin{definition}[Strongly connected graph]
		\label{def:strongly_connected_graph}
		$G$ \defword{strongly connected} if it has a single strong component
		\end{definition}
\end{frame}
		

\begin{frame}
	\begin{definition}[Minimally connected graph]
	$G$ is \defword{minimally connected} if it is strongly connected and removal of any arc destroys strong-connectedness
	\end{definition}
	\vfill
	\begin{definition}[Contraction]
	$G=(V,U)$. The \defword{contraction} of the set $A\subset V$ of vertices consists in replacing $A$ by a single vertex $a$ and replacing each arc into (resp. out of) $A$ by an arc with same index into (resp. out of) $a$
	\end{definition}
\end{frame}
	
		
\begin{frame}{Quasi-strong connectedness}
	\begin{definition}[Quasi-strong connectedness]
	$G$ \defword{quasi-strongly connected} if $\forall x,y\in V$, exists $z\in V$ (denoted $z(x,y)$ to emphasize dependence on $x,y$) from which there is a path to $x$ and a path to $y$
	\end{definition}
	\vfill
	Strongly connected $\implies$ quasi-strongly connected (take $z(x,y)=x$); converse not true
	\vfill
	Quasi-strongly connected $\implies$ connected
	\vfill
	\begin{lemma}
	$G=(V,U)$ has a root $\iff$ $G$ quasi-strongly connected
	\end{lemma}
\end{frame}

\begin{frame}{Weak-connectedness}
\begin{definition}[Weakly connected graph]
	$G=(V,U)$ \defword{weakly connected} if $G=(V,E)$ connected, where $E$ is obtained from $U$ by ignoring the direction of arcs
\end{definition}
\end{frame}


\begin{frame}{Weak components}
Define for $x,y\in V$ the relation $x\equiv y$ as ``$x=y$ or $x\neq y$ and there is a chain in $G$ connecting $x$ and $y$'' [like for components in an undirected graph, except the graph is directed here]
\vfill
This defines an equivalence relation
\begin{definition}[Weak components]
Sets of the form 
\[
	A(x_0)=\{x:x\in V, x\equiv x_0\}
\]
are equivalence classes partitioning $V$ into the \defword{weakly connected components} of $G$
\end{definition}
\vfill
$G=(V,U)$ is weakly connected if there is a single weak component
\end{frame}

\begin{frame}{Cliques}
	\begin{definition}[Clique in undirected graphs]
		$G=(V,E)$ a simple undirected graph. A \defword{clique} is a subgraph $G'$ of $G$ such that all vertices in $G'$ are adjacent
	\end{definition}
	\begin{definition}[$n$-clique]
		A simple, complete graph on $n$ vertices is called an $n$-\defword{clique} and is often denoted $K_n$
	\end{definition}
	\begin{definition}[Clique in directed graphs]
		$G=(V,U)$ a simple directed graph. A \defword{clique} is a subgraph $G'$ of $G$ such that all vertices in $G'$ are mutually adjacent
	\end{definition}
	\begin{definition}[Maximal clique]
		A \defword{maximal clique} is a clique that cannot be extended by adding another adjacent vertex
	\end{definition}
\end{frame}

\begin{frame}{$k$-core}
	\begin{definition}[$k$-core of a graph]
		$G=(V,U)$ a graph. The $\mathbf{k}$-\defword{core} of $G$ is a maximal subgraph in which each vertex has degree at least $k$
	\end{definition}
	\vfill
	\begin{definition}[Coreness of a vertex]
		$G=(V,U)$ a graph, $x\in V$. The \defword{coreness} of $x$ is $k$ if $x$ belongs to the $k$-core of $G$ but not to the $k+1$ core of $G$
	\end{definition}
	\vfill
	For directed graphs, in-cores or out-cores depending on whether in-degree or out-degree is used
\end{frame}





\end{document}
