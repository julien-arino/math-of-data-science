\documentclass[aspectratio=169]{beamer}

% Set lecture number for later use
<<set-lecture-number,echo=FALSE>>=
lecture_number = "09"
@

% Part common to all the lectures
\subtitle{MATH 2740 -- Mathematics of Data Science -- Lecture \Sexpr{lecture_number}}
\author{\texorpdfstring{Julien Arino\newline\url{julien.arino@umanitoba.ca}}{Julien Arino}}
\institute{Department of Mathematics @ University of Manitoba}
\date{Fall 202X}

% Title of the lecture
\title{Matrix methods -- Singular value decomposition}

<<set-options,echo=FALSE,warning=FALSE,message=FALSE>>=
# Source the code common to all lectures
source("common-code.R")
@

<<set-slide-background,echo=FALSE,results='asis'>>=
# Are we plotting for a dark background? Setting is in common-code.R, but
# cat command must run here.
cat(input_setup)
@

\begin{document}

% Cross-reference setup to reference previous lectures
%\zexternaldocument{L08-matrix-methods-QR-2-SVD-1}

% Continue theorem numbering from L08 (manual setup - check L08 for current count)
% L08 has several theorems, so we continue from where it left off
\setcounter{theorem}{6}  % Adjust this number based on actual count in L08

% Set up cross-references and counter persistence

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE AND OUTLINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titlepagewithfigure{FIGS-slides-admin/Gemini_Generated_Image_7iz9ft7iz9ft7iz9.jpeg}
\outlinepage{FIGS-slides-admin/Gemini_Generated_Image_iyzqdwiyzqdwiyzq.jpeg}



%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{Computing the SVD}{FIGS-slides-admin/Gemini_Generated_Image_38bqnt38bqnt38bq.jpeg}

\begin{frame}{Computing the SVD (case of $\neq$ eigenvalues)}
To compute the SVD, we use the following result
\vfill
\begin{theorem}\label{th:eigenvectors_of_symmetric_are_orthogonal}
Let $A\in\M_n$ symmetric, $(\lambda_1,\bu_1)$ and $(\lambda_2,\bu_2)$ be eigenpairs, $\lambda_1\neq\lambda_2$. Then $\bu_1\bullet\bu_2=0$
\end{theorem}
\end{frame}

\begin{frame}{Proof of Theorem~\ref{th:eigenvectors_of_symmetric_are_orthogonal}}
$A\in\M_n$ symmetric, $(\lambda_1,\bu_1)$ and $(\lambda_2,\bu_2)$ eigenpairs with $\lambda_1\neq\lambda_2$
\begin{align*}
\lambda_1(\bv_1\bullet\bv_2) 
&= (\lambda_1\bv_1)\bullet\bv_2 \\
&= A\bv_1\bullet\bv_2 \\
&= (A\bv_1)^T\bv_2 \\
&= \bv_1^TA^T\bv_2 \\
&= \bv_1^T(A\bv_2)  \qquad\textrm{[$A$ symmetric so $A^T=A$]} \\
&= \bv_1^T(\lambda_2\bv_2) \\
&= \lambda_2(\bv_1^T\bv_2) \\
&= \lambda_2(\bv_1\bullet\bv_2)
\end{align*}
\vfill
So $(\lambda_1-\lambda_2)(\bv_1\bullet\bv_2)=0$. But $\lambda_1\neq\lambda_2$, so $\bv_1\bullet\bv_2=0$\hfill\qed
\end{frame}


\begin{frame}{Computing the SVD (case of $\neq$ eigenvalues)}
If all eigenvalues of $A^TA$ (or $AA^T$) are distinct, we can use Theorem~\ref{th:eigenvectors_of_symmetric_are_orthogonal}
\vfill
\begin{enumerate}
\item Compute $A^TA\in\M_n$
\item Compute eigenvalues $\lambda_1,\ldots,\lambda_n$ of $A^TA$; order them as $\lambda_1>\cdots>\lambda_n\geq 0$ ($>$ not $\geq$ since $\neq$)
\item Compute singular values $\sigma_1=\sqrt{\lambda_1},\ldots,\sigma_n=\sqrt{\lambda_n}$
\item Diagonal matrix $D$ in $\Sigma$ is either in $\M_n$ (if $\sigma_n>0$) or in $\M_{n-1}$ (if $\sigma_n=0$)
\end{enumerate}
\end{frame}


\begin{frame}
\begin{enumerate}
\setcounter{enumi}{4}
\item Since eigenvalues are distinct, Theorem~\ref{th:eigenvectors_of_symmetric_are_orthogonal} $\implies$ eigenvectors are orthogonal set. Compute these eigenvectors in the same order as the eigenvalues
\item Normalise them and use them to make the matrix $V$, i.e., $V=[\bv_1\cdots\bv_n]$
\item To find the $\bu_i$, compute, for $i=1,\ldots,r$,
\[
\bu_i = \frac{1}{\sigma_i}A\bv_i
\]
and ensure that $\|\bu_i\|=1$
\end{enumerate}
\end{frame}


\begin{frame}{Computing the SVD (case where some eigenvalues are $=$)}
\begin{enumerate}
\item Compute $A^TA\in\M_n$
\item Compute eigenvalues $\lambda_1,\ldots,\lambda_n$ of $A^TA$; order them as $\lambda_1\geq\cdots\geq\lambda_n\geq 0$
\item Compute singular values $\sigma_1=\sqrt{\lambda_1},\ldots,\sigma_n=\sqrt{\lambda_n}$, with $r\leq n$ the index of the last positive singular value
\item For eigenvalues that are distinct, proceed as before
\item For eigenvalues with multiplicity $>1$, we need to ensure that the resulting eigenvectors are LI \emph{and} orthogonal
\end{enumerate}
\end{frame}

\begin{frame}{Dealing with eigenvalues with multiplicity $>1$}
When an eigenvalue has (algebraic) multiplicity $>1$, e.g., characteristic polynomial contains a factor like $(\lambda-2)^2$, things can become a little bit more complicated
\vfill
The proper way to deal with this involves the so-called Jordan Normal Form (another matrix decomposition)
\vfill
In short: not all square matrices are diagonalisable, but all square matrices admit a JNF
\end{frame}


\begin{frame}
Sometimes, we can find several LI eigenvectors associated to the same eigenvalue. Check this. If not, need to use the following
\vfill
\begin{definition}[Generalised eigenvectors]
The vector $\bx\neq\b0$ is a \textbf{generalized eigenvector} of rank $m$ of $A\in\M_n$ corresponding to eigenvalue $\lambda$ if
\[
(A-\lambda\II)^{m}\bx = \b0
\]
but
\[
(A-\lambda\II)^{m-1}\bx\neq \b0
\]
\end{definition}
\end{frame}


\begin{frame}{Procedure for generalised eigenvectors}
$A\in\M_n$ and assume $\lambda$ eigenvalue with algebraic multiplicity $k$
\vfill
Find $\bv_1$, ``classic" eigenvector, i.e., $\bv_1\neq\b0$ s.t. $(A-\lambda\II)\bv_1=\b0$
\vfill
Find generalised eigenvector $\bv_2$ of rank 2 by solving for $\bv_2\neq\b0$,
\[
(A-\lambda\II)\bv_2 = \bv_1
\]
$\ldots$
\vfill
Find generalised eigenvector $\bv_k$ of rank $k$ by solving for $\bv_k\neq\b0$,
\[
(A-\lambda\II)\bv_k = \bv_{k-1}
\]
\vfill
Then $\{\bv_1,\ldots,\bv_k\}$ LI
\end{frame}


\begin{frame}{Back to the normal procedure}
With the LI eigenvectors $\{\bv_1,\ldots,\bv_k\}$ corresponding to $\lambda$
\vfill
Apply Gram-Schmidt to get orthogonal set
\vfill
For all eigenvalues with multiplicity $>1$, check that you either have LI eigenvectors or do what we just did
\vfill
When you are done, be back on your merry way to step 6 in the case where eigenvalues are all $\neq$
\vfill
I am caricaturing a little here: there can be cases that do not work exactly like this, but this is general enough..
\end{frame}


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{Applications of the SVD -- Least squares}{FIGS-slides-admin/Gemini_Generated_Image_38bqnt38bqnt38bq.jpeg}


\begin{frame}{Pseudoinverse of a matrix}
\begin{definition}[Pseudoinverse]
$A=U\Sigma V^T$ an SVD for $A\in\M_{mn}$, where 
\[
\Sigma = \begin{pmatrix}
D & 0 \\ 0 & 0
\end{pmatrix},
\textrm{ with }
D=\mathsf{diag}(\sigma_1,\ldots,\sigma_r)
\]
($D$ contains the nonzero singular values of $A$ ordered as usual)
\vskip0.5cm
The \textbf{pseudoinverse} (or \textbf{Moore-Penrose inverse}) of $A$ is $A^+\in\M_{nm}$ given by
\[
A^+ = V\Sigma^+ U^T
\]
with
\[
\Sigma^+ =
\begin{pmatrix}
D^{-1} & 0 \\ 0 & 0
\end{pmatrix}\in\M_{nm}
\]
\end{definition}
\end{frame}



\begin{frame}{Least squares revisited}
\begin{theorem}
Let $A\in\M_{mn}$, $\bx\in\IR^n$ and $\bb\in\IR^m$. The least squares problem $A\bx=\bb$ has a unique least squares solution $\tilde\bx$ of \emph{minimal length} (closest to the origin) given by
\[
\tilde\bx = A^+\bb
\]
where $A^+$ is the \emph{pseudoinverse} of $A$
\end{theorem}
\end{frame}




%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{Summary -- Least squares methods}{FIGS-slides-admin/Gemini_Generated_Image_38bqnt38bqnt38bq.jpeg}

\begin{frame}{The least squares problem}
\textbf{Problem Statement:}
\vfill
Given a system $A\bx = \bb$ where $A\in\M_{mn}$, $\bx\in\IR^n$, $\bb\in\IR^m$ (typically $m>n$), find $\tilde\bx$ that minimizes
$$
\|\bb - A\bx\|^2 = \sum_{i=1}^m (b_i - \sum_{j=1}^n A_{ij}x_j)^2
$$
\vfill
\textbf{Geometric interpretation:}
Find the vector $A\tilde\bx$ in the column space of $A$ that is closest to $\bb$
\vfill
\textbf{Solution:} $A\tilde\bx = \text{proj}_{\text{col}(A)}(\bb)$
\end{frame}

\begin{frame}{Method 1: Normal equations}
\textbf{The normal equations:}
$$A^TA\tilde\bx = A^T\bb$$
\vfill
\textbf{When this works:}
\begin{itemize}
\item Always has at least one solution
\item Any solution $\tilde\bx$ to the normal equations is a least squares solution
\end{itemize}
\vfill
\textbf{Computational issues:}
\begin{itemize}
\item Forming $A^TA$ can be numerically unstable
\item Condition number of $A^TA$ is the square of the condition number of $A$
\item Still useful for theoretical analysis
\end{itemize}
\end{frame}

\begin{frame}{Method 2: when $A$ Has linearly independent columns}
\textbf{Condition:} $A\in\M_{mn}$ has linearly independent columns
\vfill
\textbf{Then:} $A^TA$ is invertible and the least squares solution is \textbf{unique}
$$\tilde\bx = (A^TA)^{-1}A^T\bb$$
\vfill
\textbf{Properties:}
\begin{itemize}
\item $A^TA \in \M_n$ is square, symmetric, and positive definite
\item $(A^TA)^{-1}A^T$ is called the \emph{left pseudoinverse} of $A$
\item This gives the unique least squares solution
\end{itemize}
\vfill
\textbf{Drawback:} Computing $(A^TA)^{-1}$ directly can be numerically unstable
\end{frame}

\begin{frame}{Method 3: QR factorization}
\textbf{QR Factorization:} If $A\in\M_{mn}$ has linearly independent columns, then
$$A = QR$$
where $Q\in\M_{mn}$ has orthonormal columns and $R\in\M_n$ is upper triangular and nonsingular
\vfill
\textbf{Least squares solution:}
$$\tilde\bx = R^{-1}Q^T\bb$$
\vfill
\textbf{Advantages:}
\begin{itemize}
\item More numerically stable than forming $A^TA$
\item $R$ is upper triangular $\Rightarrow$ solving $R\tilde\bx = Q^T\bb$ by back substitution
\item Condition number of $R$ equals condition number of $A$
\item Gram-Schmidt or Householder reflections can compute QR factorization
\end{itemize}
\end{frame}

\begin{frame}{Method 4: Singular Value Decomposition (SVD)}
\textbf{SVD:} For any $A\in\M_{mn}$,
$$A = U\Sigma V^T$$
where $U\in\M_m$ orthogonal, $V\in\M_n$ orthogonal, $\Sigma\in\M_{mn}$ with $\Sigma_{ii} = \sigma_i \geq 0$ (singular values)
\vfill
\textbf{Pseudoinverse:} $A^+ = V\Sigma^+ U^T$ where $\Sigma^+$ has $(\Sigma^+)_{ii} = 1/\sigma_i$ if $\sigma_i > 0$, else $0$
\vfill
\textbf{Least squares solution:}
$$\tilde\bx = A^+\bb$$
\vfill
\textbf{Key advantages:}
\begin{itemize}
\item Works for \emph{any} matrix $A$ (even when columns are linearly dependent)
\item Gives the solution of \emph{minimal length} when multiple solutions exist
\item Most numerically stable method
\item Reveals the rank of $A$ through the number of non-zero singular values
\end{itemize}
\end{frame}

\begin{frame}{When to use which method}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Method} & \textbf{When to use} & \textbf{Advantages/Drawbacks} \\
\hline
Normal equations & Theory, small problems & Simple, but unstable \\
\hline
$(A^TA)^{-1}A^T$ & $A$ has LI columns & Explicit formula, unstable \\
\hline
QR Factorization & $A$ has LI columns & Stable, efficient \\
\hline
SVD & Any $A$, rank-deficient & Most stable, handles all cases \\
\hline
\end{tabular}
\end{center}
\vfill
Use QR for well-conditioned problems with LI columns, SVD for rank-deficient or ill-conditioned problems
\end{frame}

% Save counters for next file

% Save theorem count for next file
\end{document}
