\documentclass[aspectratio=169]{beamer}

% Set lecture number for later use
<<set-lecture-number,echo=FALSE>>=
lecture_number = "11"
@

% Part common to all the lectures
\subtitle{MATH 2740 -- Mathematics of Data Science -- Lecture \Sexpr{lecture_number}}
\author{\texorpdfstring{Julien Arino\newline\url{julien.arino@umanitoba.ca}}{Julien Arino}}
\institute{Department of Mathematics @ University of Manitoba}
\date{Fall 202X}

% Title of the lecture
\title{Matrix methods -- Principal component analysis (2)}

<<set-options,echo=FALSE,warning=FALSE,message=FALSE>>=
# Source the code common to all lectures
source("common-code.R")
@

<<set-slide-background,echo=FALSE,results='asis'>>=
# Are we plotting for a dark background? Setting is in common-code.R, but
# cat command must run here.
cat(input_setup)
@

\begin{document}

% Set up cross-references and counter persistence

% Set up cross-references and counter persistence

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE AND OUTLINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titlepagewithfigure{FIGS-slides-admin/Gemini_Generated_Image_7iz9ft7iz9ft7iz9.jpeg}
\outlinepage{FIGS-slides-admin/Gemini_Generated_Image_iyzqdwiyzqdwiyzq.jpeg}



%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{Back to PCA}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}


\begin{frame}{Setting things up}
I will use notation (mostly) as in Joliffe's \emph{Principal Component Analysis} (PDF of older version available for free from UofM Libraries)
\vfill
$\bx=(x_1,\ldots,x_p)$ vector of $p$ random variables
\end{frame}


\begin{frame} 
We seek a linear function $\bm{\alpha}_1^T\bx$ with maximum variance, where $\bm{\alpha}_1=(\alpha_{11},\ldots,\alpha_{1p})$, i.e.,
\[
\bm{\alpha}_1^T\bx = \sum_{j=1}^p\alpha_{1j}x_j
\]
\vfill
Then we seek a linear function $\bm{\alpha}_2^T\bx$ with maximum variance, uncorrelated to $\bm{\alpha}_1^T\bx$
\vfill
And we continue...
\vfill
At $k$th stage, we find a linear function $\bm{\alpha}_k^T\bx$ with maximum variance, uncorrelated to $\bm{\alpha}_1^T\bx,\ldots,\bm{\alpha}_{k-1}^T\bx$
\vfill
$\bm{\alpha}_i^T\bx$ is the $i$th \textbf{principal component} (PC)
\end{frame}

\begin{frame}{Case of known covariance matrix}
Suppose we know $\Sigma$, covariance matrix of $\bx$ (i.e., typically: we know $\bx$)
\vfill
Then the $k$th PC is 
\[
z_k=\bm{\alpha}_k^T\bx
\]
where $\bm{\alpha}_k$ is an eigenvector of $\Sigma$ corresponding to the $k$th largest eigenvalue $\lambda_k$
\vfill
If, additionally, $\|\bm{\alpha}_k\|=\bm{\alpha}_k^T\bm{\alpha}=1$, then $\lambda_k=\Var z_k$
\end{frame}


\begin{frame}{Why is that?}
Let us start with
\[
\bm{\alpha}_1^T\bx
\]
\vfill
We want maximum variance, where $\bm{\alpha}_1=(\alpha_{11},\ldots,\alpha_{1p})$, i.e.,
\[
\bm{\alpha}_1^T\bx = \sum_{j=1}^p\alpha_{1j}x_j
\]
with the constraint that $\|\bm{\alpha}_1\|=1$
\vfill
We have
\[
\Var \bm{\alpha}_1^T\bx
=\bm{\alpha}_1^T\Sigma\bm{\alpha}_1
\]
\end{frame}

\begin{frame}{Objective}
We want to maximise $\Var \bm{\alpha}_1^T\bx$, i.e.,
\[
\bm{\alpha}_1^T\Sigma\bm{\alpha}_1
\]
under the constraint that $\|\bm{\alpha}_1\|=1$
\vfill
$\implies$ use \textbf{Lagrange multipliers}
\end{frame}


\begin{frame}{Maximisation using Lagrange multipliers}
\framesubtitle{(A.k.a. super-brief intro to multivariable calculus)}
We want the max of $f(x_1,\ldots,x_n)$ under the constraint $g(x_1,\ldots,x_n)=k$
\begin{enumerate}
\item Solve
\begin{align*}
\nabla f(x_1,\ldots,x_n) &= \lambda\nabla g(x_1,\ldots,x_n) \\
g(x_1,\ldots,x_n) &= k
\end{align*}
where $\nabla=(\frac{\partial}{\partial x_1},\ldots,\frac{\partial}{\partial x_n})$ is the \textbf{gradient operator}
\item Plug all solutions into $f(x_1,\ldots,x_n)$ and find maximum values (provided values exist and $\nabla g\neq \b0$ there)
\end{enumerate}
\vfill
$\lambda$ is the \textbf{Lagrange multiplier}
\end{frame}


\begin{frame}{The gradient}
\framesubtitle{(Continuing our super-brief intro to multivariable calculus)}
$f:\IR^n\to\IR$ function of several variables, $\nabla=\left(\frac{\partial}{\partial x_1},\ldots,\frac{\partial}{\partial x_n}\right)$ the gradient operator
\vfill
Then
\[
\nabla f = \left(
\frac{\partial}{\partial x_1}f,\ldots,
\frac{\partial}{\partial x_n}f
\right)
\]
\vfill
So $\nabla f$ is a \emph{vector-valued} function, $\nabla f:\IR^n\to\IR^n$; also written as
\[
\nabla f = f_{x_1}(x_1,\ldots,x_n)\be_1+\cdots f_{x_n}(x_1,\ldots,x_n)\be_n
\]
where $f_{x_i}$ is the partial derivative of $f$ with respect to $x_i$ and $\{\be_1,\ldots,\be_n\}$ is the standard basis of $\IR^n$
\end{frame}


\begin{frame}{Bear with me..}
\framesubtitle{(You may experience a brief period of discomfort)}
$\bm{\alpha}_1^T\Sigma\bm{\alpha}_1$ and $\|\bm{\alpha}_1\|^2=\bm{\alpha}_1^T\bm{\alpha_1}$ are functions of $\bm{\alpha}_1=(\alpha_{11},\ldots,\alpha_{1p})$
\vfill
In the notation of the previous slide, we want the max of 
\[
f(\alpha_{11},\ldots,\alpha_{1p}) := \bm{\alpha}_1^T\Sigma\bm{\alpha}_1
\]
under the constraint that
\[
g(\alpha_{11},\ldots,\alpha_{1p}) := \bm{\alpha}_1^T\bm{\alpha_1} = 1
\]
and with gradient operator
\[
\nabla = \left(
\frac{\partial}{\partial \alpha_{11}},
\ldots,
\frac{\partial}{\partial \alpha_{1p}}
\right)
\]
\end{frame}


\begin{frame}{Effect of $\nabla$ on $g$}
$g$ is easiest to see:
\begin{align*}
\nabla g(\alpha_{11},\ldots,\alpha_{1p}) &=
\left(
\frac{\partial}{\partial \alpha_{11}},
\ldots,
\frac{\partial}{\partial \alpha_{1p}}
\right) (\alpha_{11},\ldots,\alpha_{1p}) 
\begin{pmatrix}
\alpha_{11}\\ \vdots\\ \alpha_{1p}
\end{pmatrix} \\
&= \left(
\frac{\partial}{\partial \alpha_{11}},
\ldots,
\frac{\partial}{\partial \alpha_{1p}}
\right) 
\left(
\alpha_{11}^2+\cdots+\alpha_{1p}^2
\right) \\
&= \left(2\alpha_{11},\ldots,2\alpha_{1p}\right)\\
&= 2\bm{\alpha}_1
\end{align*}
\vfill
(And that's a general result: $\nabla\|\bx\|_2^2=2\bx$ with $\|\cdot\|_2$ the Euclidean norm)
\end{frame}

\begin{frame}{Effect of $\nabla$ on $f$}
Expand (write $\Sigma=[s_{ij}]$ and do not exploit symmetry)
\begin{align*}
\bm{\alpha}_1^T\Sigma\bm{\alpha}_1 &=
\left(\alpha_{11},\ldots,\alpha_{1p}\right)
\begin{pmatrix}
s_{11} & s_{12} & \cdots & s_{1p} \\
s_{21} & s_{22} & \cdots & s_{2p} \\
\vdots & \vdots & & \vdots \\
s_{p1} & s_{p2} & & s_{pp}
\end{pmatrix}
\begin{pmatrix}
\alpha_{11} \\ \alpha_{12} \\ \vdots \\ \alpha_{1p}
\end{pmatrix} \\
&=
\left(\alpha_{11},\ldots,\alpha_{1p}\right)
\begin{pmatrix}
s_{11}\alpha_{11}+s_{12}\alpha_{12}+\cdots+s_{1p}\alpha_{1p} \\
s_{21}\alpha_{11}+s_{22}\alpha_{12}+\cdots+s_{2p}\alpha_{1p} \\
\vdots \\
s_{p1}\alpha_{11}+s_{p2}\alpha_{12}+\cdots+s_{pp}\alpha_{1p}
\end{pmatrix} \\
&=
(s_{11}\alpha_{11}+s_{12}\alpha_{12}+\cdots+s_{1p}\alpha_{1p})\alpha_{11} \\
&\quad +
(s_{21}\alpha_{11}+s_{22}\alpha_{12}+\cdots+s_{2p}\alpha_{1p})\alpha_{12} \\
&\quad\;\;\vdots \\
&\quad +
(s_{p1}\alpha_{11}+s_{p2}\alpha_{12}+\cdots+s_{pp}\alpha_{1p})\alpha_{1p}
\end{align*}
\end{frame}

\begin{frame}
We have
\begin{align*}
\bm{\alpha}_1^T\Sigma\bm{\alpha}_1 &=
(s_{11}\alpha_{11}+s_{12}\alpha_{12}+\cdots+s_{1p}\alpha_{1p})\alpha_{11} \\
&\quad +
(s_{21}\alpha_{11}+s_{22}\alpha_{12}+\cdots+s_{2p}\alpha_{1p})\alpha_{12} \\
&\quad\;\;\vdots \\
&\quad +
(s_{p1}\alpha_{11}+s_{p2}\alpha_{12}+\cdots+s_{pp}\alpha_{1p})\alpha_{1p} 
\end{align*}
\begin{align*}
\implies\frac{\partial}{\partial \alpha_{11}}
\bm{\alpha}_1^T\Sigma\bm{\alpha}_1  
&= 
(s_{11}\alpha_{11}+s_{12}\alpha_{12}+\cdots+s_{1p}\alpha_{1p})+s_{11}\alpha_{11} \\
&\quad + s_{21}\alpha_{12} +\cdots + s_{p1}\alpha_{1p} \\
&= s_{11}\alpha_{11}+s_{12}\alpha_{12}+\cdots+s_{1p}\alpha_{1p} \\
&\quad+
s_{11}\alpha_{11}+s_{21}\alpha_{12}+\cdots+s_{p1}\alpha_{1p} \\
&= 2(s_{11}\alpha_{11}+s_{12}\alpha_{12}+\cdots+s_{1p}\alpha_{1p})
\end{align*}
(last equality stems from symmetry of $\Sigma$)
\end{frame}

\begin{frame}
In general, for $i=1,\ldots,p$,
\begin{align*}
\frac{\partial}{\partial \alpha_{1i}}
\bm{\alpha}_1^T\Sigma\bm{\alpha}_1  
&= s_{i1}\alpha_{11}+s_{i2}\alpha_{12}+\cdots+s_{ip}\alpha_{1p}\\
&\quad+s_{i1}\alpha_{11}+s_{2i}\alpha_{12}+\cdots+s_{pi}\alpha_{1p} \\
&= 2(s_{i1}\alpha_{11}+s_{i2}\alpha_{12}+\cdots+s_{ip}\alpha_{1p})
\end{align*}
(because of symmetry of $\Sigma$)
\vfill
As a consequence,
\[
\nabla \bm{\alpha}_1^T\Sigma\bm{\alpha}_1
=2\Sigma\bm{\alpha}_1
\]
\end{frame}

\begin{frame}
So solving
\[
\nabla f(x_1,\ldots,x_n) = \lambda\nabla g(x_1,\ldots,x_n) 
\]
means solving
\[
2\Sigma\bm{\alpha}_1 = \lambda 2\bm{\alpha}_1 
\]
i.e.,
\[
\Sigma\bm{\alpha}_1 = \lambda\bm{\alpha}_1 
\]
\vfill
$\implies$
$(\lambda,\bm{\alpha}_1)$ eigenpair of $\Sigma$, with $\bm{\alpha}_1$ having unit length
\end{frame}


\begin{frame}{Picking the right eigenvalue}
$(\lambda,\bm{\alpha}_1)$ eigenpair of $\Sigma$, with $\bm{\alpha}_1$ having unit length
\vfill
But which $\lambda$ to choose?
\vfill
Recall that we want $\Var \bm{\alpha}_1^T\bx=\bm{\alpha}_1^T\Sigma\bm{\alpha}_1$ maximal
\vfill
We have
\[
\Var \bm{\alpha}_1^T\bx 
= \bm{\alpha}_1^T\Sigma\bm{\alpha}_1 
= \bm{\alpha}_1^T(\Sigma\bm{\alpha}_1) 
= \bm{\alpha}_1^T(\lambda\bm{\alpha}_1) 
= \lambda(\bm{\alpha}_1^T\bm{\alpha}_1) = \lambda
\]
\vfill
$\implies$ we pick $\lambda=\lambda_1$, the largest eigenvalue (covariance matrix symmetric so eigenvalues real)
\end{frame}


\begin{frame}{What we have this far..}
The first principal component is $\bm{\alpha}_1^T\bx$ and has variance $\lambda_1$, where $\lambda_1$ the largest eigenvalue of $\Sigma$ and $\bm{\alpha}_1$ an associated eigenvector with $\|\bm{\alpha}_1\|=1$
\vfill
We want the second principal component to be \emph{uncorrelated} with $\bm{\alpha}_1^T\bx$ and to have maximum variance $\Var \bm{\alpha}_2^T\bx=\bm{\alpha}_2^T\Sigma\bm{\alpha}_2$, under the constraint that $\|\bm{\alpha}_2\|=1$
\vfill
$\bm{\alpha}_2^T\bx$ uncorrelated to $\bm{\alpha}_1^T\bx$ if $\cov(\bm{\alpha}_1^T\bx,\bm{\alpha}_2^T\bx)=0$
\end{frame}

\begin{frame}
We have
\begin{align*}
\cov(\bm{\alpha}_1^T\bx,\bm{\alpha}_2^T\bx) &= 
\bm{\alpha}_1^T\Sigma\bm{\alpha}_2 \\
&= \bm{\alpha}_2^T\Sigma^T\bm{\alpha}_1 \\
&= \bm{\alpha}_2^T\Sigma\bm{\alpha}_1 \quad\textrm{[$\Sigma$ symmetric]} \\
&= \bm{\alpha}_2^T(\lambda_1\bm{\alpha}_1) \\
&= \lambda \bm{\alpha}_2^T\bm{\alpha}_1
\end{align*}
\vfill
So $\bm{\alpha}_2^T\bx$ uncorrelated to $\bm{\alpha}_1^T\bx$ if $\bm{\alpha}_1\perp\bm{\alpha}_2$
\vfill
This is beginning to sound a lot like Gram-Schmidt, no?
\end{frame}

\begin{frame}{In short}
Take whatever covariance matrix is available to you (known $\Sigma$ or sample $S_X$) -- assume sample from now on for simplicity
\vfill
For $i=1,\ldots,p$, the $i$th principal component is
\[
z_i = \bv_i^T\bx
\]
where $\bv_i$ eigenvector of $S_X$ associated to the $i$th largest eigenvalue $\lambda_i$
\vfill
If $\bv_i$ is normalised, then $\lambda_i=\Var z_k$
\end{frame}


\begin{frame}{Covariance matrix}
$\Sigma$ the covariance matrix of the random variable, $S_X$ the sample covariance matrix
\vfill
$X\in\M_{mp}$ the data, then the (sample) covariance matrix $S_X$ takes the form
\[
S_X = \frac{1}{n-1}X^TX
\]
where the data is centred!
\vfill
Sometimes you will see $S_X=1/(n-1)XX^T$. This is for matrices with observations in columns and variables in rows. Just remember that you want the covariance matrix to have size the number of variables, not observations, this will give you the order in which to take the product
\end{frame}



%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{Hockey players}{FIGS-slides-admin/Gemini_Generated_Image_ee8lhqee8lhqee8l.jpeg}

<<load-hockey-data, echo=FALSE>>=
# Load the hockey data (as in previous lecture) and centre
# it.
# From https://figshare.com/ndownloader/files/5303173
data = read.csv("https://github.com/julien-arino/math-of-data-science/raw/refs/heads/main/DATA/hockey-players.csv")
data$weight.c = data$weight-mean(data$weight)
data$height.c = data$height-mean(data$height)
@

\begin{frame}[fragile]\frametitle{Covariance}
The function \code{cov} returns the covariance of two samples
\vfill
Note that the functions deals equally well with data that is not centred as with data that is centred
\vfill
<<>>=
cov(data$height, data$weight)
cov(data$height.c, data$weight.c)
@
\end{frame}


\begin{frame}[fragile]\frametitle{Covariance matrix}
As we could see from plotting the data, there is a positive linear relationship between the two variables
\vfill
Let us compute the sample covariance matrix
\vfill
<<>>=
X = as.matrix(data[,c("height.c", "weight.c")])
S = 1/(dim(X)[1]-1)*t(X) %*% X
S
@
\end{frame}

\begin{frame}[fragile]\frametitle{Covariance matrix}
The off-diagonal entries do match the computed covariance. Let us check that the variances are indeed a match too.


<<>>=
var(X[,1])
var(X[,2])
@


Hey, that works. Is math not cool? ;)
\end{frame}


\begin{frame}[fragile]\frametitle{Principal components}
Now compute the principal components. We need eigenvalues and eigenvectors
\vfill
<<>>=
ev = eigen(S)
ev
@
\vfill
(\code{eigen} returns eigenvalues sorted in decreasing order and normalised eigenvectors)
\end{frame}



\begin{frame}[fragile]\frametitle{First principal component}
Let us plot this first eigenvector (well, the line carrying this first eigenvector) 
\vfill
To use the function \code{abline}, we need to give the coefficients of the line in the form of (intercept,slope). Intercept is easy, as the line goes through the origin (by construction and because we have centred the data). The slope is also quite simple..
\vfill
<<plot-hockey-centred-evector>>=
plot(data$height.c, data$weight.c,
    pch = 19, col = "dodgerblue4",
    main = "IIHF players 2001-2016 (with first component)",
    xlab = "Height (cm)", ylab = "Weight (kg)")
abline(a = 0, b = ev$vectors[2,1]/ev$vectors[1,1],
       col = "red", lwd = 3)
@
\end{frame}

\maxFrameImage{\Sexpr{knitr::fig_chunk("plot-hockey-centred-evector", "pdf")}}

\begin{frame}[fragile]\frametitle{Rotating the data}
Let us rotate the data so that the red line becomes the $x$-axis
\vfill
To do that, we use a rotation matrix
$$
R_\theta = \begin{pmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{pmatrix}
$$
\vfill
To find the angle $\theta$, recall that $\tan\theta$ is equal to opposite length over adjacent length, i.e.,
$$
\tan\theta = \frac{\tt ev\$vectors[2,1]}{\tt ev\$vectors[1,1]}
$$
So we just use the $\arctan$ of this 
\vfill 
Note that angles are in radians
\end{frame}


\begin{frame}[fragile]\frametitle{Rotating the data}
<<>>=
theta = atan(ev$vectors[2,1]/ev$vectors[1,1])
theta
R_theta = matrix(c(cos(theta), -sin(theta),
                  sin(theta), cos(theta)),
                nr = 2, byrow = TRUE)
R_theta
@
\end{frame}


\begin{frame}[fragile]\frametitle{Rotating the data}
And now we rotate the points
\vfill
(In this case, we think of the points as vectors, of course)
\vfill
<<>>=
tmp_in = matrix(c(data$weight.c, data$height.c),
                nc = 2)
tmp_out = c()
for (i in 1:dim(tmp_in)[1]) {
    tmp_out = rbind(tmp_out,
                    t(R_theta %*% tmp_in[i,]))
}
data$weight.c_r = tmp_out[,1]
data$height.c_r = tmp_out[,2]
@
\end{frame}


<<plot-hockey-centred-evector-2,echo=FALSE>>=
plot(data$height.c_r, data$weight.c_r,
    pch = 19, col = "dodgerblue4",
    main = "IIHF players 2001-2016 (rotated to first component)",
    xlab = "x-axis", ylab = "y-axis")
@

\maxFrameImage{\Sexpr{knitr::fig_chunk("plot-hockey-centred-evector-2", "pdf")}}


\begin{frame}[fragile]\frametitle{Principal components}
Note that the axes have changed quite a lot, hence the very different aspect
\vfill
Let us plot with the same range as for the non-rotated data for the y-axis
\vfill

<<plot-hockey-centred-rotated>>=
plot(data$height.c_r, data$weight.c_r,
    pch = 19, col = "dodgerblue4",
    xlab = "x-axis", ylab = "y-axis",
    main = "IIHF players 2001-2016 (rotated to first component)",
    ylim = range(data$weight.c))
abline(h = 0, col = "red", lwd = 2)
@
\end{frame}


\maxFrameImage{\Sexpr{knitr::fig_chunk("plot-hockey-centred-rotated", "pdf")}}


\begin{frame}[fragile]\frametitle{First and second principal components}
Plot the first and second eigenvectors
\vfill
<<plot-hockey-centred-2evectors>>=
plot(data$height.c, data$weight.c,
    pch = 19, col = "dodgerblue4",
    main = "IIHF players 2001-2016 (with first and second components)",
    xlab = "Height (cm)", ylab = "Weight (kg)")
abline(a = 0, b = ev$vectors[2,1]/ev$vectors[1,1],
       col = "red", lwd = 3)
abline(a = 0, b = ev$vectors[2,2]/ev$vectors[1,2],
       col = "darkgreen", lwd = 3)
@
\end{frame}

\maxFrameImage{\Sexpr{knitr::fig_chunk("plot-hockey-centred-2evectors", "pdf")}}


\begin{frame}\frametitle{Proper change of basis}
Let us change the basis so that, in the new basis, the first component is the $x$-axis and the second component is the $y$-axis
\vfill
We want to use Theorem~\ref{th:change-basis-construction}
\vfill
We need the coordinates of the new basis in the canonical basis of $\IR^2$
\vfill
Since both axes go through the origin, we can just use $y=ax$, with $a$ the slope of the lines and, say, $x=1$, i.e., $(x,y)=(1,a)$
\vfill
We then normalise the resulting vectors
\end{frame}


\begin{frame}[fragile]\frametitle{Proper change of basis}
<<plot-hockey-proper-changed-basis,echo=1:12>>=
red_line = c(1, ev$vectors[2,1]/ev$vectors[1,1])
red_line = red_line/sqrt(sum(red_line^2))
green_line = c(1, ev$vectors[2,2]/ev$vectors[1,2])
green_line = green_line/sqrt(sum(green_line^2))
augmented_M = cbind(red_line,green_line, diag(2))
P = rref(augmented_M)[,3:4]

tmp_in = matrix(c(data$weight.c, data$height.c), nc = 2)
tmp_out = c()
for (i in 1:dim(tmp_in)[1]) {
    tmp_out = rbind(tmp_out, t(P %*% tmp_in[i,]))
}
data$weight.c_r2 = tmp_out[,1]
data$height.c_r2 = tmp_out[,2]
plot(data$height.c_r2, data$weight.c_r2,
    pch = 19, col = "dodgerblue4",
    xlab = "x-axis", ylab = "y-axis",
    main = "IIHF players 2001-2016 (rotated to first component)",
    ylim = range(data$weight.c))
abline(h = 0, col = "red", lwd = 2)
abline(v = 0, col = "darkgreen", lwd = 2)
@
\end{frame}

\maxFrameImage{\Sexpr{knitr::fig_chunk("plot-hockey-proper-changed-basis", "pdf")}}


\begin{frame}[fragile]\frametitle{PCA using built-in functions}
Now do things ``properly''
<<>>=
GS = pracma::gramSchmidt(A = ev$vectors, tol = 1e-10)
GS
@
\end{frame}


\begin{frame}[fragile]\frametitle{PCA using built-in functions}
Now recall we saw a theorem that told us how to construct a new basis..
\vfill
<<pca-built-in-functions>>=
A=matrix(c(GS$Q,1,0,0,1), nr = 2)
A
pracma::rref(A)
@
\end{frame}

\begin{frame}[fragile]\frametitle{PCA using built-in functions}
<<plot-hockey-centred-evector-3,echo=c(1,3,4)>>=
P = pracma::rref(A)[,c(3,4)]
P
X.new = X %*% t(P)

plot(X.new[,1], X.new[,2],
     xlab = "x-axis", ylab = "y-axis",
     main = "IIHF players 2001-2016 (rotated to first component)",
     pch = 19, col = "dodgerblue4")
@
\end{frame}

\maxFrameImage{\Sexpr{knitr::fig_chunk("plot-hockey-centred-evector-3", "pdf")}}



% %%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%
% \subsection{Back to fingerprints}
% \newSubSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_ut188hut188hut18.jpeg}
% 
% 
% \begin{frame}
% We get the data from \href{https://repository.lboro.ac.uk/articles/dataset/Height_weight_and_fingerprint_measurements_collected_from_200_participants/7539206}{here}
% \vfill
% This time, we first download the data, then open the file
% \vfill
% The file is an excel table, so we need to use a library for doing that
% \end{frame}
% 
% 
% \begin{frame}[fragile]\frametitle{Loading the excel fingerprint data}
% <<open-file>>=
% download.file(url = "https://repository.lboro.ac.uk/ndownloader/files/14015774",
%              destfile = "../CODE/fingerprint_data.xlsx")
% data = openxlsx::read.xlsx("../CODE/fingerprint_data.xlsx")
% head(data, n=3)
% @
% \end{frame}
% 
% 
% \begin{frame}[fragile]\frametitle{Some wrangling}
% Let us rework the names of columns a bit, for convenience. Let us also get rid of a few columns we are not using
% \vfill
% <<start-wrangling>>=
% data = data[,2:dim(data)[2]]
% colnames(data) = c("gender", "age", "handedness", "height", "weight",
%                   "fing_temp", "fing_height", "fing_width",
%                   "fing_area", "fing_circ")
% head(data, n=3)
% @
% \end{frame}
% 
% 
% 
% \begin{frame}\frametitle{Some wrangling -- Centering}
% Plotting all these variables is complicated, so we forgo this for the time being
% \vfill
% Let us centre the data. That there are some \code{NA} values, so we remove them using the function \code{complete.cases}, which identifies rows where at least one of the variables is \code{NA}
% \vfill
% (We could also use \code{na.rm = TRUE} when taking the average to remove these values.) 
% \vfill
% We make new columns with the prefix \code{.c}, just to still have the initial data handy if need be.
% \end{frame}
% 
% \begin{frame}[fragile]\frametitle{Some wrangling -- Centering}
% <<center-the-data>>=
% data = data[complete.cases(data),]
% to_centre = c("age", "height", 
%               "weight", "fing_temp", 
%               "fing_height", "fing_width",
%               "fing_area", "fing_circ")
% for (c in to_centre) {
%     new_c = sprintf("%s.c", c)
%     data[[new_c]] = data[[c]] - mean(data[[c]], na.rm = TRUE)
% }
% head(data)
% @
% \end{frame}
% 
% 
% 
% \begin{frame}[fragile]\frametitle{Covariance matrix}
% <<compute-sample-covariance-matrix>>=
% X = as.matrix(data[, to_centre])
% S = 1/(dim(X)[1]-1)*t(X) %*% X
% S
% @
% \end{frame}
% 
% 
% 
% 
% \begin{frame}[fragile]\frametitle{Eigenvalues}
% <<>>=
% ev = eigen(S)
% ev$values
% @
% \vfill
% Let us add the singular values to \code{ev}
% \vfill
% <<>>=
% ev$sing_values = sqrt(ev$values)
% @
% \end{frame}
% 
% 
% 
% \begin{frame}[fragile]\frametitle{Use built-in functions}
% <<>>=
% GS = pracma::gramSchmidt(A = ev$vectors)
% GS$Q
% # Just to check that Q is indeed with normalised columns
% colSums(GS$Q[,1:dim(GS$Q)[2]]^2)
% GS$Q[,1] %*% GS$Q[,2]
% @
% So \code{Q} is indeed an orthogonal matrix
% \end{frame}
% 
% 
% 
% \begin{frame}[fragile]\frametitle{Some wrangling}
% Now recall we saw a theorem that told us how to construct a new basis..
% 
% 
% <<>>=
% # Make an identity matrix
% Id = diag(dim(GS$Q)[1])
% # Make the augmented matrix
% A = cbind(GS$Q, Id)
% # Compute the RREF and extract the relevant matrix
% P = pracma::rref(A)[,(dim(GS$Q)[2]+1):dim(A)[2]]
% X.new = X %*% t(P)
% @
% \end{frame}
% 
% 
% \begin{frame}[fragile]\frametitle{Use built-in functions}
% Use the built in function \code{prcomp} or \code{PCA} from the \code{FactoMineR} package
% \vfill
% <<>>=
% # data.pca = prcomp(X, center = TRUE, scale = TRUE)
% data.pca = PCA(X, scale.unit = TRUE, graph = FALSE)
% summary(data.pca)
% @
% \end{frame}
% 
% 
% \begin{frame}[fragile]\frametitle{Percentage of variance}
% The ``proportion of variance'' (or ``percentage of variance'') information is actually the proportion (and then cumulative proportion) represented by the singular value associated to each principal component
% \vfill
% We check this (approximately) by comparing with the singular values we computed
% \vfill
% <<>>=
% ev$sing_values/(sum(ev$sing_values))
% cumsum(ev$sing_values)/(sum(ev$sing_values))
% @
% \end{frame}
% 
% 
% 
% % \begin{frame}[fragile]\frametitle{Some wrangling}
% % <<>>=
% % str(data.pca)
% % @
% % 
% % 
% % <<>>=
% % #library(devtools)
% % #install_github("vqv/ggbiplot")
% % library(ggbiplot)
% % @
% % 
% % \end{frame}
% % 
% % 
% % \begin{frame}[fragile]\frametitle{Some wrangling}
% % <<>>=
% % ggbiplot(data.pca, groups = data$handedness)
% % @
% % \end{frame}
% 
% \begin{frame}[fragile]\frametitle{Plot results}
% <<plot-PCA-1>>=
% plot.PCA(data.pca, axes = c(1,2), choix = "ind", habillage = 4)
% @
% \end{frame}
% 
% \maxFrameImage{\Sexpr{knitr::fig_chunk("plot-PCA-1", "pdf")}}

% \begin{frame}[fragile]\frametitle{Plot results with ellipses}
% <<plot-PCA-2>>=
% plotellipses(data.pca)
% @
% \end{frame}
% 
% \maxFrameImage{\Sexpr{knitr::fig_chunk("plot-PCA-2", "pdf")}}


% \begin{frame}[fragile]\frametitle{Some wrangling}
% Not that it makes much difference, but here we realise that handedness is badly encoded, in the sense that there are some individuals with lowercase handedness and others where the word starts with a capital letter. Let us fix this and plot again
% 
% <<>>=
% data$handedness = tolower(data$handedness)
% ggbiplot(data.pca, groups = data$handedness)
% @
% \end{frame}
% 
% 
% \begin{frame}[fragile]\frametitle{Some wrangling}
% Something else you can plot: ellipses containing most elements in a group, for the groups we have selected (here, handedness).
% 
% 
% <<>>=
% ggbiplot(data.pca, groups = data$handedness, ellipse = TRUE)
% @
% \end{frame}






% Save counters for next file

% Save theorem count for next file
\end{document}
