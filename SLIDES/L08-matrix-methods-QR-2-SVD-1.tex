\documentclass[aspectratio=169]{beamer}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

% Set lecture number for later use


% Part common to all the lectures
\subtitle{MATH 2740 -- Mathematics of Data Science -- Lecture 08}
\author{\texorpdfstring{Julien Arino\newline\url{julien.arino@umanitoba.ca}}{Julien Arino}}
\institute{Department of Mathematics @ University of Manitoba}
\date{Fall 202X}

% Title of the lecture
\title{Matrix methods -- QR factorisation (2) \& SVD (1)}



\input{slides-setup-whiteBG.tex}

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

% Set up cross-references and counter persistence

% Set up cross-references and counter persistence

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE AND OUTLINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titlepagewithfigure{FIGS-slides-admin/Gemini_Generated_Image_7iz9ft7iz9ft7iz9.jpeg}
\outlinepage{FIGS-slides-admin/Gemini_Generated_Image_iyzqdwiyzqdwiyzq.jpeg}


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{The QR factorisation \& Least squares}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}

\begin{frame}{The QR factorisation}
\begin{theorem}\label{th:QR_factorisation}
Let $A\in\M_{mn}$ with LI columns. Then $A$ can be factored as
\[
A=QR
\]
where $Q\in\M_{mn}$ has orthonormal columns and $R\in\M_n$ is nonsingular upper triangular
\end{theorem}
\end{frame}


\begin{frame}{Back to least squares}
So what was the point of all that..?
\vfill
\begin{theorem}[Least squares with QR factorisation]
\label{th:LSQ_with_QR}
$A\in\M_{mn}$ with LI columns, $\bb\in\IR^m$. If $A=QR$ is a QR factorisation of $A$, then the unique least squares solution $\tilde\bx$ of $A\bx=\bb$ is
\[
\tilde\bx = R^{-1}Q^T\bb
\]
\end{theorem}
\end{frame}


\begin{frame}{Proof of Theorem~\ref{th:LSQ_with_QR}}
$A$ has LI columns so 
\begin{itemize}
\item least squares $A\bx=\bb$ has unique solution $\tilde\bx=(A^TA)^{-1}A^T\bb$
\item by Theorem~\ref{th:QR_factorisation}, $A$ can be written as $A=QR$ with $Q\in\M_{mn}$ with orthonormal columns and $R\in\M_n$ nonsingular and upper triangular
\end{itemize}
So
\begin{align*}
A^TA\tilde\bx= A^T\bb &\implies (QR)^TQR\tilde\bx = (QR)^T\bb \\
&\implies R^TQ^TQR\tilde\bx = R^TQ^T\bb \\
&\implies R^T\II_nR\tilde\bx = R^TQ^T\bb \\
&\implies R^TR\tilde\bx = R^TQ^T\bb \\
&\implies (R^T)^{-1}R\tilde\bx = (R^T)^{-1}R^TQ^T\bb \\
&\implies R\tilde\bx = Q^T\bb \\
&\implies \tilde\bx = R^{-1}Q^T\bb\hfill\qed
\end{align*}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{Singular values}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}

\begin{frame}{Matrix factorisations (continued)}
The singular value decomposition (known mostly by its acronym, SVD) is yet another type of factorisation/decomposition..
\end{frame}

\begin{frame}{Singular values}
\begin{definition}[Singular value]
Let $A\in\M_{mn}(\IR)$. The \textbf{singular values} of $A$ are the real numbers 
\[
\sigma_1\geq \sigma_2\geq\cdots\sigma_n\geq 0
\]
that are the square roots of the eigenvalues of $A^TA$
\end{definition}
\end{frame}


\begin{frame}{Singular values are real and nonnegative?}
Recall that $\forall A\in\M_{mn}$, $A^TA$ is symmetric
\vfill
\textbf{Claim 1.} Real symmetric matrices have real eigenvalues
\vfill
\textbf{Proof.} $A\in\M_n(\IR)$ symmetric and $(\lambda,\bv)$ eigenpair of $A$, i.e, $A\bv=\lambda\bv$. Taking the complex conjugate, $\overline{A\bv}=\overline{\lambda\bv}$
\vfill
Since $A\in\M_n(\IR)$, $\overline{A}=A$\qquad ($z=\bar z\iff z\in\IR$)
\vfill
So
\[
A\bar\bv=\overline{A}\bar\bv=\overline{A\bv}=\overline{\lambda\bv}=\overline{\lambda}\bar\bv
\]
i.e., if $(\lambda,\bv)$ eigenpair, $(\bar\lambda,\bar\bv)$ also eigenpair
\end{frame}

\begin{frame}
Still assuming $A\in\M_n(\IR)$ symmetric and $(\lambda,\bv)$ eigenpair of $A$ and using what we just proved (that $(\bar\lambda,\bar\bv)$ also eigenpair), take transposes
\begin{align*}
A\bar\bv = \bar\lambda\bar\bv &\iff (A\bar\bv)^T = (\bar\lambda\bar\bv)^T \\
&\iff \bar\bv^TA^T=\bar\lambda\bar\bv^T \\
&\iff \bar\bv^T A = \bar\lambda\bar\bv^T \qquad{\textrm{[$A$ symmetric]}}
\end{align*}
\vfill
Let us now compute $\lambda (\bar\bv\bullet\bv)$. We have
\begin{align*}
\lambda (\bar\bv\bullet\bv) &= \lambda\bar\bv^T\bv = \bar\bv^T(\lambda\bv) \\
&= \bar\bv^T(A\bv) = (\bar\bv^TA)\bv \\
&= (\bar\lambda\bar\bv^T)\bv = \bar\lambda(\bar\bv\bullet\bv) \\
&\iff (\lambda-\bar\lambda)(\bar\bv\bullet\bv) = 0
\end{align*}
\end{frame}

\begin{frame}
We have shown
\[
(\lambda-\bar\lambda)(\bar\bv\bullet\bv) = 0
\]
Let 
\[
\bv = \begin{pmatrix}
a_1+ib_1 \\
\vdots \\
a_n+ib_n
\end{pmatrix}
\]
Then
\[
\bar\bv = \begin{pmatrix}
a_1-ib_1 \\
\vdots \\
a_n-ib_n
\end{pmatrix}
\]
So
\[
\bar\bv\bullet\bv = (a_1^2+b_1^2)+\cdots+(a_n^2+b_n^2)
\]
But $\bv$ eigenvector is $\neq\b0$, so $\bar\bv\bullet\bv\neq 0$, so
\[
(\lambda-\bar\lambda)(\bar\bv\bullet\bv) = 0
\iff \lambda-\bar\lambda=0
\iff \lambda=\bar\lambda\iff \lambda\in\IR\qed
\]
\end{frame}


\begin{frame}
\textbf{Claim 2.} For $A\in\M_{mn}(\IR)$, the eigenvalues of $A^TA$ are real and nonnegative

\vfill
\textbf{Proof.}
We know that for $A\in\M_{mn}$, $A^TA$ symmetric and from previous claim, if $A\in\M_{mn}(\IR)$, then $A^TA$ is symmetric and real and with real eigenvalues
\vfill
Let $(\lambda,\bv)$ be an eigenpair of $A^TA$, with $\bv$ chosen so that $\|\bv\|=1$
\vfill 
Norms are functions $V\to\IR_+$, so $\|A\bv\|$ and $\|A\bv\|^2$ are $\geq 0$ and thus
\begin{align*}
0\leq \|A\bv\|^2 &= (A\bv)\bullet(A\bv) = (A\bv)^T(A\bv) \\
&= \bv^TA^TA\bv = \bv^T(A^TA\bv) = \bv^T(\lambda\bv) \\
&= \lambda(\bv^T\bv) = \lambda(\bv\bullet\bv) = \lambda\|\bv\|^2 \\
&= \lambda\hfill\qed
\end{align*}
\end{frame}

\begin{frame}
\textbf{Claim 3.} For $A\in\M_{mn}(\IR)$, the nonzero eigenvalues of $A^TA$ and $AA^T$ are the same
\vfill
\textbf{Proof.}
Let $(\lambda,\bv)$ be an eigenpair of $A^TA$ with $\lambda\neq 0$. Then $\bv\neq\b0$ and
\[
	A^TA\bv=\lambda\bv\neq\b0
\]
Left multiply by $A$
\[
	AA^TA\bv = \lambda A\bv
\]
Let $\bw=A\bv$, we thus have $AA^T\bw=\lambda\bw$; in other words, $A\bv$ is an eigenvector of $AA^T$ corresponding to the (nonzero) eigenvalue $\lambda$
\vfill
The reverse works the same way.. \qed
\end{frame}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{The SVD}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}

\begin{frame}{The singular value decomposition (SVD)}
\begin{importanttheorem}[SVD]\label{th:SVD}
$A\in\M_{mn}$ with singular values $\sigma_1\geq\cdots\geq\sigma_r>0$ and $\sigma_{r+1}=\cdots=\sigma_n=0$
\vskip0.5cm
Then there exists $U\in\M_m$ orthogonal, $V\in\M_n$ orthogonal and a block matrix $\Sigma\in\M_{mn}$ taking the form
\[
\Sigma=
\begin{pmatrix}
D & 0_{r,n-r} \\
0_{m-r,r} & 0_{m-r,n-r}
\end{pmatrix}
\]
where 
\[
D = \mathsf{diag}(\sigma_1,\ldots,\sigma_r)\in\M_r
\] 
such that
\[
A=U\Sigma V^T
\]
\end{importanttheorem}
\end{frame}


\begin{frame}
\begin{definition}
We call a factorisation as in Theorem~\ref{th:SVD} the \textbf{singular value decomposition} of $A$. The columns of $U$ and $V$ are, respectively, the \textbf{left} and \textbf{right singular vectors} of $A$
\end{definition}
\vfill
$U$ and $V^T$ are \emph{rotation} or \emph{reflection} matrices, $\Sigma$ is a \emph{scaling} matrix
\vfill
$U\in\M_m$ orthogonal matrix with columns the eigenvectors of $AA^T$
\vfill
$V\in\M_n$ orthogonal matrix with columns the eigenvectors of $A^TA$
\end{frame}


\begin{frame}{Outer product form of the SVD}
\begin{theorem}[Outer product form of the SVD]\label{th:SVD_outer_product_form}
$A\in\M_{mn}$ with singular values $\sigma_1\geq\cdots\geq\sigma_r>0$ and $\sigma_{r+1}=\cdots=\sigma_n=0$, $\bu_1,\ldots,\bu_r$ and $\bv_1,\ldots,\bv_r$, respectively, left and right singular vectors of $A$ corresponding to these singular values
\vskip0.5cm
Then 
\begin{equation}\label{eq:outer-product-form-SVD}
A=\sigma_1\bu_1\bv_1^T+\cdots+\sigma_r\bu_r\bv_r^T
\end{equation}
\end{theorem}
\end{frame}

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\Ssection{An application of the SVD -- Image compression}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}


\begin{frame}{Applications of the SVD}
Many applications of the SVD, both theoretical and practical..
\vfill
\begin{enumerate}
\item Obtaining a unique solutions to least squares when $A^TA$ singular
\item Image compression
\end{enumerate}
\end{frame}



\begin{frame}{Compressing images}
Consider an image (for simplicity, assume in shades of grey). This can be stored in a matrix $A\in\M_{mn}$
\vfill
Take the SVD of $A$. Then the small singular values carry information about the regions with little variation and can perhaps be omitted, whereas the large singular values carry information about more ``dynamic'' regions of the image
\vfill
Suppose $A$  has $r$ nonzero singular values. For $k\leq r$, let
\[
A_k = \sigma_1\bu_1\bv_1^T+\cdots+\sigma_k\bu_k\bv_k^T
\]
\vfill
For $k=r$ we get the usual outer product form \eqref{eq:outer-product-form-SVD}
\end{frame}

\begin{frame}[fragile]
Load the image using \code{bmp::read.bmp}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{my_image} \hlkwb{=} \hldef{bmp}\hlopt{::}\hlkwd{read.bmp}\hldef{(}\hlsng{"../CODE/Julien_and_friend_1000x800.bmp"}\hldef{)}
\hldef{my_image_g} \hlkwb{=} \hldef{pixmap}\hlopt{::}\hlkwd{pixmapGrey}\hldef{(my_image)}
\hldef{my_image_g}
\end{alltt}
\begin{verbatim}
## Pixmap image
##   Type          : pixmapGrey 
##   Size          : 800x1000 
##   Resolution    : 1x1 
##   Bounding box  : 0 0 1000 800
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}

\maxFrameImage{FIGS/L08-plot-image-svd-original-1.png}

\begin{frame}[fragile]\frametitle{Doing the computations ``by hand''}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{M} \hlkwb{=} \hldef{my_image_g}\hlopt{@}\hlkwc{grey}
\hldef{MTM} \hlkwb{=} \hlkwd{t}\hldef{(M)} \hlopt{%*%} \hldef{M}
\hlcom{# Ensure matrix is symmetric}
\hldef{MTM} \hlkwb{=} \hldef{(MTM}\hlopt{+}\hlkwd{t}\hldef{(MTM))}\hlopt{/}\hlnum{2}
\hldef{ev} \hlkwb{=} \hlkwd{eigen}\hldef{(MTM)}
\end{alltt}
\end{kframe}
\end{knitrout}
\vfill
Given the size and nature of the entries, the matrix $M^TM$ is symmetric only to \code{1e-5} precision, so we use a little trick to make it symmetric no matter what: take the average of $M^TM$ and its transpose $MM^T$
\end{frame}


\begin{frame}[fragile]\frametitle{Which version of the algorithm to use?}
Make zero the eigenvalues that are close to zero (200 out of 1000)
\vfill
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{ev}\hlopt{$}\hldef{values} \hlkwb{=} \hldef{ev}\hlopt{$}\hldef{values}\hlopt{*}\hldef{(ev}\hlopt{$}\hldef{values}\hlopt{>}\hlnum{1e-10}\hldef{)}
\end{alltt}
\end{kframe}
\end{knitrout}
\vfill
Can we use the algorithm for all eigenvalues being distinct or do we have repeated ones?
\vfill
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{any}\hldef{(}\hlkwd{duplicated}\hldef{(ev}\hlopt{$}\hldef{values[ev}\hlopt{$}\hldef{values}\hlopt{>}\hlnum{1e-10}\hldef{]))}
\end{alltt}
\begin{verbatim}
## [1] FALSE
\end{verbatim}
\end{kframe}
\end{knitrout}
\vfill
So we can use the standard algorithm
\end{frame}


\begin{frame}[fragile]\frametitle{Computing the SVD}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{idx_positive_ev} \hlkwb{=} \hlkwd{which}\hldef{(ev}\hlopt{$}\hldef{values}\hlopt{>}\hlnum{1e-10}\hldef{)}
\hldef{sv} \hlkwb{=} \hlkwd{sqrt}\hldef{(ev}\hlopt{$}\hldef{values[idx_positive_ev])}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{frame}


\begin{frame}[fragile]\frametitle{Computing the SVD}
Then $D=\mathsf{diag}(\sigma_1,\ldots,\sigma_r)$, $V$ is the matrix of normalised eigenvectors in the same order as the $\sigma_i$ and for $i=1,\ldots,r$
$$
\mathbf{u}_i = \frac{1}{\sigma_i}A\mathbf{v}_i
$$
ensuring that $\|\mathbf{u}_i\|=1$
\vfill
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{D} \hlkwb{=} \hlkwd{diag}\hldef{(sv)}
\hldef{V} \hlkwb{=} \hldef{ev}\hlopt{$}\hldef{vectors[idx_positive_ev, idx_positive_ev]}
\hldef{c1} \hlkwb{=} \hlkwd{colSums}\hldef{(V)}
\hlkwa{for} \hldef{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{dim}\hldef{(V)[}\hlnum{2}\hldef{]) \{}
    \hldef{V[,i]} \hlkwb{=} \hldef{V[,i]}\hlopt{/}\hldef{c1[i]}
\hldef{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{frame}


\begin{frame}[fragile]\frametitle{Computing the SVD}
Finally, we compute the $\bu_i$'s
\vfill
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{U} \hlkwb{=} \hldef{M} \hlopt{%*%} \hldef{V} \hlopt{%*%} \hlkwd{diag}\hldef{(}\hlnum{1}\hlopt{/}\hldef{sv)}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in M \%*\% V: non-conformable arguments}}\begin{alltt}
\hldef{r} \hlkwb{=} \hlkwd{length}\hldef{(sv)}
\hldef{im} \hlkwb{=} \hlkwd{list}\hldef{(}\hlkwc{u}\hldef{=U,} \hlkwc{d}\hldef{=sv,} \hlkwc{v}\hldef{=V)}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error: object 'U' not found}}\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile]\frametitle{Using built-in functions}
We can also use the built-in function \code{svd} to compute the SVD of $M$
\vfill
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{M.svd} \hlkwb{=} \hlkwd{svd}\hldef{(M)}
\end{alltt}
\end{kframe}
\end{knitrout}
\vfill
The results are stored in a list with components \code{u}, \code{d} and \code{v}
\end{frame}

\begin{frame}\frametitle{Make function to recreate an image from the SVD}
Given the SVD \code{im} of an image and a number of singular values to keep \code{n}, we can recreate the image using the function \code{compress\_image}
\vfill
We output the new image, but also, the amount of information required to encode this new image, as a percentage of the original image size
\end{frame}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{compress_image} \hlkwb{=} \hlkwa{function}\hldef{(}\hlkwc{im}\hldef{,} \hlkwc{n}\hldef{) \{}
  \hlkwa{if} \hldef{(n} \hlopt{>} \hlkwd{length}\hldef{(im}\hlopt{$}\hldef{d)) \{}
    \hlcom{# Check that we gave a value of n within range, otherwise }
    \hlcom{# just set to the max}
    \hldef{n} \hlkwb{=} \hlkwd{length}\hldef{(im}\hlopt{$}\hldef{d)}
  \hldef{\}}
  \hldef{d_tmp} \hlkwb{=} \hldef{im}\hlopt{$}\hldef{d[}\hlnum{1}\hlopt{:}\hldef{n]}
  \hldef{u_tmp} \hlkwb{=} \hldef{im}\hlopt{$}\hldef{u[,}\hlnum{1}\hlopt{:}\hldef{n]}
  \hldef{v_tmp} \hlkwb{=} \hldef{im}\hlopt{$}\hldef{v[,}\hlnum{1}\hlopt{:}\hldef{n]}
  \hlcom{# We store the results in a list (so we can return other information)}
    \hldef{out} \hlkwb{=} \hlkwd{list}\hldef{()}
    \hlcom{# First, compute the resulting image}
    \hldef{out}\hlopt{$}\hldef{img} \hlkwb{=} \hlkwd{mat.or.vec}\hldef{(}\hlkwc{nr} \hldef{=} \hlkwd{dim}\hldef{(im}\hlopt{$}\hldef{u)[}\hlnum{1}\hldef{],} \hlkwc{nc} \hldef{=} \hlkwd{dim}\hldef{(im}\hlopt{$}\hldef{v)[}\hlnum{1}\hldef{])}
    \hlkwa{for} \hldef{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hldef{n) \{}
        \hldef{out}\hlopt{$}\hldef{img} \hlkwb{=} \hldef{out}\hlopt{$}\hldef{img} \hlopt{+} \hldef{d_tmp[i]} \hlopt{*} \hldef{u_tmp[,i]} \hlopt{%*%} \hlkwd{t}\hldef{(v_tmp[,i])}
    \hldef{\}}


    \hlcom{# Values of the "colours" must be between 0 and 1, so we shift and rescale}
    \hlkwa{if} \hldef{(}\hlkwd{min}\hldef{(}\hlkwd{min}\hldef{(out}\hlopt{$}\hldef{img))} \hlopt{<} \hlnum{0} \hldef{) \{}
        \hldef{out}\hlopt{$}\hldef{img} \hlkwb{=} \hldef{out}\hlopt{$}\hldef{img} \hlopt{-} \hlkwd{min}\hldef{(}\hlkwd{min}\hldef{(out}\hlopt{$}\hldef{img))}
    \hldef{\}}
    \hldef{out}\hlopt{$}\hldef{img} \hlkwb{=} \hldef{out}\hlopt{$}\hldef{img} \hlopt{/} \hlkwd{max}\hldef{(}\hlkwd{max}\hldef{(out}\hlopt{$}\hldef{img))}
    \hlcom{# Store some information: number of points needed and percentage of the original required}
    \hldef{out}\hlopt{$}\hldef{nb_pixels_original} \hlkwb{=} \hlkwd{dim}\hldef{(im}\hlopt{$}\hldef{u)[}\hlnum{1}\hldef{]} \hlopt{*} \hlkwd{dim}\hldef{(im}\hlopt{$}\hldef{v)[}\hlnum{2}\hldef{]}
    \hldef{out}\hlopt{$}\hldef{nb_pixels_compressed} \hlkwb{=} \hlkwd{length}\hldef{(d_tmp)} \hlopt{+} \hlkwd{dim}\hldef{(u_tmp)[}\hlnum{1}\hldef{]}\hlopt{*}\hlkwd{dim}\hldef{(u_tmp)[}\hlnum{2}\hldef{]} \hlopt{+} \hlkwd{dim}\hldef{(v_tmp)[}\hlnum{1}\hldef{]}\hlopt{*}\hlkwd{dim}\hldef{(v_tmp)[}\hlnum{2}\hldef{]}
    \hldef{out}\hlopt{$}\hldef{pct_of_original} \hlkwb{=} \hldef{out}\hlopt{$}\hldef{nb_pixels_compressed} \hlopt{/} \hldef{out}\hlopt{$}\hldef{nb_pixels_original} \hlopt{*} \hlnum{100}
    \hlcom{# Return the result}
    \hlkwd{return}\hldef{(out)}
\hldef{\}}
\end{alltt}
\end{kframe}
\end{knitrout}


\begin{frame}[fragile]\frametitle{Recreating the image}
We can now recreate the image using the function \code{compress\_image}
\vfill
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{new_image} \hlkwb{=} \hldef{my_image_g}
\hldef{M.svd} \hlkwb{=} \hlkwd{svd}\hldef{(M)}
\hldef{M_tmp} \hlkwb{=} \hlkwd{compress_image}\hldef{(M.svd,} \hlnum{2}\hldef{)}
\hldef{new_image}\hlopt{@}\hlkwc{grey} \hlkwb{=} \hldef{M_tmp}\hlopt{$}\hldef{img}
\hlkwd{plot}\hldef{(new_image)}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}\frametitle{Using $n=2$ singular values}
\begin{center}
\includegraphics[width=0.45\textwidth]{FIGS/L08-plot-image-svd-original-1.png}
\includegraphics[width=0.45\textwidth]{FIGS/L08-plot-image-svd-n2-1.png}
\end{center}
\vfill
Uses 0.56\% of the original information
\end{frame}



\begin{frame}\frametitle{Using $n=5$ singular values}
\begin{center}
\includegraphics[width=0.45\textwidth]{FIGS/L08-plot-image-svd-original-1.png}
\includegraphics[width=0.45\textwidth]{FIGS/L08-plot-image-svd-n5-1.png}
\end{center}
\vfill
Uses 1.41\% of the original information
\end{frame}




\begin{frame}\frametitle{Using $n=10$ singular values}
\begin{center}
\includegraphics[width=0.45\textwidth]{FIGS/L08-plot-image-svd-original-1.png}
\includegraphics[width=0.45\textwidth]{FIGS/L08-plot-image-svd-n10-1.png}
\end{center}
\vfill
Uses 2.81\% of the original information
\end{frame}





\begin{frame}\frametitle{Using $n=20$ singular values}
\begin{center}
\includegraphics[width=0.45\textwidth]{FIGS/L08-plot-image-svd-original-1.png}
\includegraphics[width=0.45\textwidth]{FIGS/L08-plot-image-svd-n20-1.png}
\end{center}
\vfill
Uses 5.63\% of the original information
\end{frame}





\begin{frame}\frametitle{Using $n=50$ singular values}
\begin{center}
\includegraphics[width=0.45\textwidth]{FIGS/L08-plot-image-svd-original-1.png}
\includegraphics[width=0.45\textwidth]{FIGS/L08-plot-image-svd-n50-1.png}
\end{center}
\vfill
Uses 14.07\% of the original information
\end{frame}




% Save counters for next file

% Save theorem count for next file
\end{document}
