\documentclass[aspectratio=169]{beamer}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

% Set lecture number for later use


% Part common to all the lectures
\subtitle{MATH 2740 -- Mathematics of Data Science -- Lecture 26}
\author{\texorpdfstring{Julien Arino\newline\url{julien.arino@umanitoba.ca}}{Julien Arino}}
\institute{Department of Mathematics @ University of Manitoba}
\date{Fall 202X}

% Title of the lecture
\title{PageRank}



\input{slides-setup-whiteBG.tex}



\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE AND OUTLINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titlepagewithfigure{FIGS-slides-admin/Gemini_Generated_Image_ssozssssozssssoz.jpeg}
\outlinepage{FIGS-slides-admin/Gemini_Generated_Image_2dtfd12dtfd12dtf.jpeg}


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What makes an important webpage?}
	In days of yore, the web was a small thing
	\vfill
	Alta Vista was the search engine of choice
	\vfill
	Google started in 1998, based on an algorithm (PageRank) described in a paper of Page, Brin, Motwani and Winograd (\href{https://storm.cis.fordham.edu/~gweiss/selected-papers/classic-pagerank-paper.pdf}{link})
\end{frame}

\begin{frame}{Overview}
	Give each page a rating (of its importance), a recursively defined measure whereby a page becomes important if important pages link to it
	\vfill
	Recursive definition: the importance of a page refers back to the importance of other pages that link to it
	\vfill
	\textbf{Random surfer} model: a random surfer on the web follows links from page to page. Page rank $\simeq$ $\mathbb{P}$ random surfer lands on a particular page. Popular page $\implies$ higher probability to go there.  
	($\IP$ stands for ``probability'')
	\vfill Example of a Markov chain
\end{frame}

%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Eigenvector centrality}
\newSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_cdlhu6cdlhu6cdlh.jpeg}

\begin{frame}{Constructing a stochastic matrix from an adjacency matrix}
	Let $A$ be the adjacency matrix of a simple graph $G=(V,E)$ and $D$ its degree matrix, i.e., the diagonal matrix $D=(d_{ij})$ with diagonal entries
	\[
		d_{ii} = \sum_{j=1}^n a_{ji} =\sum_{j=1}^n a_{ij}
	\]
	(Recall that $A$ symmetric since $G$ nondirected.)
	Then the matrix $AD^{-1}$ is column stochastic
\end{frame}

\begin{frame}
	Indeed, write entries in $D^{-1}$ as $d_{ij}^{-1}$. Of course, $d_{ii}^{-1} = 1/d_{ii}$, $1\leq i\leq n$ and $d_{ij}^{-1} = 0$ if $i\neq j$
	\vfill
	Then
	\begin{align*}
		AD^{-1} &= \sum_{k=1}^n a_{ik}d_{kj}^{-1}
		= a_{ij}d_{jj}^{-1}
		,\quad i,j=1,\ldots,n
	\end{align*}
	So the sum of column $j$ of $AD^{-1}$ is 
	\begin{align*}
		\sum_{i=1}^n a_{ij}d_{jj}^{-1} 
		&= d_{jj}^{-1} \sum_{i=1}^n a_{ij} \\
		&= d_{jj}^{-1} \sum_{i=1}^n a_{ji} \\
		&= d_{jj}^{-1} d_{jj} \\
		&= 1
	\end{align*}
	and the matrix is column-stochastic
\end{frame}
	
\begin{frame}{Eigenvector centrality (undirected graph)}
Let $\bx$ be an eigenvector corresponding to the largest eigenvalue $\lambda$ of the non-negative adjacency matrix $A$ of the undirected graph $G = (V, E)$
\vfill
(We often call $\lambda$ the \textbf{Perron root} of $A$ and $\bx$ a \textbf{Perron eigenvector})
\vfill
The \textbf{eigenvector centrality} (or \textbf{prestige score}) of vertex $i$ is the $i$th component of the eigenvector $\bx$ of the (column) stochastic matrix $N := AD^{-1}$ corresponding to the eigenvalue 1:
\[
	N\bx = \bx
\]
\end{frame}

\begin{frame}
Consider a particular vertex $i$ with its neighbouring vertices $\N(i)$:
\[
	x_i = \sum_{j\in\N(i)} x_j 
	= \sum_j A_{ij} x_j	
\]
\vfill
The eigenvector centrality defined this way depends both on the number of neighbours $|\N(i)|$ and the quality of its connections $x_j$, $j\in\N(i)$
\end{frame}

\begin{frame}
	Let $A = (a_{ij})$ be the adjacency matrix of a graph. The eigenvector centrality $x_{i}$ of vertex $i$ is given by
	$$
	x_i = \frac{1}{\lambda} \sum_k a_{ki} \, x_k
	$$ 
	where $\lambda \neq 0$ is a constant. In matrix form
	$$
	\bx^T A=\lambda\bx^T
	$$
\vfill
Hence the centrality vector $\bx$ is the left eigenvector of the adjacency matrix $A$ associated with the eigenvalue $\lambda$
\end{frame}


\begin{frame}{Power method to solve eigenvector centrality}
$m(v)$: signed component of maximal magnitude of vector $v$; if more than one maximal component, let $m(v)$ be the first one. E.g., $m(-3,3,2) = -3$
\vfill
Let $x^{(0)}$ be an arbitrary vector. For $k \geq 1$
\begin{itemize}
	\item repeatedly compute $x^{(k)} = x^{(k-1)} A$
	\item normalize $x^{(k)} = x^{(k)} / m(x^{(k)})$
\end{itemize}
until desired precision is achieved
\vfill
Then $x^{(k)}$ converges to the dominant eigenvector of $A$ and $m(x^{(k)})$ converges to the dominant eigenvalue of $A$
\vfill
If matrix $A$ is sparse, each vector-matrix product can be performed in linear time in the size of the graph.
\end{frame}


\begin{frame}
Power method converges when the dominant (largest) and the sub-dominant (second largest) eigenvalues of $A$ $\lambda_1$ and $\lambda_2$ are separated, i.e., are different in absolute value, i.e., when $|\lambda_1| > |\lambda_2|$
\vfill
Rate of convergence is rate at which $(\lambda_2 / \lambda_1)^k$ goes to $0$. Hence, if the sub-dominant eigenvalue is small compared to the dominant one, the method converges quickly
\end{frame}


\begin{frame}{Why use the leading eigenvector?}
	We want a nonnegative measure, so we want a vector in $\IR_+$
	\vfill
	We know from the Perron-Frobenius Theorem that the eigenvector corresponding to the dominant eigenvalue of a nonnegative matrix is nonnegative
	\vfill
	Furthermore, if the graph is strongly connected, the matrix is irreducible and the eigenvector corresponding to the dominant eigenvalue is \emph{positive}
\end{frame}



%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{PageRank}
\newSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_n8c43hn8c43hn8c4.jpeg}

\begin{frame}{PageRank}
Variant of the Eigenvector centrality measure for directed network
\vfill
\textbf{Basic PageRank}
\begin{itemize}
\item Whenever a vertex $i$ has no outgoing link, we add a self-loop to $i$ such that $k^{in}_i=k^{out}_i=1$. Therefore $A_{ii}=1$ for such vertices in the adjacency matrix
\item Let $D^+$ be the diagonal matrix of outdegrees where each element $D_{ii}^+ = k_i^{out}$
\item Define a column stochastic matrix $N = A(D^+)^{-1}$
\item
The PageRank centrality of node i is equal to the 
eigenvector xi of matrix N (The leading eigenvalue is 1):
x = Nx
\end{itemize}
\end{frame}


\begin{frame}
	\begin{block}{Problem}
		Given n interlinked webpages, rank them in order of ``importance''
	\end{block}
	\vfill
Assign the pages importance scores $x_1, x_2,\ldots, x_n\geq 0$
\vfill
Key insight: use the existing link structure of the web to determine importance. A link to a page ``is'' a vote for its importance
\vfill
How does this help with web searches?
\end{frame}


\begin{frame}{}
	First attempt: let $x_k$ equal the number of links to page $k$
\vfill
	Criticism: a link from an ``important'' page (like Google) should carry more weight than a link from some random blog!
\end{frame}

\begin{frame}{}
	Second attempt: let $x_k$ equal the sum of the importance scores of all pages linking to page $k$
	\vfill
	Criticism 1: a webpage gets more ``votes'' (exerts more influence) if it has many outgoing links
	\vfill
	Criticism 2: this system only has the trivial solution!	
\end{frame}


\begin{frame}{}
	Third attempt (Brin and Page, late 90s): let $x_k$ equal the sum of $x_j/n_j$, where
	the sum is taken over all the pages $j$ that link to page $k$, and $n_j$ is the number of outgoing links on page $j$
	\vfill
	A page's number of votes is then its importance score, and gets split evenly among the pages it links to
\end{frame}

\begin{frame}{}
	Summary: given a web with $n$ pages, construct an $n\times n$ matrix $A$ as
	\[
	a_{ij} =
	\begin{cases}
		1/n_j & \text{if page $j$ links to page $i$} \\
		0 & \text{otherwise}
	\end{cases}
	\]
	where $n_j$ is number of outgoing links on page $j$
	\vfill
	Sum of $j$th column is $n_j/n_j = 1$, so $A$ is a stochastic matrix
	\vfill
	The ranking vector $\tilde x$ solves $A\tilde x = \tilde x$
\end{frame}


\begin{frame}{}
	Possible issues: existence of solution with nonnegative entries? Non-unique solutions?	
	\vfill
	PF Theorem guarantees a unique steady-state vector if entries of $A$ are strictly positive or $A$ irreducible. But irreducible $\cancel{\Rightarrow}$ $\lambda_1$ and $\lambda_2$ separated, so make it primitive
	\vfill
	Brin-Page: replace $A$ with
	\[
		B = 0.85A + \frac{0.15}{n}\IJ
	\]
	where $\IJ$ is the matrix of all ones
\vfill
$B>0$ is primitive $\implies$
PF Theorem says B has a unique steady-state vector, ~x.
So ~x can be used for rankings!
\end{frame}

\begin{frame}{The Random Surfer}
	Why Markov chains?
	\vfill
\begin{block}{Brin and Page}
	PageRank can be thought of as a model of user behavior. We assume there is a ``random surfer'' who is given a web page at random and keeps clicking on links, never hitting ``back'' but eventually gets bored and starts on another random page
\end{block}
\vfill
Surfer clicks on a link on the current page with probability 0.85; opens up a random page with probability 0.15
\vfill
A page's rank is the probability the random user will end up on that page, OR, equivalently the fraction of time the random user spends on that page in the long run
\end{frame}


\begin{frame}{In practice}
	Estimates of the number of web pages vary.. $4.77\times 10^9$ to more than $50\times 10^9$
	\vfill 
	Computing stationary distribution is \textbf{hard} computationally
	\vfill
	Instead, use power method, i.e., an iterative method, starting with initial distribution $(1/n,\ldots,1/n)$
\end{frame}

\end{document}
