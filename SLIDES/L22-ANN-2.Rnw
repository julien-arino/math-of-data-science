\documentclass[aspectratio=169]{beamer}

<<set-options,echo=FALSE,warning=FALSE,message=FALSE>>=
# Load required libraries
required_packages = c("dplyr", 
                      "knitr", 
                      "magick",
                      "neuralnet")
for (p in required_packages) {
  if (!require(p, character.only = TRUE)) {
    install.packages(p, dependencies = TRUE)
    require(p, character.only = TRUE)
  }
}
# Prepare figure crop hook
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)
# Knitr options
opts_chunk$set(echo = TRUE, 
               warning = FALSE, 
               message = FALSE,
               crop = TRUE,
               dev = c("pdf", "png"),
               fig.width = 6, 
               fig.height = 4, 
               fig.path = "FIGS/ann-",
               fig.keep = "high",
               fig.show = "hide")
@


<<set-slide-background,echo=FALSE,results='asis'>>=
# Are we plotting for a dark background?
plot_blackBG = FALSE
if (plot_blackBG) {
  bg_color = "black"
  fg_color = "white"
  input_setup = "\\input{slides-setup-blackBG.tex}"
} else {
  bg_color = "white"
  fg_color = "black"
  input_setup = "\\input{slides-setup-whiteBG.tex}"
}
cat(input_setup)
@

\title{Clustering \& Classification using ANNs}
\author{\texorpdfstring{Julien Arino\newline University of Manitoba\newline\url{julien.arino@umanitoba.ca}}{Julien Arino}}
\date{}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE AND OUTLINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titlepagewithfigure{FIGS-slides-admin/Gemini_Generated_Image_pbzl1mpbzl1mpbzl.jpeg}
\outlinepage{FIGS-slides-admin/Gemini_Generated_Image_yegaxcyegaxcyega.jpeg}

%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\section{Neural networks (the perceptron)}
\newSectionSlide{FIGS-slides-admin/Gemini_Generated_Image_5g0m5e5g0m5e5g0m.jpeg}

\begin{frame}[fragile]{Setting Up R}

<<setup-nn-1>>=
# Install nicely
if (!require("neuralnet")) {
  install.packages("neuralnet")
  library(neuralnet)
}
@

\vfill
The \texttt{neuralnet} package provides:
\begin{itemize}
    \item Flexible network specification
    \item Training via backpropagation
    \item Prediction on new data
    \item Network visualization
\end{itemize}
\end{frame}

\begin{frame}[fragile]\frametitle{Using \code{neuralnet} to learn OR}
First, create the truth table
<<OR-truth-table>>=
OR_table = matrix(c(0, 0, 0,
                    1, 0, 1,
                    0, 1, 1,
                    1, 1, 1),
                  nc = 3, 
                  byrow = TRUE)
OR_table = as.data.frame(OR_table)
colnames(OR_table) = c("x1", "x2", "OR")
@
\end{frame}

\begin{frame}[fragile]\frametitle{Now create and train the NN}
The ``formula'' is to find the OR column using the x1 and x2 columns. We use no hidden layer
\vfill
<<plot-NN-for-OR>>=
nn_OR = neuralnet(OR ~ x1 + x2,
                  data = OR_table,
                  act.fct = "logistic",
                  hidden = 0,
                  linear.output = FALSE)
# Plot the result
plot(nn_OR, rep = "best")
@
\end{frame}

\maxFrameImage{\Sexpr{fig_chunk('plot-NN-for-OR', 'pdf')}}

\begin{frame}[fragile]\frametitle{Testing the result}
<<>>=
pred = predict(nn_OR, OR_table)
OR_table$result = pred > 0.5
kable(OR_table, "latex", booktabs = TRUE)
@
\end{frame}


\begin{frame}{Learning XOR}
    Let us now look at the XOR truth table
    \begin{center}
        \begin{tabular}{cccc}
            0 & 0 & $\mapsto$ & 0 \\
            1 & 0 & $\mapsto$ & 1 \\
            0 & 1 & $\mapsto$ & 1 \\
            1 & 1 & $\mapsto$ & 0 \\
        \end{tabular}
    \end{center}
    This problem is not solvable with a simple perceptron of the type we just used, as truth table is not \emph{linearly separable}
    \vfill
    Indeed, we would get weights $w_1>0$, $w_2>0$ to activate when presenting $[1,0]$ and $[0,1]$, but would require that the sum of the weights when applied to the input $[1,1]$, give a negative value.
\end{frame}

\begin{frame}{Linear separability and OR and XOR}
    \begin{center}
    \begin{tikzpicture}[scale=3,auto]
        \draw[step=1cm,gray!25!,very thin] (-0.25,-0.25) grid (1.25,1.25);
        \draw[thick,->] (-0.25,0) -- (1.5,0) node[anchor=north west] {$x_1$};
        \draw[thick,->] (0,-0.25) -- (0,1.5) node[anchor=south east] {$x_2$};
        \node[circle,draw,very thick] (c) at (0,0){};
        \fill (1,0)  circle[radius=2pt];
        \fill (0,1)  circle[radius=2pt];
        \fill (1,1)  circle[radius=2pt];
    \end{tikzpicture}\quad
    \begin{tikzpicture}[scale=3,auto]
        \draw[step=1cm,gray!25!,very thin] (-0.25,-0.25) grid (1.25,1.25);
        \draw[thick,->] (-0.25,0) -- (1.5,0) node[anchor=north west] {$x_1$};
        \draw[thick,->] (0,-0.25) -- (0,1.5) node[anchor=south east] {$x_2$};
        \node[circle,draw,very thick] (c) at (0,0){};
        \fill (1,0)  circle[radius=2pt];
        \fill (0,1)  circle[radius=2pt];
        \node[circle,draw,very thick] (c) at (1,1){};
    \end{tikzpicture}
    \end{center}
    A single-layer perceptron can only learn linearly separable problems
\end{frame}


\begin{frame}{Adding a hidden layer}
    It is possible to do XOR, but we need to add a \textbf{hidden layer}
    \begin{center}
        \def\hhskip{4cm}
        \def\vvskip{4cm}
        \begin{tikzpicture}[scale=1, 
		every node/.style={transform shape},
		auto,
		cloud/.style={minimum width={width("N-1")+2pt},
			draw, ellipse},
		connected/.style={dotted,-}]
		%% Input nodes
		\node [cloud] at (0,0*\vvskip) (i1) {$0\lor 1$};
		\node [cloud] at (0,-1*\vvskip) (i2) {$0\lor 1$};
        %% Hidden node
        \node [cloud] at (1*\hhskip,-0.5*\vvskip) (h1) {$\theta=1$};
		%% Output node
		\node [cloud] at (2*\hhskip,-0.5*\vvskip) (o1) {$\theta=0$};
		%% Arcs
		\path [line] (i1) to node [midway, above, sloped] (TextNode) {$w=1$} (o1);
		\path [line] (i2) to node [midway, below, sloped] (TextNode) {$w=1$} (o1);
		\path [line] (i1) to node [midway, below, sloped] (TextNode) {$w=0.6$} (h1);
		\path [line] (i2) to node [midway, above, sloped] (TextNode) {$w=0.6$} (h1);
		\path [line] (h1) to node [pos=0.25, above] (TextNode) {$w=-2$} (o1);
	\end{tikzpicture}
    \end{center}
\end{frame}




\begin{frame}[fragile]\frametitle{Now the XOR truth table}
<<>>=
XOR_table = matrix(c(0, 0, 0,
                     1, 0, 1,
                     0, 1, 1,
                     1, 1, 0),
                  nc = 3, byrow = TRUE)
XOR_table = as.data.frame(XOR_table)
colnames(XOR_table) = c("x1", "x2", "XOR")
kable(XOR_table, "latex", booktabs = TRUE)
@
\end{frame}

\begin{frame}[fragile]\frametitle{Try to learn it without a hidden layer}
<<>>=
nn_XOR = neuralnet(XOR ~ x1 + x2,
                   data = XOR_table,
                   act.fct = "logistic",
                   hidden = 0,
                   linear.output = FALSE)
pred = predict(nn_XOR, XOR_table)
XOR_table$result = pred > 0.5
kable(XOR_table, "latex", booktabs = TRUE)
@
\end{frame}

\begin{frame}[fragile]\frametitle{Now with a hidden layer}
<<>>=
nn_XOR = neuralnet(XOR ~ x1 + x2,
                   data = XOR_table,
                   act.fct = "tanh",
                   hidden = 1)
pred = predict(nn_XOR, XOR_table)
XOR_table$result = pred > 0.5
kable(XOR_table, "latex", booktabs = TRUE)
@
\vfill
Still hard with one hidden node
\end{frame}

\begin{frame}[fragile]{Using two hidden nodes}
<<ANN-XOR-2-hidden-1>>=
set.seed(42)
nn_XOR <- neuralnet(XOR ~ x1 + x2,
                    data = XOR_table,
                    hidden = 2,                 # need >=2 neurons
                    act.fct = "logistic",       # sigmoid
                    linear.output = FALSE,      # classification
                    err.fct = "ce",             # cross-entropy
                    algorithm = "rprop+",       # rprop+ often works well
                    stepmax = 1e6,
                    threshold = 1e-4,
                    rep = 1)
# Use compute() to get predictions
pr <- compute(nn_XOR, XOR_table[, c("x1", "x2")])$net.result
XOR_table$pred_prob <- pr[,1]
XOR_table$result <- (pr > 0.5)
@
\end{frame}

\begin{frame}[fragile]{Did it work?}
<<ANN-XOR-2-hidden-2>>=
kable(XOR_table, "latex", booktabs = TRUE)
@
\end{frame}

\begin{frame}[fragile]{Using two hidden nodes (another way)}
<<ANN-XOR-2-hidden-3>>=
set.seed(42)
nn_XOR_tanh <- neuralnet(XOR ~ x1 + x2,
                         data = XOR_table,
                         hidden = 2,
                         act.fct = function(x) tanh(x),
                         linear.output = FALSE,
                         err.fct = "sse", # SSE is OK with tanh targets
                         algorithm = "rprop+",
                         stepmax = 1e6,
                         threshold = 1e-4)

pr2 <- compute(nn_XOR_tanh, XOR_table[, c("x1", "x2")])$net.result
XOR_table$pred_tanh <- pr2[,1]
XOR_table$result_tanh <- as.integer(pr2 > 0)  # positive -> class 1
@
\end{frame}

\begin{frame}[fragile]{Did it work?}
<<ANN-XOR-2-hidden-4>>=
kable(XOR_table, "latex", booktabs = TRUE)
@
\end{frame}

\begin{frame}[fragile]\frametitle{An example from the \code{neuralnet} manual -- Training vs testing sets}
\code{iris} is a built-in \code{R} dataset detailing physical characteristics of \Sexpr{nrow(iris)} flowers from \Sexpr{length(unique(iris$Species))} iris species
\vfill
<<>>=
train_idx <- sample(nrow(iris), 2/3 * nrow(iris))
iris_train <- iris[train_idx, ]
iris_test <- iris[-train_idx, ]
@
\vfill
Thus we pick at random 2/3 of the data for training and 1/3 for testing. See some considerations on training, validation and testing on \href{https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets}{this Wikipedia page}
\end{frame}

\begin{frame}[fragile]\frametitle{An example from the \code{neuralnet} manual}
<<>>=
nn <- neuralnet(Species == "setosa" ~ Petal.Length + Petal.Width,
                iris_train, linear.output = FALSE)
pred <- predict(nn, iris_test)
table(iris_test$Species == "setosa", pred[, 1] > 0.5)
@
\end{frame}

\begin{frame}[fragile]\frametitle{Another example -- multiclass classification}
<<>>=
nn <- neuralnet((Species == "setosa") + 
                  (Species == "versicolor") + 
                  (Species == "virginica")
                 ~ Petal.Length + Petal.Width, 
                iris_train, linear.output = FALSE)
pred <- predict(nn, iris_test)
table(iris_test$Species, apply(pred, 1, which.max))
@
\end{frame}

\end{document}