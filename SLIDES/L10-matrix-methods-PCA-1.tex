\documentclass[aspectratio=169]{beamer}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

% Set lecture number for later use


% Part common to all the lectures
\subtitle{MATH 2740 -- Mathematics of Data Science -- Lecture 10}
\author{\texorpdfstring{Julien Arino\newline\url{julien.arino@umanitoba.ca}}{Julien Arino}}
\institute{Department of Mathematics @ University of Manitoba}
\date{Fall 202X}

% Title of the lecture
\title{Matrix methods -- Principal component analysis (1)}



\input{slides-setup-whiteBG.tex}

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

% Set up cross-references and counter persistence
\setcounter{theorem}{79}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE AND OUTLINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titlepagewithfigure{FIGS-slides-admin/Gemini_Generated_Image_7iz9ft7iz9ft7iz9.jpeg}
\outlinepage{FIGS-slides-admin/Gemini_Generated_Image_iyzqdwiyzqdwiyzq.jpeg}



\begin{frame}{Dimensionality reduction}
One of the reasons the SVD is used is for dimensionality reduction. However, SVD has many many other uses
\vfill
Now we look at another dimensionality reduction technique, PCA
\vfill
PCA is often used as a blackbox technique, here we take a look at the math behind it
\end{frame}


\begin{frame}{What is PCA?}
Linear algebraic technique 
\vfill
Helps reduce a complex dataset to a lower dimensional one
\vfill
Non-parametric method: does not assume anything about data distribution (distribution from the statistical point of view)
\end{frame}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{A running example: hockey players}{FIGS-slides-admin/Gemini_Generated_Image_ee8lhqee8lhqee8l.jpeg}

\begin{frame}[fragile]{A 2D example}
Dataset (\href{https://opendata.stackexchange.com/questions/7793/age-weight-and-height-dataset}{link}) of height and weight of some hockey players
\vfill
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# From https://figshare.com/ndownloader/files/5303173}
\hldef{data} \hlkwb{=} \hlkwd{read.csv}\hldef{(}\hlsng{"https://github.com/julien-arino/math-of-data-science/raw/refs/heads/main/DATA/hockey-players.csv"}\hldef{)}
\hlkwd{dim}\hldef{(data)}
\end{alltt}
\begin{verbatim}
## [1] 6292   13
\end{verbatim}
\end{kframe}
\end{knitrout}
\vfill
In case you are wondering, this is a database of ice hockey players at IIHF world championships, 2001-2016, assembled by the dataset's author
\vfill
See some comments \href{https://ikashnitsky.github.io/2017/ice-hockey-players-height/}{here}
\vfill

\end{frame}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{head}\hldef{(data,} \hlkwc{n}\hldef{=}\hlnum{3}\hldef{)}
\end{alltt}
\begin{verbatim}
##   year country no                name position side height weight      birth
## 1 2001     RUS 10    tverdovsky oleg         D    L    185     84 1976-05-18
## 2 2001     RUS  2  vichnevsky vitali         D    L    188     86 1980-03-18
## 3 2001     RUS 26 petrochinin evgeni         D    L    182     95 1976-02-07
##                      club      age cohort      bmi
## 1   anaheim mighty ducks  24.95277   1976 24.54346
## 2   anaheim mighty ducks  21.11978   1980 24.33228
## 3 severstal cherepovetal  25.22930   1976 28.68011
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{frame}
As usual, it is a good idea to plot this to get a sense of the lay of the land
\end{frame}




\maxFrameImage{FIGS/L10-plot-hockey-1-1.pdf}

\begin{frame}[fragile]
The author of the study is interested in the evolution of weights, so it is likely that the same person will be in the dataset several times
\vfill
Let us check this: first check will be \code{FALSE} if the number of unique names does not match the number of rows in the dataset

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{length}\hldef{(}\hlkwd{unique}\hldef{(data}\hlopt{$}\hldef{name))} \hlopt{==} \hlkwd{dim}\hldef{(data)[}\hlnum{1}\hldef{]}
\end{alltt}
\begin{verbatim}
## [1] FALSE
\end{verbatim}
\begin{alltt}
\hlkwd{length}\hldef{(}\hlkwd{unique}\hldef{(data}\hlopt{$}\hldef{name))}
\end{alltt}
\begin{verbatim}
## [1] 3278
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile]
Not interested in the evolution of weights, so simplify: if more than one record for someone, take average of recorded weights and heights
\vfill
To be extra careful, could check as well that there are no major variations on player height (homonymies?)
\vfill
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{data_simplified} \hlkwb{=} \hlkwd{data.frame}\hldef{(}\hlkwc{name} \hldef{=} \hlkwd{unique}\hldef{(data}\hlopt{$}\hldef{name))}
\hldef{w} \hlkwb{=} \hlkwd{c}\hldef{()}
\hldef{h} \hlkwb{=} \hlkwd{c}\hldef{()}
\hlkwa{for} \hldef{(n} \hlkwa{in} \hldef{data_simplified}\hlopt{$}\hldef{name) \{}
    \hldef{tmp} \hlkwb{=} \hldef{data[}\hlkwd{which}\hldef{(data}\hlopt{$}\hldef{name} \hlopt{==} \hldef{n),]}
    \hldef{h} \hlkwb{=} \hlkwd{c}\hldef{(h,} \hlkwd{mean}\hldef{(tmp}\hlopt{$}\hldef{height))}
    \hldef{w} \hlkwb{=} \hlkwd{c}\hldef{(w,} \hlkwd{mean}\hldef{(tmp}\hlopt{$}\hldef{weight))}
\hldef{\}}
\hldef{data_simplified}\hlopt{$}\hldef{weight} \hlkwb{=} \hldef{w}
\hldef{data_simplified}\hlopt{$}\hldef{height} \hlkwb{=} \hldef{h}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{data} \hlkwb{=} \hldef{data_simplified}
\hlkwd{head}\hldef{(data_simplified,} \hlkwc{n} \hldef{=} \hlnum{6}\hldef{)}
\end{alltt}
\begin{verbatim}
##                  name weight height
## 1    tverdovsky oleg    84.0  185.0
## 2  vichnevsky vitali    86.0  188.0
## 3 petrochinin evgeni    95.0  182.0
## 4    zhdan alexander    85.5  178.5
## 5    orekhovsky oleg    88.0  175.0
## 6      zhukov sergei    92.5  193.0
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}



\maxFrameImage{FIGS/L10-plot-hockey-2-1.pdf}

\begin{frame}[fragile]\frametitle{Centre the data}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{mean}\hldef{(data}\hlopt{$}\hldef{weight)}
\end{alltt}
\begin{verbatim}
## [1] 87.71555
\end{verbatim}
\begin{alltt}
\hlkwd{mean}\hldef{(data}\hlopt{$}\hldef{height)}
\end{alltt}
\begin{verbatim}
## [1] 183.8596
\end{verbatim}
\begin{alltt}
\hldef{data}\hlopt{$}\hldef{weight.c} \hlkwb{=} \hldef{data}\hlopt{$}\hldef{weight}\hlopt{-}\hlkwd{mean}\hldef{(data}\hlopt{$}\hldef{weight)}
\hldef{data}\hlopt{$}\hldef{height.c} \hlkwb{=} \hldef{data}\hlopt{$}\hldef{height}\hlopt{-}\hlkwd{mean}\hldef{(data}\hlopt{$}\hldef{height)}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{frame}

\maxFrameImage{FIGS/L10-plot-hockey-centred-1.pdf}



\begin{frame}{Setting things up}
Each player is a row in the matrix (an \emph{observation}), each variable (\emph{height} and \emph{weight}) is a column
\vfill
After deduplication, we have an 3278$\times 2$ matrix (actually, 3278$\times 4$ if we consider the uncentred and centred variables, but we will use one or the other, not both uncentred and centred)
\vfill
We want to find what carries the most information
\vfill
For this, we are going to project the information in a new basis in which the first ``dimension'' will carry most information (in a sense we'll define later), the second dimension will carry a little less, etc.
\vfill
In order to do so, we need to learn how to change bases
\end{frame}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{Change of basis}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}


\begin{frame}
	In the following slide, 
	\[
	[\bx]_\B
	\]
	denotes the coordinates of $\bx$ in the basis $\B$
	\vfill
	The aim of a change of basis is to express vectors in another coordinate system (another basis)
	\vfill
	We do so by finding a matrix allowing to move from one basis to another
\end{frame}

\begin{frame}{Change of basis}
\begin{definition}[Change of basis matrix]
$\B=\{\bu_1,\ldots,\bu_n\}$ and $\C=\{\bv_1,\ldots,\bv_n\}$ bases of vector space $V$
\vfill
The \textbf{change of basis matrix} $P_{\C\leftarrow\B}\in\M_n$,
\[
P_{\C\leftarrow\B}
=\left[
[\bu_1]_\C \cdots [\bu_n]_\C
\right]
\]
has columns the coordinate vectors $[\bu_1]_\C,\ldots,[\bu_n]_\C$ of vectors in $\B$ with respect to $\C$
\end{definition}
\vfill
\begin{theorem}
$\B=\{\bu_1,\ldots,\bu_n\}$ and $\C=\{\bv_1,\ldots,\bv_n\}$ bases of vector space $V$ and $P_{\C\leftarrow\B}$ a change of basis matrix from $\B$ to $\C$
\begin{enumerate}
\item $\forall\bx\in V$, $P_{\C\leftarrow\B}[\bx]_\B = [\bx]_\C$
\item $P_{\C\leftarrow\B}$ s.t. $\forall\bx\in V$, $P_{\C\leftarrow\B}[\bx]_\B = [\bx]_\C$ is \textbf{unique}
\item $P_{\C\leftarrow\B}$ invertible and $P_{\C\leftarrow\B}^{-1}=P_{\B\leftarrow\C}$
\end{enumerate}
\end{theorem}
\end{frame}


\begin{frame}{Row-reduction method for changing bases}
\begin{importanttheorem}
\label{th:change-basis-construction}
$\B=\{\bu_1,\ldots,\bu_n\}$ and $\C=\{\bv_1,\ldots,\bv_n\}$ bases of vector space $V$. Let $\E$ be any basis for $V$,
\[
B = [[\bu_1]_\E,\ldots,[\bu_n]_\E] 
\textrm{ and }
C = [[\bv_1]_\E,\ldots,[\bv_n]_\E] 
\]
and let $[C|B]$ be the augmented matrix constructed using $C$ and $B$. Then
\[
RREF\left([C|B]\right)
=[\II|P_{\C\leftarrow\B}]
\]
\end{importanttheorem}
\vfill
If working in $\IR^n$, this is quite useful with $\E$ the standard basis of $\IR^n$ (it does not matter if $\B=\E$)
\end{frame}

\begin{frame}
So the question now becomes
\begin{quote}
How do we find what new basis to look at our data in?
\end{quote}
\vfill
(Changing the basis does not change the data, just the view you have of it)
\vfill
(Think of what happens when you do a headstand.. your up becomes down, your right and left switch, but the world does not change, just your view of it)
\vfill
(Changes of bases are \emph{fundamental} operations in Science)
\end{frame}







%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{Example of change of basis}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}

\begin{frame}{Worked example: change of basis}
\textbf{Problem:} Find the change of basis matrix from basis $\B$ to basis $\C$ in $\IR^2$, where
$$\B = \left\{\begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix}\right\} \quad \text{and} \quad \C = \left\{\begin{pmatrix} 1 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ -1 \end{pmatrix}\right\}$$

\vfill
Then use this to find the coordinates of $\bx = \begin{pmatrix} 3 \\ 2 \end{pmatrix}$ in basis $\C$
\end{frame}

\begin{frame}{Step 1 -- Set up the matrices}
\begin{itemize}
\item $\B$ is the standard basis of $\IR^2$, so $[\bu_1]_\E = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$, $[\bu_2]_\E = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$
\item For $\C$: $[\bv_1]_\E = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$, $[\bv_2]_\E = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$
\end{itemize}
\end{frame}

\begin{frame}{Step 2 -- Row reduce}
Using Theorem~\ref{th:change-basis-construction}, we form the augmented matrix $[C|B]$:
$$[C|B] = \left[\begin{array}{cc|cc} 1 & 1 & 1 & 0 \\ 1 & -1 & 0 & 1 \end{array}\right]$$

\vfill
\textbf{Row reduce to RREF:}
\begin{align*}
\left[\begin{array}{cc|cc} 1 & 1 & 1 & 0 \\ 1 & -1 & 0 & 1 \end{array}\right] &\xrightarrow{R_2 \leftarrow R_2 - R_1} \left[\begin{array}{cc|cc} 1 & 1 & 1 & 0 \\ 0 & -2 & -1 & 1 \end{array}\right] \\
&\xrightarrow{R_2 \leftarrow -\frac{1}{2}R_2} \left[\begin{array}{cc|cc} 1 & 1 & 1 & 0 \\ 0 & 1 & \frac{1}{2} & -\frac{1}{2} \end{array}\right] \\
&\xrightarrow{R_1 \leftarrow R_1 - R_2} \left[\begin{array}{cc|cc} 1 & 0 & \frac{1}{2} & \frac{1}{2} \\ 0 & 1 & \frac{1}{2} & -\frac{1}{2} \end{array}\right]
\end{align*}
\end{frame}

\begin{frame}{Step 3 -- Extract the change of basis matrix}
From the RREF form $[\II|P_{\C\leftarrow\B}]$, we get:
$$P_{\C\leftarrow\B} = \begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & -\frac{1}{2} \end{pmatrix}$$

\vfill
\textbf{Verification:} Let's check that this matrix works correctly.
\begin{itemize}
\item $[\bu_1]_\C = P_{\C\leftarrow\B}[\bu_1]_\B = \begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & -\frac{1}{2} \end{pmatrix}\begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} \frac{1}{2} \\ \frac{1}{2} \end{pmatrix}$
\item Check: $\frac{1}{2}\begin{pmatrix} 1 \\ 1 \end{pmatrix} + \frac{1}{2}\begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$
\end{itemize}
\end{frame}

\begin{frame}{Step 4 -- Coordinates of $\bx$ in basis $\C$}
Now we find $[\bx]_\C$ for $\bx = \begin{pmatrix} 3 \\ 2 \end{pmatrix}$:

$$[\bx]_\C = P_{\C\leftarrow\B}[\bx]_\B = \begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & -\frac{1}{2} \end{pmatrix}\begin{pmatrix} 3 \\ 2 \end{pmatrix} = \begin{pmatrix} \frac{5}{2} \\ \frac{1}{2} \end{pmatrix}$$

\vfill
\textbf{Verification:} Check that this gives us back the original vector:
$$\frac{5}{2}\begin{pmatrix} 1 \\ 1 \end{pmatrix} + \frac{1}{2}\begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} \frac{5}{2} + \frac{1}{2} \\ \frac{5}{2} - \frac{1}{2} \end{pmatrix} = \begin{pmatrix} 3 \\ 2 \end{pmatrix}$$

\vfill
\textbf{Answer:} The coordinates of $\bx = \begin{pmatrix} 3 \\ 2 \end{pmatrix}$ in basis $\C$ are $[\bx]_\C = \begin{pmatrix} \frac{5}{2} \\ \frac{1}{2} \end{pmatrix}$
\end{frame}

\begin{frame}{Alternative method -- Direct calculation}
We could also solve this directly by setting up the system:
$$\bx = c_1\bv_1 + c_2\bv_2$$
$$\begin{pmatrix} 3 \\ 2 \end{pmatrix} = c_1\begin{pmatrix} 1 \\ 1 \end{pmatrix} + c_2\begin{pmatrix} 1 \\ -1 \end{pmatrix}$$

This gives us the system:
\begin{align*}
c_1 + c_2 &= 3 \\
c_1 - c_2 &= 2
\end{align*}

Solving: $c_1 = \frac{5}{2}$, $c_2 = \frac{1}{2}$

\vfill
This confirms our result: $[\bx]_\C = \begin{pmatrix} \frac{5}{2} \\ \frac{1}{2} \end{pmatrix}$
\end{frame}


%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\Ssection{A crash course on probability}{FIGS-slides-admin/Gemini_Generated_Image_c06eixc06eixc06e.jpeg}


\begin{frame}{Why probability?}
We said earlier that we would look for a basis in which the first dimension carries most information
\vfill
But how do we define \emph{information}?
\vfill
We use concepts from probability and statistics to do so
\vfill
A good measure of information is \emph{variance} (how much data varies around the mean)
\end{frame} 

\begin{frame}{Brief ``review'' of some probability concepts}
Proper definition of \emph{probability} requires to use \emph{measure theory}.. will not get into details here
\vfill
A \textbf{random variable} $X$ is a \emph{measurable} function $X:\Omega\to E$, where $\Omega$ is a set of outcomes (\emph{sample space}) and $E$ is a measurable space
\vfill
$\IP(X\in S\subseteq E) = \IP(\omega\in\Omega|X(\omega)\in S)$
\vfill
\textbf{Distribution function} of a r.v., $F(x)=\IP(X\leq x)$, describes the distribution of a r.v.
\vfill
R.v. can be discrete or continuous or .. other things. 
\end{frame}

\begin{frame}
\begin{definition}[Variance]
Let $X$ be a random variable. The \textbf{variance} of $X$ is given by
\[
\Var X = E\left[\left(X-E(X)\right)^2\right]
\]
where $E$ is the expected value
\end{definition}
\vfill
\begin{definition}[Covariance]
Let $X,Y$ be jointly distributed random variables. The \textbf{covariance} of $X$ and $Y$ is given by
\[
\cov (X,Y) = E\left[\left(X-E(X)\right)\left(Y-E(Y)\right)\right]
\]
\end{definition}
\vfill
Note that $\cov(X,X)=E\left[\left(X-E(X)\right)^2\right] = \Var X$
\end{frame}

\begin{frame}{In practice: ``true law'' versus ``observation''}
In statistics: we reason on the \emph{true law} of distributions, but we usually have only access to a sample
\vfill
We then use \textbf{estimators} to .. estimate the value of a parameter, e.g., the mean, variance and covariance
\vfill
\end{frame}
    
\begin{frame}
\begin{definition}[Unbiased estimators of the mean and variance]
Let $x_1,\ldots,x_n$ be data points (the \emph{sample}) and 
\[
\bar x = \frac 1n \sum_{i=1}^n x_i
\]
be the \textbf{mean} of the data. An unbiased estimator of the variance of the sample is
\[
\sigma^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i-\bar x)^2
\]
\end{definition}
\end{frame}

\begin{frame}
\begin{definition}[Unbiased estimator of the covariance]
Let $(x_1,y_1),\ldots,(x_n,y_n)$ be data points,
\[
\bar x = \frac 1n \sum_{i=1}^n x_i
\textrm{ and }
\bar y = \frac 1n \sum_{i=1}^n y_i
\]
be the means of the data. An estimator of the covariance of the sample is
\[
\cov(x,y) = \frac{1}{n}\sum_{i=1}^n (x_i-\bar x)(y_i-\bar y)
\]
\end{definition}
\end{frame}

\begin{frame}{What does covariance do?}
Variance explains how data disperses around the mean, in a 1-D case
\vfill
Covariance measures the relationship between two dimensions. E.g., height and weight
\vfill
More than the exact value, the sign is important:
\begin{itemize}
    \item $\cov(X,Y)>0$: both dimensions change in the same ``direction''; e.g., larger height usually means higher weight
    \item $\cov(X,Y)<0$: both dimensions change in reverse directions; e.g., time spent on social media and performance in this class
    \item $\cov(X,Y)=0$: the dimensions are independent from one another; e.g., sex/gender and ``intelligence''
\end{itemize}
\end{frame}

\begin{frame}{The covariance matrix (we usually have more than 2 variables)}
\begin{definition}
Suppose $p$ random variables $X_1,\ldots,X_p$. Then the covariance matrix is the symmetric matrix
\[
\begin{pmatrix}
\cov(X_1,X_1) & \cov(X_1,X_2) & \cdots & \cov(X_1,X_p) \\
\cov(X_2,X_1) & \cov(X_2,X_2) & \cdots & \cov(X_2,X_p) \\
\vdots & \vdots & & \vdots \\
\cov(X_p,X_1) & \cov(X_p,X_2) & \cdots & \cov(X_p,X_p) 
\end{pmatrix}
\]
i.e., using the properties of covariance,
\[
\begin{pmatrix}
\Var X_1 & \cov(X_1,X_2) & \cdots & \cov(X_1,X_p) \\
\cov(X_1,X_2) & \Var X_2 & \cdots & \cov(X_2,X_p) \\
\vdots & \vdots & & \vdots \\
\cov(X_1,X_p) & \cov(X_2,X_p) & \cdots & \Var X_p 
\end{pmatrix}
\]
\end{definition}
\end{frame}



% Save counters for next file

% Save theorem count for next file
\end{document}
